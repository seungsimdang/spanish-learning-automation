# .github/workflows/spanish-learning-automation.yml
name: ìŠ¤í˜ì¸ì–´ í•™ìŠµ ìë£Œ ìë™ ìˆ˜ì§‘

on:
  schedule:
    # ë§¤ì¼ UTC 23:00 (í•œêµ­ì‹œê°„ ì˜¤ì „ 8ì‹œ)ì— ì‹¤í–‰
    - cron: "0 23 * * 1-5" # í‰ì¼ë§Œ ì‹¤í–‰
  workflow_dispatch: # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥

env:
  NOTION_TOKEN: ${{ secrets.NOTION_TOKEN }}
  NOTION_DATABASE_ID: ${{ secrets.NOTION_DATABASE_ID }}

jobs:
  collect-learning-materials:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install dependencies
        run: |
          pip install requests beautifulsoup4 feedparser python-dateutil lxml

      - name: Calculate learning phase and schedule
        id: phase
        run: |
          python3 << 'EOF'
          import os
          from datetime import datetime, timedelta

          # í•™ìŠµ ì‹œì‘ì¼ (2025-07-01)
          start_date = datetime(2025, 7, 1)
          current_date = datetime.now()

          # ì£¼ì°¨ ê³„ì‚°
          week_num = (current_date - start_date).days // 7 + 1
          weekday = current_date.weekday()  # 0=ì›”ìš”ì¼

          # ë…í•´ ì†ŒìŠ¤ ê²°ì •
          if week_num <= 2:
              reading_source = "20minutos"
              reading_url = "https://www.20minutos.es/"
              reading_difficulty = "B2"
          elif week_num <= 4:
              reading_source = "El PaÃ­s ë‹¨ì‹ "
              reading_url = "https://elpais.com/"
              reading_difficulty = "B2"
          else:
              reading_source = "El PaÃ­s ì‚¬ì„¤"
              reading_url = "https://elpais.com/opinion/"
              reading_difficulty = "C1"
              
          # íŒŸìºìŠ¤íŠ¸ ì¼ì • (RSS í”¼ë“œì™€ Apple Podcasts ë§í¬)
          podcast_schedule = {
              0: {
                  "name": "Hoy Hablamos",
                  "rss": "https://feeds.feedburner.com/hoyhablamos",
                  "apple_base": "https://podcasts.apple.com/kr/podcast/hoy-hablamos-podcast-diario-para-aprender-espaÃ±ol-learn/id1201483158",
                  "region": "ìŠ¤í˜ì¸",
                  "backup_url": "https://www.hoyhablamos.com/"
              },
              1: {
                  "name": "Radio Ambulante", 
                  "rss": "https://feeds.npr.org/510311/podcast.xml",
                  "apple_base": "https://podcasts.apple.com/kr/podcast/radio-ambulante/id527614348",
                  "region": "ì¤‘ë‚¨ë¯¸",
                  "backup_url": "https://radioambulante.org/"
              },
              2: {
                  "name": "Advanced Spanish",
                  "rss": "https://feeds.buzzsprout.com/1829091.rss", 
                  "apple_base": "https://podcasts.apple.com/kr/podcast/advanced-spanish-podcast-espaÃ±ol-avanzado/id1632291264",
                  "region": "ìŠ¤í˜ì¸",
                  "backup_url": "https://www.spanishlanguagecoach.com/podcast/"
              },
              3: {
                  "name": "Radio Ambulante",
                  "rss": "https://feeds.npr.org/510311/podcast.xml",
                  "apple_base": "https://podcasts.apple.com/kr/podcast/radio-ambulante/id527614348", 
                  "region": "ì¤‘ë‚¨ë¯¸",
                  "backup_url": "https://radioambulante.org/"
              },
              4: {
                  "name": "DELE Podcast",
                  "rss": "https://anchor.fm/s/f4f4a4f0/podcast/rss",
                  "apple_base": "https://podcasts.apple.com/us/podcast/examen-dele/id1705001626",
                  "region": "ìŠ¤í˜ì¸", 
                  "backup_url": "https://anchor.fm/examen-dele"
              }
          }

          podcast_info = podcast_schedule.get(weekday, podcast_schedule[0])

          # GitHub Actions í™˜ê²½ë³€ìˆ˜ë¡œ ì¶œë ¥
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f"week_num={week_num}\n")
              f.write(f"reading_source={reading_source}\n")
              f.write(f"reading_url={reading_url}\n")
              f.write(f"reading_difficulty={reading_difficulty}\n")
              f.write(f"podcast_name={podcast_info['name']}\n")
              f.write(f"podcast_rss={podcast_info['rss']}\n")
              f.write(f"podcast_apple_base={podcast_info['apple_base']}\n")
              f.write(f"podcast_region={podcast_info['region']}\n")
              f.write(f"podcast_backup={podcast_info['backup_url']}\n")
              f.write(f"date={current_date.strftime('%Y-%m-%d')}\n")
              f.write(f"weekday_name={['ì›”ìš”ì¼', 'í™”ìš”ì¼', 'ìˆ˜ìš”ì¼', 'ëª©ìš”ì¼', 'ê¸ˆìš”ì¼', 'í† ìš”ì¼', 'ì¼ìš”ì¼'][weekday]}\n")
          EOF

      - name: Collect articles and podcast episodes with full content analysis
        id: materials
        run: |
          python3 << 'EOF'
          import os
          import requests
          import feedparser
          from datetime import datetime
          import re
          from bs4 import BeautifulSoup
          from urllib.parse import urljoin
          import time

          def get_article_content(url):
              """ì‹¤ì œ ê¸°ì‚¬ URLì— ì ‘ì†í•´ì„œ ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
              try:
                  headers = {
                      'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
                  }
                  
                  response = requests.get(url, headers=headers, timeout=10)
                  response.raise_for_status()
                  response.encoding = 'utf-8'
                  
                  soup = BeautifulSoup(response.content, 'html.parser')
                  
                  # ì‚¬ì´íŠ¸ë³„ ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
                  content = ""
                  
                  if '20minutos.es' in url:
                      # 20minutos ë³¸ë¬¸ ì¶”ì¶œ
                      article_body = soup.find('div', class_='article-text') or soup.find('div', class_='content')
                      if article_body:
                          paragraphs = article_body.find_all(['p', 'div'])
                          content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                  
                  elif 'elpais.com' in url:
                      # El PaÃ­s ë³¸ë¬¸ ì¶”ì¶œ
                      article_body = soup.find('div', {'data-dtm-region': 'articulo_cuerpo'}) or \
                                   soup.find('div', class_='a_c clearfix') or \
                                   soup.find('div', class_='articulo-cuerpo')
                      if article_body:
                          paragraphs = article_body.find_all('p')
                          content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
                  
                  # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (fallback)
                  if not content:
                      # ì¼ë°˜ì ì¸ article íƒœê·¸ë‚˜ main íƒœê·¸ì—ì„œ ì¶”ì¶œ
                      article = soup.find('article') or soup.find('main')
                      if article:
                          paragraphs = article.find_all('p')
                          content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()][:10])  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨ë§Œ
                  
                  # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
                  if len(content) < 200:
                      all_paragraphs = soup.find_all('p')
                      content = ' '.join([p.get_text().strip() for p in all_paragraphs if len(p.get_text().strip()) > 50][:8])
                  
                  return content[:2000]  # ì²˜ìŒ 2000ìë§Œ ë°˜í™˜
                  
              except Exception as e:
                  print(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
                  return ""

          def extract_vocabulary_from_content(content, difficulty="B2"):
              """ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì—ì„œ í•™ìŠµì ìˆ˜ì¤€ì— ë§ëŠ” í•µì‹¬ ì–´íœ˜ë¥¼ ì¶”ì¶œ"""
              if not content:
                  return []
              
              # ìˆ˜ì¤€ë³„ ì–´íœ˜ ì¶”ì¶œ ê¸°ì¤€
              if difficulty == "B2":
                  # B2 ìˆ˜ì¤€: ì¤‘ê¸‰ ì–´íœ˜, ì¼ìƒì ì´ì§€ ì•Šì€ í‘œí˜„ë“¤
                  target_patterns = [
                      # ë™ì‚¬ë¥˜
                      r'\b(desarrollar|establecer|implementar|generar|promover|fortalecer|consolidar|impulsar|fomentar|garantizar)\b',
                      # ëª…ì‚¬ë¥˜  
                      r'\b(iniciativa|propuesta|medida|estrategia|polÃ­tica|programa|proyecto|inversiÃ³n|desarrollo|crecimiento)\b',
                      # í˜•ìš©ì‚¬ë¥˜
                      r'\b(fundamental|esencial|crucial|significativo|relevante|considerable|notable|destacado|principal|prioritario)\b',
                      # ì—°ê²°ì–´
                      r'\b(ademÃ¡s|asimismo|por tanto|sin embargo|no obstante|en consecuencia|por consiguiente|en definitiva)\b',
                      # ê¸°ê´€/ì¡°ì§ ê´€ë ¨
                      r'\b(entidad|organismo|instituciÃ³n|administraciÃ³n|departamento|ministerio|ayuntamiento|comunidad)\b'
                  ]
              elif difficulty == "C1":
                  # C1 ìˆ˜ì¤€: ê³ ê¸‰ ì–´íœ˜, ì „ë¬¸ì /í•™ìˆ ì  í‘œí˜„ë“¤
                  target_patterns = [
                      # ê³ ê¸‰ ë™ì‚¬
                      r'\b(implementar|consolidar|incrementar|optimizar|diversificar|potenciar|materializar|vehicular|canalizar)\b',
                      # ì „ë¬¸ ëª…ì‚¬
                      r'\b(sostenibilidad|competitividad|rentabilidad|eficiencia|transparencia|gobernanza|paradigma|metodologÃ­a)\b',
                      # ê³ ê¸‰ í˜•ìš©ì‚¬
                      r'\b(innovador|sostenible|competitivo|eficiente|transparente|inclusivo|participativo|colaborativo)\b',
                      # í•™ìˆ ì  ì—°ê²°ì–´
                      r'\b(en este sentido|cabe destacar|es preciso|conviene seÃ±alar|resulta evidente|se constata)\b',
                      # ì „ë¬¸ ë¶„ì•¼ ìš©ì–´
                      r'\b(digitalizaciÃ³n|transformaciÃ³n|modernizaciÃ³n|reestructuraciÃ³n|reconversiÃ³n|reorientaciÃ³n)\b'
                  ]
              else:
                  # ê¸°ë³¸ B2 íŒ¨í„´ ì‚¬ìš©
                  target_patterns = [
                      r'\b(desarrollar|establecer|implementar|medida|estrategia|fundamental|ademÃ¡s|sin embargo)\b'
                  ]
              
              found_vocab = []
              content_lower = content.lower()
              
              # ì–´íœ˜ ì¶”ì¶œ ë° ì˜ë¯¸ ë§¤í•‘
              vocab_meanings = {
                  # B2 ìˆ˜ì¤€ ì–´íœ˜
                  'desarrollar': 'ê°œë°œí•˜ë‹¤, ë°œì „ì‹œí‚¤ë‹¤',
                  'establecer': 'ì„¤ë¦½í•˜ë‹¤, í™•ë¦½í•˜ë‹¤',
                  'implementar': 'ì‹œí–‰í•˜ë‹¤, êµ¬í˜„í•˜ë‹¤',
                  'generar': 'ìƒì„±í•˜ë‹¤, ë§Œë“¤ì–´ë‚´ë‹¤',
                  'promover': 'ì´‰ì§„í•˜ë‹¤, ì¥ë ¤í•˜ë‹¤',
                  'fortalecer': 'ê°•í™”í•˜ë‹¤',
                  'consolidar': 'í†µí•©í•˜ë‹¤, ê²¬ê³ íˆ í•˜ë‹¤',
                  'impulsar': 'ì¶”ì§„í•˜ë‹¤, ì´‰ì§„í•˜ë‹¤',
                  'fomentar': 'ì¥ë ¤í•˜ë‹¤, ì´‰ì§„í•˜ë‹¤',
                  'garantizar': 'ë³´ì¥í•˜ë‹¤',
                  'iniciativa': 'ê³„íš, ì£¼ë„ê¶Œ',
                  'propuesta': 'ì œì•ˆ',
                  'medida': 'ì¡°ì¹˜, ëŒ€ì±…',
                  'estrategia': 'ì „ëµ',
                  'polÃ­tica': 'ì •ì±…',
                  'programa': 'í”„ë¡œê·¸ë¨',
                  'proyecto': 'í”„ë¡œì íŠ¸',
                  'inversiÃ³n': 'íˆ¬ì',
                  'desarrollo': 'ë°œì „, ê°œë°œ',
                  'crecimiento': 'ì„±ì¥',
                  'fundamental': 'ê¸°ë³¸ì ì¸, ê·¼ë³¸ì ì¸',
                  'esencial': 'í•„ìˆ˜ì ì¸',
                  'crucial': 'ì¤‘ìš”í•œ, ê²°ì •ì ì¸',
                  'significativo': 'ì˜ë¯¸ìˆëŠ”, ì¤‘ìš”í•œ',
                  'relevante': 'ê´€ë ¨ìˆëŠ”, ì¤‘ìš”í•œ',
                  'considerable': 'ìƒë‹¹í•œ',
                  'notable': 'ì£¼ëª©í•  ë§Œí•œ',
                  'destacado': 'ë›°ì–´ë‚œ, ë‘ë“œëŸ¬ì§„',
                  'principal': 'ì£¼ìš”í•œ',
                  'prioritario': 'ìš°ì„ ì ì¸',
                  'ademÃ¡s': 'ê²Œë‹¤ê°€, ë˜í•œ',
                  'asimismo': 'ë§ˆì°¬ê°€ì§€ë¡œ',
                  'por tanto': 'ë”°ë¼ì„œ',
                  'sin embargo': 'ê·¸ëŸ¬ë‚˜',
                  'no obstante': 'ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ',
                  'en consecuencia': 'ê²°ê³¼ì ìœ¼ë¡œ',
                  'por consiguiente': 'ë”°ë¼ì„œ',
                  'en definitiva': 'ê²°êµ­',
                  'entidad': 'ê¸°ê´€, ë‹¨ì²´',
                  'organismo': 'ê¸°ê´€, ì¡°ì§',
                  'instituciÃ³n': 'ê¸°ê´€, ì œë„',
                  'administraciÃ³n': 'í–‰ì •ë¶€',
                  'departamento': 'ë¶€ì„œ',
                  'ministerio': 'ë¶€ (ì •ë¶€ê¸°ê´€)',
                  'ayuntamiento': 'ì‹œì²­',
                  'comunidad': 'ì§€ì—­ì‚¬íšŒ, ê³µë™ì²´',
                  
                  # C1 ìˆ˜ì¤€ ì–´íœ˜
                  'incrementar': 'ì¦ê°€ì‹œí‚¤ë‹¤',
                  'optimizar': 'ìµœì í™”í•˜ë‹¤',
                  'diversificar': 'ë‹¤ì–‘í™”í•˜ë‹¤',
                  'potenciar': 'ê°•í™”í•˜ë‹¤, ì ì¬ë ¥ì„ í‚¤ìš°ë‹¤',
                  'materializar': 'ì‹¤í˜„í•˜ë‹¤',
                  'vehicular': 'ì „ë‹¬í•˜ë‹¤, ìˆ˜ë‹¨ì´ ë˜ë‹¤',
                  'canalizar': 'ê²½ë¡œë¥¼ ì œê³µí•˜ë‹¤',
                  'sostenibilidad': 'ì§€ì†ê°€ëŠ¥ì„±',
                  'competitividad': 'ê²½ìŸë ¥',
                  'rentabilidad': 'ìˆ˜ìµì„±',
                  'eficiencia': 'íš¨ìœ¨ì„±',
                  'transparencia': 'íˆ¬ëª…ì„±',
                  'gobernanza': 'ê±°ë²„ë„ŒìŠ¤, í†µì¹˜',
                  'paradigma': 'íŒ¨ëŸ¬ë‹¤ì„',
                  'metodologÃ­a': 'ë°©ë²•ë¡ ',
                  'innovador': 'í˜ì‹ ì ì¸',
                  'sostenible': 'ì§€ì†ê°€ëŠ¥í•œ',
                  'competitivo': 'ê²½ìŸì ì¸',
                  'eficiente': 'íš¨ìœ¨ì ì¸',
                  'transparente': 'íˆ¬ëª…í•œ',
                  'inclusivo': 'í¬ìš©ì ì¸',
                  'participativo': 'ì°¸ì—¬ì ì¸',
                  'colaborativo': 'í˜‘ë ¥ì ì¸',
                  'digitalizaciÃ³n': 'ë””ì§€í„¸í™”',
                  'transformaciÃ³n': 'ë³€í™”, ë³€í˜',
                  'modernizaciÃ³n': 'í˜„ëŒ€í™”',
                  'reestructuraciÃ³n': 'êµ¬ì¡°ì¡°ì •',
                  'reconversiÃ³n': 'ì „í™˜',
                  'reorientaciÃ³n': 'ë°©í–¥ ì „í™˜'
              }
              
              # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì–´íœ˜ ì°¾ê¸°
              for pattern in target_patterns:
                  matches = re.findall(pattern, content_lower, re.IGNORECASE)
                  for match in matches:
                      if match.lower() in vocab_meanings and match.lower() not in [v.split(' (')[0].lower() for v in found_vocab]:
                          meaning = vocab_meanings[match.lower()]
                          found_vocab.append(f"{match} ({meaning})")
              
              # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 5ê°œ ë°˜í™˜
              unique_vocab = []
              seen = set()
              for vocab in found_vocab:
                  word = vocab.split(' (')[0].lower()
                  if word not in seen:
                      unique_vocab.append(vocab)
                      seen.add(word)
                      if len(unique_vocab) >= 5:
                          break
              
              return unique_vocab

          def extract_category_from_content(title, content):
              """ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
              full_text = (title + " " + content).lower()
              
              keywords = {
                  'ì •ì¹˜': ['gobierno', 'polÃ­tica', 'elecciones', 'parlamento', 'ministro', 'rey', 'presidente', 'votaciÃ³n', 'congreso'],
                  'ê²½ì œ': ['economÃ­a', 'banco', 'euro', 'empleo', 'crisis', 'mercado', 'dinero', 'trabajo', 'empresa', 'inversiÃ³n'],
                  'ì‚¬íšŒ': ['sociedad', 'educaciÃ³n', 'sanidad', 'vivienda', 'familia', 'salud', 'poblaciÃ³n', 'ciudadanos'],
                  'ìŠ¤í¬ì¸ ': ['fÃºtbol', 'real madrid', 'barcelona', 'liga', 'deporte', 'partido', 'atletico', 'champions'],
                  'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³vil', 'digital', 'app', 'inteligencia', 'innovaciÃ³n'],
                  'ë¬¸í™”': ['cultura', 'arte', 'mÃºsica', 'teatro', 'festival', 'libro', 'cine', 'exposiciÃ³n'],
                  'êµ­ì œ': ['internacional', 'mundial', 'europa', 'amÃ©rica', 'china', 'estados unidos', 'uniÃ³n europea']
              }
              
              category_scores = {}
              for category, words in keywords.items():
                  score = sum(1 for word in words if word in full_text)
                  if score > 0:
                      category_scores[category] = score
              
              if category_scores:
                  return max(category_scores, key=category_scores.get)
              return 'ì¼ë°˜'

          def extract_episode_number(title):
              patterns = [
                  r'Ep\.?\s*(\d+)',
                  r'Episode\s*(\d+)',
                  r'#(\d+)',
                  r'(\d{3,4})'
              ]
              
              for pattern in patterns:
                  match = re.search(pattern, title, re.IGNORECASE)
                  if match:
                      return match.group(1)
              return None

          def extract_duration_from_feed(entry):
              # iTunes ë“€ë ˆì´ì…˜ ë¨¼ì € í™•ì¸
              if hasattr(entry, 'itunes_duration'):
                  duration = entry.itunes_duration
                  # ì´ˆ ë‹¨ìœ„ì¸ ê²½ìš° ë¶„:ì´ˆë¡œ ë³€í™˜
                  if duration.isdigit():
                      total_seconds = int(duration)
                      minutes = total_seconds // 60
                      seconds = total_seconds % 60
                      return f"{minutes}:{seconds:02d}"
                  return duration
              
              # ìš”ì•½ì—ì„œ ì¬ìƒì‹œê°„ ì¶”ì¶œ ì‹œë„
              summary = entry.get('summary', '') + entry.get('description', '')
              duration_patterns = [
                  r'(\d+)\s*min',
                  r'(\d+)\s*ë¶„',
                  r'(\d+):(\d+)',
                  r'Duration:\s*(\d+)'
              ]
              
              for pattern in duration_patterns:
                  match = re.search(pattern, summary)
                  if match:
                      if ':' in pattern:
                          return f"{match.group(1)}:{match.group(2)}"
                      else:
                          return f"{match.group(1)}ë¶„"
              
              return "15-25ë¶„"

          def extract_topic_keywords(title, summary=""):
              content = (title + " " + summary).lower()
              
              topic_keywords = {
                  'ë¬¸ë²•': ['gramÃ¡tica', 'verbos', 'subjuntivo', 'pretÃ©rito', 'sintaxis'],
                  'ë¬¸í™”': ['cultura', 'tradiciÃ³n', 'costumbres', 'historia', 'arte'],
                  'ìš”ë¦¬': ['cocina', 'comida', 'receta', 'gastronomÃ­a', 'plato'],
                  'ì—¬í–‰': ['viajes', 'turismo', 'ciudades', 'lugares', 'destinos'],
                  'ì§ì—…': ['trabajo', 'empleo', 'profesiÃ³n', 'carrera', 'oficina'],
                  'ê°€ì¡±': ['familia', 'padres', 'hijos', 'matrimonio', 'casa'],
                  'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³viles', 'digital', 'aplicaciones'],
                  'ì •ì¹˜': ['polÃ­tica', 'gobierno', 'elecciones', 'democracia'],
                  'ê²½ì œ': ['economÃ­a', 'dinero', 'banco', 'trabajo', 'crisis', 'preferentes', 'ahorros'],
                  'ì‚¬íšŒ': ['sociedad', 'gente', 'problemas', 'cambios', 'vida'],
                  'ê±´ê°•': ['salud', 'medicina', 'hospital', 'enfermedad', 'mÃ©dico'],
                  'êµìœ¡': ['educaciÃ³n', 'estudiantes', 'universidad', 'aprender']
              }
              
              for topic, keywords in topic_keywords.items():
                  if any(keyword in content for keyword in keywords):
                      return topic
              return 'ì¼ë°˜ ì£¼ì œ'

          def create_detailed_memo(content_type, data, weekday_name):
              if content_type == "article":
                  category = data.get('category', 'ì¼ë°˜')
                  vocabulary = data.get('vocabulary', [])
                  difficulty = data.get('difficulty', 'B2')
                  
                  vocab_text = ""
                  if vocabulary:
                      vocab_list = ", ".join(vocabulary[:3])  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                      vocab_text = f"ğŸ“š í•µì‹¬ ì–´íœ˜: {vocab_list} "
                  
                  return (f"ğŸ“° {category} ë¶„ì•¼ ê¸°ì‚¬ ({difficulty} ìˆ˜ì¤€) "
                         f"ğŸ“… ë°œí–‰: {data.get('published', 'ì˜¤ëŠ˜')} "
                         f"ğŸ¯ í•™ìŠµëª©í‘œ: 15ë¶„ ë…í•´, {difficulty} ìˆ˜ì¤€ ì–´íœ˜ ì •ë¦¬ "
                         f"{vocab_text}"
                         f"ğŸ“ ê¶Œì¥: ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ì„ í†µí•œ ë§ì¶¤ ì–´íœ˜ í•™ìŠµ")

              elif content_type == "podcast":
                  podcast_name = data.get('podcast_name', '')
                  duration = data.get('duration', '15-25ë¶„')
                  topic = data.get('topic', 'ì¼ë°˜ ì£¼ì œ')
                  episode_num = data.get('episode_number', '')
                  
                  # ì£¼ì œì— ë”°ë¥¸ í•™ìŠµëª©í‘œ ì„¤ì •
                  learning_goals = {
                      'ê²½ì œ': 'ê¸ˆìœµ ì–´íœ˜',
                      'ì •ì¹˜': 'ì •ì¹˜ ìš©ì–´',
                      'ë¬¸í™”': 'ë¬¸í™” í‘œí˜„',
                      'ì‚¬íšŒ': 'ì‚¬íšŒ ì´ìŠˆ ì–´íœ˜',
                      'êµìœ¡': 'êµìœ¡ ê´€ë ¨ ì–´íœ˜',
                      'ê±´ê°•': 'ì˜ë£Œ ìš©ì–´',
                      'ê¸°ìˆ ': 'ê¸°ìˆ  ìš©ì–´',
                      'ë¬¸ë²•': 'ë¬¸ë²• êµ¬ì¡°'
                  }
                  goal = learning_goals.get(topic, 'í•µì‹¬ ì–´íœ˜')
                  
                  # ì¬ìƒì‹œê°„ì— ë”°ë¥¸ ì²­ì·¨ ê³„íš ì„¤ì •
                  if ':' in duration:
                      try:
                          minutes, seconds = duration.split(':')
                          total_minutes = int(minutes)
                          if total_minutes > 30:
                              listen_plan = f"(30ë¶„ ì²­ì·¨ ëª©í‘œ)"
                          elif total_minutes > 20:
                              listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
                          else:
                              listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
                      except:
                          listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
                  else:
                      listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
                  
                  return (f"ğŸ§ {podcast_name} Ep.{episode_num} - {weekday_name} ìŠ¤í˜ì¸ íŒŸìºìŠ¤íŠ¸ "
                         f"â±ï¸ ì¬ìƒì‹œê°„: {duration} {listen_plan} "
                         f"ğŸ¯ í•™ìŠµëª©í‘œ: {goal} 5ê°œ ì •ë¦¬ "
                         f"ğŸŒ ì£¼ì œ: {topic} "
                         f"ğŸ“ ê¶Œì¥: í•µì‹¬ ì–´íœ˜ì— ì§‘ì¤‘í•˜ì—¬ ì²­ì·¨")

          # ê¸°ì‚¬ ìˆ˜ì§‘ ë° ì‹¤ì œ ë‚´ìš© ë¶„ì„
          reading_source = "${{ steps.phase.outputs.reading_source }}"
          reading_difficulty = "${{ steps.phase.outputs.reading_difficulty }}"
          article_data = None

          try:
              if reading_source == "20minutos":
                  feed_url = "https://www.20minutos.es/rss/"
              elif "El PaÃ­s" in reading_source:
                  if "ì‚¬ì„¤" in reading_source:
                      feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/opinion"
                  else:
                      feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada"
              
              print(f"RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {feed_url}")
              feed = feedparser.parse(feed_url)
              
              if feed.entries:
                  latest = feed.entries[0]
                  article_url = latest.link
                  clean_title = latest.title.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
                  
                  print(f"ê¸°ì‚¬ URL ì ‘ì† ì¤‘: {article_url}")
                  # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
                  article_content = get_article_content(article_url)
                  
                  if article_content:
                      print(f"ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ ì¤‘... (ë‚´ìš© ê¸¸ì´: {len(article_content)}ì)")
                      # ì‹¤ì œ ë‚´ìš©ì—ì„œ ì–´íœ˜ ì¶”ì¶œ
                      vocabulary = extract_vocabulary_from_content(article_content, reading_difficulty)
                      category = extract_category_from_content(clean_title, article_content)
                      
                      print(f"ì¶”ì¶œëœ ì–´íœ˜: {vocabulary}")
                      print(f"ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")
                      
                      article_data = {
                          'title': clean_title,
                          'url': article_url,
                          'published': latest.get('published', ''),
                          'category': category,
                          'vocabulary': vocabulary,
                          'difficulty': reading_difficulty,
                          'content_preview': article_content[:200] + "..." if len(article_content) > 200 else article_content
                      }
                  else:
                      print("ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ RSS ìš”ì•½ ì‚¬ìš©")
                      # ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
                      summary = latest.get('summary', '')
                      vocabulary = extract_vocabulary_from_content(summary, reading_difficulty)
                      category = extract_category_from_content(clean_title, summary)
                      
                      article_data = {
                          'title': clean_title,
                          'url': article_url,
                          'published': latest.get('published', ''),
                          'category': category,
                          'vocabulary': vocabulary,
                          'difficulty': reading_difficulty,
                          'content_preview': summary
                      }
                      
              else:
                  print("RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                  
          except Exception as e:
              print(f"ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

          # íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
          podcast_rss = "${{ steps.phase.outputs.podcast_rss }}"
          podcast_name = "${{ steps.phase.outputs.podcast_name }}"
          weekday_name = "${{ steps.phase.outputs.weekday_name }}"
          podcast_data = None

          try:
              print(f"íŒŸìºìŠ¤íŠ¸ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {podcast_rss}")
              feed = feedparser.parse(podcast_rss)
              if feed.entries:
                  latest = feed.entries[0]
                  episode_number = extract_episode_number(latest.title)
                  duration = extract_duration_from_feed(latest)
                  topic = extract_topic_keywords(latest.title, latest.get('summary', ''))
                  
                  apple_base = "${{ steps.phase.outputs.podcast_apple_base }}"
                  episode_link = latest.link
                  
                  # Apple Podcasts ë§í¬ ìƒì„± ê°œì„ 
                  if 'npr.org' in episode_link or 'radioambulante' in episode_link: