#!/usr/bin/env python3
"""
LLM-based Spanish content analyzer using ChatGPT API.
ChatGPT APIë¥¼ ì‚¬ìš©í•œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ë¶„ì„ê¸°
"""

import os
import json
import requests
import time
import unicodedata
import html
from typing import List, Dict, Optional

class SpanishLLMAnalyzer:
    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize the LLM analyzer with ChatGPT API
        """
        self.api_key = api_key or os.environ.get('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable.")
        
        self.base_url = "https://api.openai.com/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def _make_api_call(self, prompt: str, max_tokens: int = 800) -> str:
        """
        Make API call to ChatGPT
        """
        try:
            payload = {
                "model": "gpt-4o-mini",  # ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ì‚¬ìš©
                "messages": [
                    {
                        "role": "system",
                        "content": "You are an expert Spanish language teacher and linguist specializing in analyzing Spanish content for language learners."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": max_tokens,
                "temperature": 0.3,  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
                "top_p": 0.9
            }
            
            response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
            
        except requests.exceptions.RequestException as e:
            print(f"API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return ""
        except Exception as e:
            print(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            return ""
    
    def analyze_podcast_colloquialisms(self, transcript: str, difficulty: str = "B2") -> List[str]:
        """
        Extract colloquial expressions from podcast transcript using LLM
        íŒŸìºìŠ¤íŠ¸ transcriptì—ì„œ êµ¬ì–´ì²´ í‘œí˜„ ì¶”ì¶œ
        """
        print(f"\n    ğŸ” êµ¬ì–´ì²´ ë¶„ì„ ì‹œì‘")
        print(f"    ğŸ“Š ì›ë³¸ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(transcript) if transcript else 0}ì")
        
        if not transcript:
            print(f"    âŒ ë¶„ì„ ì‹¤íŒ¨: transcriptê°€ ë¹„ì–´ìˆìŒ")
            return []
        
        # ì›ë³¸ í…ìŠ¤íŠ¸ ë‚´ìš© ë¶„ì„
        original_preview = transcript[:200].replace('\n', ' ').strip()
        print(f"    ğŸ“„ ì›ë³¸ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {original_preview}...")
        
        # ë©”íƒ€ë°ì´í„°ë§Œ ìˆëŠ”ì§€ í™•ì¸
        if self.is_metadata_only(transcript):
            print(f"    âš ï¸  ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨ëœ ì½˜í…ì¸ ë¡œ íŒë‹¨ë¨ - êµ¬ì–´ì²´ ë¶„ì„ ê±´ë„ˆë›°ê¸°")
            print(f"    ğŸ“ ë©”íƒ€ë°ì´í„° ìœ í˜•: ì œëª©, ì„¤ëª…, ê¸°ìˆ ì  ì •ë³´ë§Œ í¬í•¨")
            return []
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
        cleaned_transcript = self.clean_text(transcript)
        print(f"    ğŸ§¹ í…ìŠ¤íŠ¸ ì •ë¦¬ í›„ ê¸¸ì´: {len(cleaned_transcript)}ì")
        
        # ì´ì¤‘ ì–¸ì–´ ì½˜í…ì¸ ì—ì„œ ìŠ¤í˜ì¸ì–´ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        spanish_transcript = self.extract_spanish_content(cleaned_transcript)
        print(f"    ğŸ‡ªğŸ‡¸ ìŠ¤í˜ì¸ì–´ ì¶”ì¶œ í›„ ê¸¸ì´: {len(spanish_transcript)}ì")
        
        # ì •ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if len(spanish_transcript.strip()) < 50:
            print(f"    âŒ ë¶„ì„ ì‹¤íŒ¨: ì •ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŒ ({len(spanish_transcript.strip())}ì)")
            print(f"    ğŸ“ ì‹¤íŒ¨ ì›ì¸: ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ì¶”ì¶œ í›„ ì˜ë¯¸ìˆëŠ” ë‚´ìš©ì´ ë¶€ì¡±")
            return []
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 2000ìë§Œ ì‚¬ìš©
        if len(spanish_transcript) > 2000:
            spanish_transcript = spanish_transcript[:2000] + "..."
            print(f"    âœ‚ï¸  í…ìŠ¤íŠ¸ ê¸¸ì´ ì¡°ì •: 2000ìë¡œ ì œí•œ")
        
        # LLM ë¶„ì„ì— ì‚¬ìš©ë  ì‹¤ì œ í…ìŠ¤íŠ¸ ìƒì„¸ ë¡œê¹…
        final_preview = spanish_transcript[:300].replace('\n', ' ').strip()
        print(f"    ğŸ¤– LLM ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {final_preview}...")
        print(f"    ğŸ“ LLM ë¶„ì„ ëŒ€ìƒ í…ìŠ¤íŠ¸ ê¸¸ì´: {len(spanish_transcript)}ì")
        
        # í…ìŠ¤íŠ¸ ìœ í˜• ë¶„ì„
        text_type = self.analyze_text_type(spanish_transcript)
        print(f"    ğŸ“‹ í…ìŠ¤íŠ¸ ìœ í˜• ë¶„ì„: {text_type}")
        
        # êµ¬ì–´ì²´ í‘œí˜„ ê°€ëŠ¥ì„± ì˜ˆì¸¡
        colloquial_likelihood = self.predict_colloquial_likelihood(spanish_transcript)
        print(f"    ğŸ¯ êµ¬ì–´ì²´ í‘œí˜„ ë°œê²¬ ê°€ëŠ¥ì„±: {colloquial_likelihood}")
        
        prompt = f"""
You are analyzing Spanish text to find ACTUAL colloquial expressions that appear in the text.

CRITICAL RULE: Only extract expressions that are ACTUALLY PRESENT in the provided text. Do not suggest or create expressions that are not in the text.

Text to analyze:
{spanish_transcript}

Instructions:
1. Read the text carefully and identify any words, phrases, or expressions that show informal/conversational Spanish
2. Look for language that is characteristic of spoken Spanish rather than formal written Spanish
3. Find expressions that native speakers would use in casual conversation, everyday situations, or informal contexts
4. Include informal vocabulary, conversational connectors, colloquial phrases, everyday expressions, and any language that shows a relaxed, natural speaking style
5. If the text is formal and contains no colloquial expressions, return "NO_COLLOQUIAL_EXPRESSIONS_FOUND"

What makes an expression colloquial:
- Words or phrases commonly used in everyday conversation
- Informal alternatives to more formal expressions
- Conversational fillers, connectors, or transitions
- Casual ways of expressing opinions, emotions, or reactions
- Language that sounds natural and spontaneous rather than scripted or formal

Response format (only if expressions are found):
- "expression" â†’ Korean meaning

If no colloquial expressions are found in this formal text, respond with: NO_COLLOQUIAL_EXPRESSIONS_FOUND
"""
        
        response = self._make_api_call(prompt, max_tokens=400)
        
        print(f"    ğŸ¤– LLM ì‘ë‹µ ë°›ìŒ: {len(response) if response else 0}ì")
        
        if not response:
            print(f"    âŒ LLM ì‘ë‹µ ì‹¤íŒ¨ - API í˜¸ì¶œ ì˜¤ë¥˜")
            return []
        
        # LLM ì‘ë‹µ ë‚´ìš© ë¡œê¹…
        response_preview = response[:200].replace('\n', ' ').strip()
        print(f"    ğŸ“„ LLM ì‘ë‹µ ë¯¸ë¦¬ë³´ê¸°: {response_preview}...")
        
        # "NO_COLLOQUIAL_EXPRESSIONS_FOUND" ì‘ë‹µ ì²˜ë¦¬
        if "NO_COLLOQUIAL_EXPRESSIONS_FOUND" in response:
            print(f"    âœ… LLM ë¶„ì„ ì™„ë£Œ: í…ìŠ¤íŠ¸ì— êµ¬ì–´ì²´ í‘œí˜„ì´ ì—†ìŒì„ í™•ì¸")
            print(f"    ğŸ“ ë¶„ì„ ê²°ê³¼: ì œê³µëœ í…ìŠ¤íŠ¸ê°€ ì •ì‹/ê³µì‹ì  ì–¸ì–´ë¡œ êµ¬ì„±ë¨")
            return []
        
        # ì‘ë‹µì—ì„œ í‘œí˜„ë“¤ ì¶”ì¶œ
        expressions = []
        lines = response.split('\n')
        
        print(f"    ğŸ” êµ¬ì–´ì²´ í‘œí˜„ íŒŒì‹± ì‹œì‘: {len(lines)}ê°œ ë¼ì¸ ë¶„ì„")
        
        for i, line in enumerate(lines):
            line = line.strip()
            print(f"    ğŸ“ ë¼ì¸ {i+1} ë¶„ì„: {line}")
            
            # ë‹¤ì–‘í•œ í˜•ì‹ ì§€ì›: 
            # 1. "expression" â†’ meaning (context)
            # 2. - "expression" â†’ meaning (context)  
            # 3. expression â†’ meaning
            if '"' in line and 'â†’' in line:
                try:
                    # ë”°ì˜´í‘œë¡œ ë¬¶ì¸ í‘œí˜„ ì¶”ì¶œ
                    start_quote = line.find('"')
                    end_quote = line.find('"', start_quote + 1)
                    if start_quote != -1 and end_quote != -1:
                        expression = line[start_quote+1:end_quote]
                        remaining = line[end_quote+1:]
                        if 'â†’' in remaining:
                            meaning_part = remaining.split('â†’')[1].strip()
                            # (usage context) ë¶€ë¶„ ì œê±°
                            if '(' in meaning_part:
                                meaning = meaning_part.split('(')[0].strip()
                            else:
                                meaning = meaning_part.strip()
                            
                            full_expression = f"{expression} ({meaning})"
                            expressions.append(full_expression)
                            print(f"    âœ… êµ¬ì–´ì²´ í‘œí˜„ ë°œê²¬: {full_expression}")
                        else:
                            print(f"    âš ï¸  â†’ ê¸°í˜¸ ì—†ìŒ")
                    else:
                        print(f"    âš ï¸  ë”°ì˜´í‘œ ìŒì´ ë§ì§€ ì•ŠìŒ")
                except Exception as e:
                    print(f"    âš ï¸  êµ¬ì–´ì²´ í‘œí˜„ íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {i+1}): {e}")
                    print(f"    ğŸ“ ë¬¸ì œ ë¼ì¸: {line}")
                    continue
            elif 'â†’' in line and not line.startswith('#') and line.strip():
                # ë”°ì˜´í‘œ ì—†ì´ â†’ ë§Œ ìˆëŠ” ê²½ìš°ë„ ì²˜ë¦¬
                try:
                    parts = line.split('â†’')
                    if len(parts) >= 2:
                        expression_part = parts[0].strip()
                        meaning_part = parts[1].strip()
                        
                        # ì•ì˜ ë¶ˆí•„ìš”í•œ ê¸°í˜¸ ì œê±° (-, *, ë“±)
                        expression_part = expression_part.lstrip('- *â€¢').strip()
                        
                        # (usage context) ë¶€ë¶„ ì œê±°
                        if '(' in meaning_part:
                            meaning = meaning_part.split('(')[0].strip()
                        else:
                            meaning = meaning_part.strip()
                        
                        if expression_part and meaning:
                            full_expression = f"{expression_part} ({meaning})"
                            expressions.append(full_expression)
                            print(f"    âœ… êµ¬ì–´ì²´ í‘œí˜„ ë°œê²¬: {full_expression}")
                except Exception as e:
                    print(f"    âš ï¸  ëŒ€ì•ˆ íŒŒì‹± ì˜¤ë¥˜ (ë¼ì¸ {i+1}): {e}")
                    continue
            else:
                print(f"    âš ï¸  êµ¬ì–´ì²´ í‘œí˜„ í˜•ì‹ì´ ì•„ë‹˜")
                continue
        
        print(f"    ğŸ“Š ìµœì¢… êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜: {len(expressions)}ê°œ")
        
        if len(expressions) == 0:
            print(f"    ğŸ¤” êµ¬ì–´ì²´ í‘œí˜„ì´ 0ê°œì¸ ì´ìœ  ë¶„ì„:")
            print(f"       â€¢ LLMì´ í…ìŠ¤íŠ¸ë¥¼ ì •ì‹/ê³µì‹ì  ì–¸ì–´ë¡œ íŒë‹¨")
            print(f"       â€¢ í…ìŠ¤íŠ¸ì— ëŒ€í™”ì²´/ë¹„ê³µì‹ì  í‘œí˜„ì´ ì‹¤ì œë¡œ ì—†ìŒ")
            print(f"       â€¢ ë©”íƒ€ë°ì´í„°ë‚˜ ì„¤ëª…ë¬¸ ìœ„ì£¼ì˜ ë‚´ìš©ì¼ ê°€ëŠ¥ì„±")
            
        return expressions[:5]  # ìµœëŒ€ 5ê°œ ë°˜í™˜
    
    def analyze_article_grammar(self, article_content: str, difficulty: str = "B2") -> Dict[str, any]:
        """
        Analyze grammar structures in article content using LLM
        ê¸°ì‚¬ ë‚´ìš©ì˜ ë¬¸ë²• êµ¬ì¡° ë¶„ì„
        """
        if not article_content:
            return {"original_sentence": "", "grammar_analysis": []}
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 1500ìë§Œ ì‚¬ìš©
        if len(article_content) > 1500:
            article_content = article_content[:1500] + "..."
        
        prompt = f"""
Analyze this Spanish article and identify 1 representative sentence for grammar analysis at {difficulty} level.

Select the most complex and educational sentence from the text that contains multiple grammar structures suitable for {difficulty} level learners.

Article content:
{article_content}

Please provide the analysis in this exact format:

ORIGINAL_SENTENCE: [exact sentence from text]

GRAMMAR_ANALYSIS:
**ë¬¸ë²• êµ¬ì¡° 1 (ìˆ˜ì¤€)**
- ì„¤ëª… 1
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 2 (ìˆ˜ì¤€)**  
- ì„¤ëª… 1
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 3 (ìˆ˜ì¤€)**
- ì„¤ëª… 1
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 4 (ìˆ˜ì¤€)**
- ì„¤ëª… 1 
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 5 (ìˆ˜ì¤€)**
- ì„¤ëª… 1
- ì„¤ëª… 2

Focus on grammar structures appropriate for {difficulty} level such as:
- B1: í˜„ì¬/ê³¼ê±° ì‹œì œ, ser vs estar, ì¬ê·€ë™ì‚¬
- B2: ì ‘ì†ë²• í˜„ì¬/ê³¼ê±°, ì¡°ê±´ë²•, ì™„ë£Œ ì‹œì œ  
- C1: ì ‘ì†ë²• ì™„ë£Œ, ë³µí•© ì¡°ê±´ë¬¸, ìˆ˜ë™íƒœ

Provide detailed explanations for each grammar point including specific words from the sentence.
"""
        
        response = self._make_api_call(prompt, max_tokens=800)
        
        if not response:
            return {"original_sentence": "", "grammar_analysis": []}
        
        # ì‘ë‹µì—ì„œ ì›ë¬¸ ë¬¸ì¥ê³¼ ë¬¸ë²• ë¶„ì„ ì¶”ì¶œ
        original_sentence = ""
        grammar_analysis = []
        
        lines = response.split('\n')
        current_section = ""
        current_grammar = ""
        current_points = []
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('ORIGINAL_SENTENCE:'):
                original_sentence = line.replace('ORIGINAL_SENTENCE:', '').strip()
            elif line.startswith('GRAMMAR_ANALYSIS:'):
                current_section = "grammar"
            elif line.startswith('**') and line.endswith('**'):
                # ì´ì „ ë¬¸ë²• êµ¬ì¡° ì €ì¥
                if current_grammar and current_points:
                    grammar_analysis.append({
                        "title": current_grammar,
                        "points": current_points
                    })
                # ìƒˆë¡œìš´ ë¬¸ë²• êµ¬ì¡° ì‹œì‘
                current_grammar = line.replace('**', '').strip()
                current_points = []
            elif line.startswith('- ') and current_section == "grammar":
                current_points.append(line[2:].strip())
        
        # ë§ˆì§€ë§‰ ë¬¸ë²• êµ¬ì¡° ì €ì¥
        if current_grammar and current_points:
            grammar_analysis.append({
                "title": current_grammar,
                "points": current_points
            })
        
        return {
            "original_sentence": original_sentence,
            "grammar_analysis": grammar_analysis
        }
        
        return {
            "original_sentence": original_sentence,
            "grammar_analysis": grammar_analysis
        }
    
    def analyze_text_difficulty(self, content: str) -> str:
        """
        Analyze text difficulty using LLM
        LLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„
        """
        if not content:
            return "B2"
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 1000ìë§Œ ì‚¬ìš©
        if len(content) > 1000:
            content = content[:1000] + "..."
        
        prompt = f"""
Analyze this Spanish text and determine its CEFR difficulty level (A1, A2, B1, B1+, B2, B2+, C1, C2).

Consider:
- Vocabulary complexity
- Grammar structures used
- Sentence length and complexity
- Abstract vs concrete concepts
- Technical vs everyday language

Text:
{content}

Respond with only the CEFR level (e.g., "B2" or "B2+" or "C1"), no additional text.
"""
        
        response = self._make_api_call(prompt, max_tokens=50)
        
        # ì‘ë‹µì—ì„œ ë ˆë²¨ ì¶”ì¶œ
        if response:
            response = response.strip().upper()
            valid_levels = ['A1', 'A2', 'B1', 'B1+', 'B2', 'B2+', 'C1', 'C2']
            for level in valid_levels:
                if level in response:
                    return level
        
        return "B2"  # ê¸°ë³¸ê°’
    
    def clean_text(self, text: str) -> str:
        """
        Clean and normalize text for better LLM analysis
        í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì •ê·œí™”
        """
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = html.unescape(text)
        
        # ì˜ëª»ëœ ì¸ì½”ë”© ìˆ˜ì •
        replacements = {
            'ÃƒÂ±': 'Ã±',  # EspaÃƒÂ±ol -> EspaÃ±ol
            'ÃƒÂ¡': 'Ã¡',
            'ÃƒÂ©': 'Ã©',
            'ÃƒÂ­': 'Ã­',
            'ÃƒÂ³': 'Ã³',
            'ÃƒÂº': 'Ãº',
            'Ãƒ ': 'Ã ',
            'ÃƒÂ¨': 'Ã¨',
            'ÃƒÂ¬': 'Ã¬',
            'ÃƒÂ²': 'Ã²',
            'ÃƒÂ¹': 'Ã¹',
            'ÃƒÂ¼': 'Ã¼',
            'Ãƒ': 'Ã‘',
            'Ã‚': '',  # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
            '\xa0': ' ',  # non-breaking space
            '\u2028': '\n',  # line separator
            '\u2029': '\n\n'  # paragraph separator
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
        text = ' '.join(text.split())
        
        return text
    
    def generate_podcast_learning_goals(self, content: str, title: str, difficulty: str = "B2", colloquial_count: int = 0) -> List[str]:
        """
        íŒŸìºìŠ¤íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í•™ìŠµ ëª©í‘œë¥¼ ìƒì„±
        
        Args:
            content: íŒŸìºìŠ¤íŠ¸ ë‚´ìš© (ë©”ëª¨/ìŠ¤í¬ë¦½íŠ¸)
            title: íŒŸìºìŠ¤íŠ¸ ì œëª©
            difficulty: í•™ìŠµì ìˆ˜ì¤€ (B1, B2, C1 ë“±)
            colloquial_count: ë¶„ì„ëœ êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜
            
        Returns:
            List of learning goals
        """
        print(f"    ğŸ¯ íŒŸìºìŠ¤íŠ¸ í•™ìŠµ ëª©í‘œ ìƒì„± ì¤‘... (ë‚œì´ë„: {difficulty}, êµ¬ì–´ì²´ í‘œí˜„: {colloquial_count}ê°œ)")
        
        # ë‚´ìš© ì •ë¦¬
        clean_content = self.clean_text(content)
        
        # êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜ì— ë”°ë¥¸ ëª©í‘œ ì¡°ì •
        if colloquial_count == 0:
            # êµ¬ì–´ì²´ í‘œí˜„ì´ 0ê°œì¸ ê²½ìš° ë‹¤ë¥¸ í•™ìŠµ ëª©í‘œì— ì§‘ì¤‘
            prompt = f"""
Analyze this Spanish podcast episode and generate 3-4 specific, actionable learning goals for a {difficulty} level Spanish learner.

PODCAST TITLE: {title}
PODCAST CONTENT: {clean_content[:1500]}
COLLOQUIAL EXPRESSIONS FOUND: 0 expressions (focus on other learning aspects)

Since no colloquial expressions were found in the analysis, focus on these alternative learning aspects:
1. Main topic/theme comprehension and vocabulary
2. Formal/academic language structures present
3. Cultural and contextual learning opportunities
4. Advanced listening comprehension strategies
5. Grammar patterns and sentence structures
6. Register and tone analysis

Create learning goals that are:
- Specific and measurable
- Appropriate for {difficulty} level
- Focus on formal language skills and comprehension strategies
- Encourage deep content understanding
- Be realistic about vocabulary numbers (5-10 key words, not excessive amounts)
- EXCLUDE colloquial expression analysis since none were found

Format your response as a numbered list in Korean:
1. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 1]
2. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 2]  
3. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 3]
(4. [ì¶”ê°€ ëª©í‘œ if relevant])

Examples of good learning goals for formal content:
- ì—í”¼ì†Œë“œ ì£¼ì œì™€ ê´€ë ¨ëœ ì „ë¬¸ ì–´íœ˜ ë° í‘œí˜„ 5-8ê°œ í•™ìŠµ
- í™”ìì˜ ë…¼ë¦¬ì  êµ¬ì¡°ì™€ ì£¼ì¥ ì „ê°œ ë°©ì‹ íŒŒì•…
- ìŠ¤í˜ì¸ì–´ ì •ì¹˜/ì‚¬íšŒì  ë§¥ë½ê³¼ ë¬¸í™”ì  ë°°ê²½ ì´í•´
- ë³µí•©ë¬¸ê³¼ ê³ ê¸‰ ë¬¸ë²• êµ¬ì¡° ë¶„ì„ ë° í•™ìŠµ
"""
        else:
            # êµ¬ì–´ì²´ í‘œí˜„ì´ ìˆëŠ” ê²½ìš° ê¸°ì¡´ ë¡œì§
            prompt = f"""
Analyze this Spanish podcast episode and generate 3-4 specific, actionable learning goals for a {difficulty} level Spanish learner.

PODCAST TITLE: {title}
PODCAST CONTENT: {clean_content[:1500]}
COLLOQUIAL EXPRESSIONS FOUND: {colloquial_count} expressions

Consider the following aspects when creating learning goals:
1. Main topic/theme of the episode
2. Vocabulary focus areas (suggest realistic number based on content length)
3. Grammar structures present
4. Cultural/contextual learning opportunities
5. Listening comprehension skills
6. Colloquial expressions: exactly {colloquial_count} expressions were found in the analysis

Create learning goals that are:
- Specific and measurable
- Appropriate for {difficulty} level
- Reflect the ACTUAL analysis results (e.g., if {colloquial_count} colloquial expressions were found, mention that exact number)
- Focus on practical language skills
- Encourage active listening and engagement
- Be realistic about vocabulary numbers (5-10 key words, not excessive amounts)

Format your response as a numbered list in Korean:
1. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 1]
2. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 2]  
3. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 3]
(4. [ì¶”ê°€ ëª©í‘œ if relevant])

Make sure to reference the actual number of colloquial expressions found: {colloquial_count}

Examples of good learning goals:
- ì—í”¼ì†Œë“œ ì£¼ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ì–´íœ˜ 5-8ê°œ í•™ìŠµ ë° í™œìš©
- í™”ìì˜ ê°ì • í‘œí˜„ ë°©ì‹ê³¼ ì–µì–‘ íŒ¨í„´ íŒŒì•…  
- ë¶„ì„ëœ êµ¬ì–´ì²´ í‘œí˜„ {colloquial_count}ê°œë¥¼ ì •ë¦¬í•˜ê³  ì¼ìƒ ëŒ€í™”ì—ì„œ í™œìš©í•˜ê¸°
- ìŠ¤í˜ì¸ì–´ ë¬¸í™”ì  ë§¥ë½ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê´€ìš© í‘œí˜„ ì´í•´
"""
        
        response = self._make_api_call(prompt, max_tokens=600)
        
        if not response:
            # ê¸°ë³¸ ëª©í‘œ ë°˜í™˜ (êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜ì— ë”°ë¼ ë‹¤ë¥¸ ëª©í‘œ)
            if colloquial_count > 0:
                return [
                    f"íŒŸìºìŠ¤íŠ¸ ì£¼ì œ ê´€ë ¨ í•µì‹¬ ì–´íœ˜ 5-7ê°œ í•™ìŠµ ({difficulty} ìˆ˜ì¤€)",
                    f"ë¶„ì„ëœ êµ¬ì–´ì²´ í‘œí˜„ {colloquial_count}ê°œ ì •ë¦¬ ë° í™œìš©",
                    "ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒê³¼ ì–µì–‘ íŒ¨í„´ í•™ìŠµ"
                ]
            else:
                # êµ¬ì–´ì²´ í‘œí˜„ì´ 0ê°œì¸ ê²½ìš° ë‹¤ë¥¸ í•™ìŠµ ëª©í‘œ ì œê³µ
                return [
                    f"íŒŸìºìŠ¤íŠ¸ ì£¼ì œ ê´€ë ¨ ì „ë¬¸ ì–´íœ˜ ë° í‘œí˜„ 5-7ê°œ í•™ìŠµ ({difficulty} ìˆ˜ì¤€)",
                    "í™”ìì˜ ë…¼ë¦¬ì  êµ¬ì¡°ì™€ ì£¼ì¥ ì „ê°œ ë°©ì‹ íŒŒì•…",
                    "ìŠ¤í˜ì¸ì–´ ì •ì¹˜/ì‚¬íšŒì  ë§¥ë½ê³¼ ë¬¸í™”ì  ë°°ê²½ ì´í•´"
                ]
        
        # ì‘ë‹µì—ì„œ ëª©í‘œë“¤ ì¶”ì¶œ
        goals = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì°¾ê¸° (1. 2. 3. í˜•íƒœ)
            if line and (line[0].isdigit() or line.startswith('â€¢') or line.startswith('-')):
                # ë²ˆí˜¸ ë¶€ë¶„ ì œê±°
                if '. ' in line:
                    goal = line.split('. ', 1)[1].strip()
                elif line.startswith('â€¢ '):
                    goal = line[2:].strip()
                elif line.startswith('- '):
                    goal = line[2:].strip()
                else:
                    goal = line.strip()
                
                if goal and len(goal) > 10:  # ë„ˆë¬´ ì§§ì€ ëª©í‘œëŠ” ì œì™¸
                    goals.append(goal)
        
        # 3-4ê°œ ëª©í‘œ ë°˜í™˜
        if goals:
            return goals[:4]
        else:
            # êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜ì— ë”°ë¥¸ fallback ëª©í‘œ
            if colloquial_count > 0:
                return [
                    f"íŒŸìºìŠ¤íŠ¸ ì£¼ì œ ê´€ë ¨ í•µì‹¬ ì–´íœ˜ í•™ìŠµ ({difficulty} ìˆ˜ì¤€)",
                    f"ë¶„ì„ëœ êµ¬ì–´ì²´ í‘œí˜„ {colloquial_count}ê°œ ì •ë¦¬ ë° í™œìš©",
                    "ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒê³¼ ì–µì–‘ íŒ¨í„´ í•™ìŠµ"
                ]
            else:
                return [
                    f"íŒŸìºìŠ¤íŠ¸ ì£¼ì œ ê´€ë ¨ ì „ë¬¸ ì–´íœ˜ í•™ìŠµ ({difficulty} ìˆ˜ì¤€)",
                    "í™”ìì˜ ë…¼ë¦¬ì  êµ¬ì¡°ì™€ ì£¼ì¥ ì „ê°œ ë°©ì‹ íŒŒì•…",
                    "ìŠ¤í˜ì¸ì–´ ì •ì¹˜/ì‚¬íšŒì  ë§¥ë½ê³¼ ë¬¸í™”ì  ë°°ê²½ ì´í•´"
                ]
    
    def simple_language_detection(self, content: str) -> str:
        """
        Simple language detection using LLM - returns SPANISH or ENGLISH
        LLMì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ì–¸ì–´ ê²€ì¦
        """
        if not content:
            return "UNKNOWN"
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        content = self.clean_text(content)
        
        # ë„ˆë¬´ ê¸´ ê²½ìš° ì²˜ìŒ ë¶€ë¶„ë§Œ ì‚¬ìš©
        if len(content) > 1000:
            content = content[:1000] + "..."
        
        prompt = f"""
Analyze the following text and determine if it's primarily in Spanish or English.

Text to analyze:
{content}

Instructions:
1. Analyze the language of the text
2. Respond with ONLY ONE WORD: either "SPANISH" or "ENGLISH"
3. Do not provide any explanation or additional text
"""
        
        try:
            response = self._make_api_call(prompt, max_tokens=10)
            response = response.strip().upper()
            
            if "SPANISH" in response:
                return "SPANISH"
            elif "ENGLISH" in response:
                return "ENGLISH"
            else:
                return "UNKNOWN"
                
        except Exception as e:
            print(f"ì–¸ì–´ ê²€ì¦ ì˜¤ë¥˜: {e}")
            return "UNKNOWN"
    
    def extract_spanish_content(self, text: str) -> str:
        """
        ì´ì¤‘ ì–¸ì–´ ì½˜í…ì¸ ì—ì„œ ìŠ¤í˜ì¸ì–´ ë¶€ë¶„ë§Œ ì¶”ì¶œ
        ì˜ì–´ì™€ ìŠ¤í˜ì¸ì–´ê°€ ì„ì¸ íŒŸìºìŠ¤íŠ¸ì—ì„œ ìŠ¤í˜ì¸ì–´ ë¶€ë¶„ë§Œ ë¶„ì„í•˜ë„ë¡ í•¨
        """
        if not text:
            return ""
        
        # ìŠ¤í˜ì¸ì–´ íŠ¹ì§•ì ì¸ ë‹¨ì–´ë“¤
        spanish_indicators = [
            'hola', 'queridos', 'amigos', 'bienvenidos', 'espaÃ±ol', 'episodio',
            'soy', 'desde', 'barcelona', 'reflexionamos', 'situaciÃ³n', 'dramÃ¡tica',
            'mundo', 'entero', 'pandemia', 'coronavirus', 'que', 'estÃ¡', 'pasando',
            'nuestro', 'sobre', 'viviendo', 'raÃ­z', 'del'
        ]
        
        # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = text.split('.')
        spanish_sentences = []
        
        for sentence in sentences:
            sentence = sentence.strip()
            if len(sentence) < 10:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì¥ ì œì™¸
                continue
            
            # ê° ë¬¸ì¥ì—ì„œ ìŠ¤í˜ì¸ì–´ íŠ¹ì§• ë‹¨ì–´ ê°œìˆ˜ ì„¸ê¸°
            spanish_word_count = 0
            words = sentence.lower().split()
            
            for word in words:
                # êµ¬ë‘ì  ì œê±°
                clean_word = word.strip('.,!?";:()[]')
                if clean_word in spanish_indicators:
                    spanish_word_count += 1
            
            # ìŠ¤í˜ì¸ì–´ íŠ¹ì§• ë‹¨ì–´ê°€ 2ê°œ ì´ìƒì´ë©´ ìŠ¤í˜ì¸ì–´ ë¬¸ì¥ìœ¼ë¡œ ê°„ì£¼
            if spanish_word_count >= 2:
                spanish_sentences.append(sentence)
            # íŠ¹ì§• ë‹¨ì–´ê°€ ì ì–´ë„ ìŠ¤í˜ì¸ì–´ ë¬¸ìê°€ ìˆìœ¼ë©´ í¬í•¨
            elif any(char in sentence for char in 'Ã±Ã¡Ã©Ã­Ã³ÃºÃ¼Â¿Â¡'):
                spanish_sentences.append(sentence)
        
        # ìŠ¤í˜ì¸ì–´ ë¬¸ì¥ë“¤ì„ ë‹¤ì‹œ í•©ì¹˜ê¸°
        spanish_content = '. '.join(spanish_sentences)
        
        # ë§Œì•½ ì¶”ì¶œëœ ë‚´ìš©ì´ ë„ˆë¬´ ì ìœ¼ë©´ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©
        if len(spanish_content.strip()) < 100:
            print(f"    ğŸ“ ìŠ¤í˜ì¸ì–´ ì¶”ì¶œ ê²°ê³¼ê°€ ë¶€ì¡±í•´ì„œ ì›ë³¸ í…ìŠ¤íŠ¸ ì‚¬ìš©")
            return text
        
        print(f"    ğŸ“ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ì¶”ì¶œ ì™„ë£Œ: {len(spanish_content)}ì")
        return spanish_content
    
    def is_metadata_only(self, text: str) -> bool:
        """
        í…ìŠ¤íŠ¸ê°€ ë©”íƒ€ë°ì´í„°ë§Œ í¬í•¨í•˜ê³  ìˆëŠ”ì§€ í™•ì¸
        ì‹¤ì œ ëŒ€í™”ë‚˜ ë‚´ìš©ì´ ì•„ë‹Œ ì œëª©, ì„¤ëª…, ê¸°ìˆ ì  ì •ë³´ë§Œ ìˆëŠ”ì§€ íŒë‹¨
        """
        if not text or len(text.strip()) < 50:
            return True
        
        text_lower = text.lower()
        
        # ë©”íƒ€ë°ì´í„° íŠ¹ì§• í‚¤ì›Œë“œë“¤
        metadata_indicators = [
            'podcast', 'episodio', 'episode', 'title', 'description',
            'duration', 'fecha', 'date', 'published', 'autor', 'author',
            'categoria', 'category', 'tags', 'subscribe', 'suscribirse',
            'web:', 'website:', 'email:', 'twitter:', 'instagram:',
            'available on', 'disponible en', 'spotify', 'apple podcasts',
            'google podcasts', 'rss feed', 'feed rss'
        ]
        
        # ì‹¤ì œ ë‚´ìš© íŠ¹ì§• í‚¤ì›Œë“œë“¤
        content_indicators = [
            'hola', 'bienvenidos', 'hoy vamos', 'en este episodio',
            'quiero hablar', 'vamos a ver', 'como ya sabes',
            'bueno', 'entonces', 'por ejemplo', 'ademÃ¡s', 'tambiÃ©n'
        ]
        
        metadata_count = sum(1 for indicator in metadata_indicators if indicator in text_lower)
        content_count = sum(1 for indicator in content_indicators if indicator in text_lower)
        
        # ë©”íƒ€ë°ì´í„° íŠ¹ì§•ì´ ë§ê³  ì‹¤ì œ ë‚´ìš© íŠ¹ì§•ì´ ì ìœ¼ë©´ ë©”íƒ€ë°ì´í„°ë¡œ íŒë‹¨
        return metadata_count >= 3 and content_count <= 1
    
    def analyze_text_type(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ ìœ í˜•ì„ ë¶„ì„í•˜ì—¬ êµ¬ì–´ì²´ í‘œí˜„ ê°€ëŠ¥ì„±ì„ ì˜ˆì¸¡
        """
        if not text:
            return "ë¹ˆ í…ìŠ¤íŠ¸"
        
        text_lower = text.lower()
        
        # ëŒ€í™”ì²´ íŠ¹ì§•
        conversational_features = [
            'hola', 'bueno', 'pues', 'entonces', 'o sea', 'sabes',
            'verdad', 'claro', 'por cierto', 'a ver', 'vamos'
        ]
        
        # ì •ì‹/ê³µì‹ íŠ¹ì§•
        formal_features = [
            'segÃºn', 'mediante', 'por tanto', 'sin embargo', 'ademÃ¡s',
            'asimismo', 'por consiguiente', 'en consecuencia', 'no obstante'
        ]
        
        # ì„¤ëª…ë¬¸ íŠ¹ì§•
        descriptive_features = [
            'descripciÃ³n', 'resumen', 'tema', 'sobre', 'acerca de',
            'informaciÃ³n', 'datos', 'estadÃ­sticas'
        ]
        
        conv_score = sum(1 for feature in conversational_features if feature in text_lower)
        formal_score = sum(1 for feature in formal_features if feature in text_lower)
        desc_score = sum(1 for feature in descriptive_features if feature in text_lower)
        
        if conv_score >= 3:
            return "ëŒ€í™”ì²´/ë¹„ê³µì‹ (êµ¬ì–´ì²´ í‘œí˜„ ê°€ëŠ¥ì„± ë†’ìŒ)"
        elif formal_score >= 2:
            return "ì •ì‹/ê³µì‹ì  (êµ¬ì–´ì²´ í‘œí˜„ ê°€ëŠ¥ì„± ë‚®ìŒ)"
        elif desc_score >= 2:
            return "ì„¤ëª…ë¬¸/ë©”íƒ€ë°ì´í„° (êµ¬ì–´ì²´ í‘œí˜„ ê°€ëŠ¥ì„± ë§¤ìš° ë‚®ìŒ)"
        else:
            return "í˜¼í•©í˜• (êµ¬ì–´ì²´ í‘œí˜„ ê°€ëŠ¥ì„± ë³´í†µ)"
    
    def predict_colloquial_likelihood(self, text: str) -> str:
        """
        í…ìŠ¤íŠ¸ì—ì„œ êµ¬ì–´ì²´ í‘œí˜„ì´ ë°œê²¬ë  ê°€ëŠ¥ì„±ì„ ì˜ˆì¸¡
        """
        if not text:
            return "ì—†ìŒ (ë¹ˆ í…ìŠ¤íŠ¸)"
        
        text_lower = text.lower()
        
        # êµ¬ì–´ì²´ í‘œí˜„ ì§€í‘œë“¤
        colloquial_indicators = [
            'bueno', 'pues', 'entonces', 'o sea', 'sabes', 'verdad',
            'claro', 'por cierto', 'a ver', 'vamos', 'oye', 'mira',
            'que tal', 'como va', 'vale', 'estÃ¡ bien', 'de acuerdo'
        ]
        
        # ì§ˆë¬¸ í˜•íƒœ (êµ¬ì–´ì²´ì—ì„œ í”í•¨)
        question_patterns = ['Â¿', '?', 'quÃ©', 'cÃ³mo', 'dÃ³nde', 'cuÃ¡ndo', 'por quÃ©']
        
        # ê°íƒ„ì‚¬ë‚˜ ê°„íˆ¬ì‚¬
        interjections = ['Â¡', '!', 'oh', 'ah', 'eh', 'uf', 'ay']
        
        colloquial_score = sum(1 for indicator in colloquial_indicators if indicator in text_lower)
        question_score = sum(1 for pattern in question_patterns if pattern in text_lower)
        interjection_score = sum(1 for interjection in interjections if interjection in text_lower)
        
        total_score = colloquial_score + question_score + interjection_score
        
        if total_score >= 5:
            return "ë†’ìŒ (5+ ì§€í‘œ)"
        elif total_score >= 3:
            return "ë³´í†µ (3-4 ì§€í‘œ)"
        elif total_score >= 1:
            return "ë‚®ìŒ (1-2 ì§€í‘œ)"
        else:
            return "ë§¤ìš° ë‚®ìŒ (0 ì§€í‘œ)"