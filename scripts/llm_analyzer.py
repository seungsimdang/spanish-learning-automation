#!/usr/bin/env python3
"""
LLM-based Spanish content analyzer using ChatGPT API.
ChatGPT APIë¥¼ ì‚¬ìš©í•œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ë¶„ì„ê¸°
"""

import os
import json
import requests
import time
import unicodedata
import html
from typing import List, Dict, Optional

class SpanishLLMAnalyzer:
    def __init__(self, api_key: Optional[str] = None):
        """
        Initialize the LLM analyzer with ChatGPT API
        """
        self.api_key = api_key or os.environ.get('OPENAI_API_KEY')
        if not self.api_key:
            raise ValueError("OpenAI API key is required. Set OPENAI_API_KEY environment variable.")
        
        self.base_url = "https://api.openai.com/v1/chat/completions"
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def _make_api_call(self, prompt: str, max_tokens: int = 800) -> str:
        """
        Make API call to ChatGPT
        """
        try:
            payload = {
                "model": "gpt-4o-mini",  # ë¹„ìš© íš¨ìœ¨ì ì¸ ëª¨ë¸ ì‚¬ìš©
                "messages": [
                    {
                        "role": "system",
                        "content": "You are an expert Spanish language teacher and linguist specializing in analyzing Spanish content for language learners."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "max_tokens": max_tokens,
                "temperature": 0.3,  # ì¼ê´€ì„± ìˆëŠ” ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ temperature
                "top_p": 0.9
            }
            
            response = requests.post(self.base_url, headers=self.headers, json=payload, timeout=30)
            response.raise_for_status()
            
            result = response.json()
            return result['choices'][0]['message']['content'].strip()
            
        except requests.exceptions.RequestException as e:
            print(f"API í˜¸ì¶œ ì˜¤ë¥˜: {e}")
            return ""
        except Exception as e:
            print(f"ë¶„ì„ ì˜¤ë¥˜: {e}")
            return ""
    
    def analyze_podcast_colloquialisms(self, transcript: str, difficulty: str = "B2") -> List[str]:
        """
        Extract colloquial expressions from podcast transcript using LLM
        íŒŸìºìŠ¤íŠ¸ transcriptì—ì„œ êµ¬ì–´ì²´ í‘œí˜„ ì¶”ì¶œ
        """
        if not transcript:
            return []
        
        # í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì¸ì½”ë”© ë¬¸ì œ í•´ê²°
        transcript = self.clean_text(transcript)
        
        # ì •ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆê±°ë‚˜ ë„ˆë¬´ ì§§ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
        if len(transcript.strip()) < 50:
            print(f"    âš ï¸  ì •ë¦¬ëœ í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤: {len(transcript.strip())}ì")
            return []
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 2000ìë§Œ ì‚¬ìš©
        if len(transcript) > 2000:
            transcript = transcript[:2000] + "..."
        
        print(f"    ğŸ“ ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ë³´ê¸°: {transcript[:100]}...")
        
        prompt = f"""
You are analyzing Spanish text to find ACTUAL colloquial expressions that appear in the text.

CRITICAL RULE: Only extract expressions that are ACTUALLY PRESENT in the provided text. Do not suggest or create expressions that are not in the text.

Text to analyze:
{transcript}

Instructions:
1. Read the text carefully
2. Look for actual colloquial expressions, informal phrases, or conversational elements that appear in the text
3. If the text is formal and contains no colloquial expressions, return "NO_COLLOQUIAL_EXPRESSIONS_FOUND"
4. If you find expressions, format them as: "expression" â†’ Korean translation (usage context)

Examples of what to look for (ONLY if they actually appear in the text):
- Conversational fillers: o sea, bueno, pues, entonces
- Question tags: verdad, no, sabes
- Informal transitions: por cierto, a proposito, ademas
- Opinion expressions: me parece que, creo que, la cosa es que

Response format (only if expressions are found in the text):
- "actual_expression_from_text" â†’ Korean meaning (context)

If no colloquial expressions are found in this formal text, respond with: NO_COLLOQUIAL_EXPRESSIONS_FOUND
"""
        
        response = self._make_api_call(prompt, max_tokens=400)
        
        if not response:
            return []
        
        # "NO_COLLOQUIAL_EXPRESSIONS_FOUND" ì‘ë‹µ ì²˜ë¦¬
        if "NO_COLLOQUIAL_EXPRESSIONS_FOUND" in response:
            print(f"    ğŸ“ LLM ë¶„ì„ ê²°ê³¼: í…ìŠ¤íŠ¸ì— êµ¬ì–´ì²´ í‘œí˜„ì´ ì—†ìŒ")
            return []
        
        # ì‘ë‹µì—ì„œ í‘œí˜„ë“¤ ì¶”ì¶œ
        expressions = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            if line.startswith('-') and '"' in line and 'â†’' in line:
                try:
                    # "- "expression" â†’ meaning (context)" í˜•ì‹ íŒŒì‹±
                    start_quote = line.find('"')
                    end_quote = line.find('"', start_quote + 1)
                    if start_quote != -1 and end_quote != -1:
                        expression = line[start_quote+1:end_quote]
                        remaining = line[end_quote+1:]
                        if 'â†’' in remaining:
                            meaning_part = remaining.split('â†’')[1].strip()
                            # (usage context) ë¶€ë¶„ ì œê±°
                            if '(' in meaning_part:
                                meaning = meaning_part.split('(')[0].strip()
                            else:
                                meaning = meaning_part.strip()
                            expressions.append(f"{expression} ({meaning})")
                except Exception as e:
                    print(f"êµ¬ì–´ì²´ í‘œí˜„ íŒŒì‹± ì˜¤ë¥˜: {e}")
                    continue
        
        return expressions[:5]  # ìµœëŒ€ 5ê°œ ë°˜í™˜
    
    def analyze_article_grammar(self, article_content: str, difficulty: str = "B2") -> Dict[str, any]:
        """
        Analyze grammar structures in article content using LLM
        ê¸°ì‚¬ ë‚´ìš©ì˜ ë¬¸ë²• êµ¬ì¡° ë¶„ì„
        """
        if not article_content:
            return {"original_sentence": "", "grammar_analysis": []}
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 1500ìë§Œ ì‚¬ìš©
        if len(article_content) > 1500:
            article_content = article_content[:1500] + "..."
        
        prompt = f"""
Analyze this Spanish article and identify 1 representative sentence for grammar analysis at {difficulty} level.

Select the most complex and educational sentence from the text that contains multiple grammar structures suitable for {difficulty} level learners.

Article content:
{article_content}

Please provide the analysis in this exact format:

ORIGINAL_SENTENCE: [exact sentence from text]

GRAMMAR_ANALYSIS:
**ë¬¸ë²• êµ¬ì¡° 1 (ìˆ˜ì¤€)**
- ì„¤ëª… 1
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 2 (ìˆ˜ì¤€)**  
- ì„¤ëª… 1
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 3 (ìˆ˜ì¤€)**
- ì„¤ëª… 1
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 4 (ìˆ˜ì¤€)**
- ì„¤ëª… 1 
- ì„¤ëª… 2

**ë¬¸ë²• êµ¬ì¡° 5 (ìˆ˜ì¤€)**
- ì„¤ëª… 1
- ì„¤ëª… 2

Focus on grammar structures appropriate for {difficulty} level such as:
- B1: í˜„ì¬/ê³¼ê±° ì‹œì œ, ser vs estar, ì¬ê·€ë™ì‚¬
- B2: ì ‘ì†ë²• í˜„ì¬/ê³¼ê±°, ì¡°ê±´ë²•, ì™„ë£Œ ì‹œì œ  
- C1: ì ‘ì†ë²• ì™„ë£Œ, ë³µí•© ì¡°ê±´ë¬¸, ìˆ˜ë™íƒœ

Provide detailed explanations for each grammar point including specific words from the sentence.
"""
        
        response = self._make_api_call(prompt, max_tokens=800)
        
        if not response:
            return {"original_sentence": "", "grammar_analysis": []}
        
        # ì‘ë‹µì—ì„œ ì›ë¬¸ ë¬¸ì¥ê³¼ ë¬¸ë²• ë¶„ì„ ì¶”ì¶œ
        original_sentence = ""
        grammar_analysis = []
        
        lines = response.split('\n')
        current_section = ""
        current_grammar = ""
        current_points = []
        
        for line in lines:
            line = line.strip()
            
            if line.startswith('ORIGINAL_SENTENCE:'):
                original_sentence = line.replace('ORIGINAL_SENTENCE:', '').strip()
            elif line.startswith('GRAMMAR_ANALYSIS:'):
                current_section = "grammar"
            elif line.startswith('**') and line.endswith('**'):
                # ì´ì „ ë¬¸ë²• êµ¬ì¡° ì €ì¥
                if current_grammar and current_points:
                    grammar_analysis.append({
                        "title": current_grammar,
                        "points": current_points
                    })
                # ìƒˆë¡œìš´ ë¬¸ë²• êµ¬ì¡° ì‹œì‘
                current_grammar = line.replace('**', '').strip()
                current_points = []
            elif line.startswith('- ') and current_section == "grammar":
                current_points.append(line[2:].strip())
        
        # ë§ˆì§€ë§‰ ë¬¸ë²• êµ¬ì¡° ì €ì¥
        if current_grammar and current_points:
            grammar_analysis.append({
                "title": current_grammar,
                "points": current_points
            })
        
        return {
            "original_sentence": original_sentence,
            "grammar_analysis": grammar_analysis
        }
        
        return {
            "original_sentence": original_sentence,
            "grammar_analysis": grammar_analysis
        }
    
    def analyze_text_difficulty(self, content: str) -> str:
        """
        Analyze text difficulty using LLM
        LLMì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„
        """
        if not content:
            return "B2"
        
        # í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì²˜ìŒ 1000ìë§Œ ì‚¬ìš©
        if len(content) > 1000:
            content = content[:1000] + "..."
        
        prompt = f"""
Analyze this Spanish text and determine its CEFR difficulty level (A1, A2, B1, B1+, B2, B2+, C1, C2).

Consider:
- Vocabulary complexity
- Grammar structures used
- Sentence length and complexity
- Abstract vs concrete concepts
- Technical vs everyday language

Text:
{content}

Respond with only the CEFR level (e.g., "B2" or "B2+" or "C1"), no additional text.
"""
        
        response = self._make_api_call(prompt, max_tokens=50)
        
        # ì‘ë‹µì—ì„œ ë ˆë²¨ ì¶”ì¶œ
        if response:
            response = response.strip().upper()
            valid_levels = ['A1', 'A2', 'B1', 'B1+', 'B2', 'B2+', 'C1', 'C2']
            for level in valid_levels:
                if level in response:
                    return level
        
        return "B2"  # ê¸°ë³¸ê°’
    
    def clean_text(self, text: str) -> str:
        """
        Clean and normalize text for better LLM analysis
        í…ìŠ¤íŠ¸ ì •ë¦¬ ë° ì •ê·œí™”
        """
        if not text:
            return ""
        
        # HTML ì—”í‹°í‹° ë””ì½”ë”©
        text = html.unescape(text)
        
        # ì˜ëª»ëœ ì¸ì½”ë”© ìˆ˜ì •
        replacements = {
            'ÃƒÂ±': 'Ã±',  # EspaÃƒÂ±ol -> EspaÃ±ol
            'ÃƒÂ¡': 'Ã¡',
            'ÃƒÂ©': 'Ã©',
            'ÃƒÂ­': 'Ã­',
            'ÃƒÂ³': 'Ã³',
            'ÃƒÂº': 'Ãº',
            'Ãƒ ': 'Ã ',
            'ÃƒÂ¨': 'Ã¨',
            'ÃƒÂ¬': 'Ã¬',
            'ÃƒÂ²': 'Ã²',
            'ÃƒÂ¹': 'Ã¹',
            'ÃƒÂ¼': 'Ã¼',
            'Ãƒ': 'Ã‘',
            'Ã‚': '',  # ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°
            '\xa0': ' ',  # non-breaking space
            '\u2028': '\n',  # line separator
            '\u2029': '\n\n'  # paragraph separator
        }
        
        for old, new in replacements.items():
            text = text.replace(old, new)
        
        # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
        text = unicodedata.normalize('NFC', text)
        
        # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ ì •ë¦¬
        text = ' '.join(text.split())
        
        return text
    
    def generate_podcast_learning_goals(self, content: str, title: str, difficulty: str = "B2", colloquial_count: int = 0) -> List[str]:
        """
        íŒŸìºìŠ¤íŠ¸ ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ì ì ˆí•œ í•™ìŠµ ëª©í‘œë¥¼ ìƒì„±
        
        Args:
            content: íŒŸìºìŠ¤íŠ¸ ë‚´ìš© (ë©”ëª¨/ìŠ¤í¬ë¦½íŠ¸)
            title: íŒŸìºìŠ¤íŠ¸ ì œëª©
            difficulty: í•™ìŠµì ìˆ˜ì¤€ (B1, B2, C1 ë“±)
            colloquial_count: ë¶„ì„ëœ êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜
            
        Returns:
            List of learning goals
        """
        print(f"    ğŸ¯ íŒŸìºìŠ¤íŠ¸ í•™ìŠµ ëª©í‘œ ìƒì„± ì¤‘... (ë‚œì´ë„: {difficulty}, êµ¬ì–´ì²´ í‘œí˜„: {colloquial_count}ê°œ)")
        
        # ë‚´ìš© ì •ë¦¬
        clean_content = self.clean_text(content)
        
        # êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜ì— ë”°ë¥¸ ëª©í‘œ ì¡°ì •
        colloquial_goal = f"êµ¬ì–´ì²´ í‘œí˜„ {colloquial_count}ê°œ" if colloquial_count > 0 else "êµ¬ì–´ì²´ í‘œí˜„"
        
        prompt = f"""
Analyze this Spanish podcast episode and generate 3-4 specific, actionable learning goals for a {difficulty} level Spanish learner.

PODCAST TITLE: {title}
PODCAST CONTENT: {clean_content[:1500]}
COLLOQUIAL EXPRESSIONS FOUND: {colloquial_count} expressions

Consider the following aspects when creating learning goals:
1. Main topic/theme of the episode
2. Vocabulary focus areas (suggest realistic number based on content length)
3. Grammar structures present
4. Cultural/contextual learning opportunities
5. Listening comprehension skills
6. Colloquial expressions: exactly {colloquial_count} expressions were found in the analysis

Create learning goals that are:
- Specific and measurable
- Appropriate for {difficulty} level
- Reflect the ACTUAL analysis results (e.g., if {colloquial_count} colloquial expressions were found, mention that exact number)
- Focus on practical language skills
- Encourage active listening and engagement
- Be realistic about vocabulary numbers (5-10 key words, not excessive amounts)

Format your response as a numbered list in Korean:
1. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 1]
2. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 2]  
3. [êµ¬ì²´ì ì¸ í•™ìŠµ ëª©í‘œ 3]
(4. [ì¶”ê°€ ëª©í‘œ if relevant])

Make sure to reference the actual number of colloquial expressions found: {colloquial_count}

Examples of good learning goals:
- ì—í”¼ì†Œë“œ ì£¼ì œì™€ ê´€ë ¨ëœ í•µì‹¬ ì–´íœ˜ 5-8ê°œ í•™ìŠµ ë° í™œìš©
- í™”ìì˜ ê°ì • í‘œí˜„ ë°©ì‹ê³¼ ì–µì–‘ íŒ¨í„´ íŒŒì•…  
- ë¶„ì„ëœ êµ¬ì–´ì²´ í‘œí˜„ {colloquial_count}ê°œë¥¼ ì •ë¦¬í•˜ê³  ì¼ìƒ ëŒ€í™”ì—ì„œ í™œìš©í•˜ê¸°
- ìŠ¤í˜ì¸ì–´ ë¬¸í™”ì  ë§¥ë½ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê´€ìš© í‘œí˜„ ì´í•´
"""
        
        response = self._make_api_call(prompt, max_tokens=600)
        
        if not response:
            # ê¸°ë³¸ ëª©í‘œ ë°˜í™˜ (êµ¬ì–´ì²´ í‘œí˜„ ê°œìˆ˜ ë°˜ì˜)
            if colloquial_count > 0:
                return [
                    f"íŒŸìºìŠ¤íŠ¸ ì£¼ì œ ê´€ë ¨ í•µì‹¬ ì–´íœ˜ 5-7ê°œ í•™ìŠµ ({difficulty} ìˆ˜ì¤€)",
                    f"ë¶„ì„ëœ êµ¬ì–´ì²´ í‘œí˜„ {colloquial_count}ê°œ ì •ë¦¬ ë° í™œìš©",
                    "ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒê³¼ ì–µì–‘ íŒ¨í„´ í•™ìŠµ"
                ]
            else:
                return [
                    f"íŒŸìºìŠ¤íŠ¸ ì£¼ì œ ê´€ë ¨ ì–´íœ˜ í•™ìŠµ ({difficulty} ìˆ˜ì¤€)",
                    "ìŠ¤í˜ì¸ì–´ êµ¬ì–´ì²´ í‘œí˜„ íŒŒì•… ë° ì´í•´",
                    "ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒê³¼ ì–µì–‘ íŒ¨í„´ í•™ìŠµ"
                ]
        
        # ì‘ë‹µì—ì„œ ëª©í‘œë“¤ ì¶”ì¶œ
        goals = []
        lines = response.split('\n')
        
        for line in lines:
            line = line.strip()
            # ìˆ«ìë¡œ ì‹œì‘í•˜ëŠ” ì¤„ ì°¾ê¸° (1. 2. 3. í˜•íƒœ)
            if line and (line[0].isdigit() or line.startswith('â€¢') or line.startswith('-')):
                # ë²ˆí˜¸ ë¶€ë¶„ ì œê±°
                if '. ' in line:
                    goal = line.split('. ', 1)[1].strip()
                elif line.startswith('â€¢ '):
                    goal = line[2:].strip()
                elif line.startswith('- '):
                    goal = line[2:].strip()
                else:
                    goal = line.strip()
                
                if goal and len(goal) > 10:  # ë„ˆë¬´ ì§§ì€ ëª©í‘œëŠ” ì œì™¸
                    goals.append(goal)
        
        # 3-4ê°œ ëª©í‘œ ë°˜í™˜
        return goals[:4] if goals else [
            f"íŒŸìºìŠ¤íŠ¸ ì£¼ì œ ê´€ë ¨ ì–´íœ˜ í•™ìŠµ ({difficulty} ìˆ˜ì¤€)",
            "ìŠ¤í˜ì¸ì–´ êµ¬ì–´ì²´ í‘œí˜„ íŒŒì•… ë° ì´í•´",
            "ìì—°ìŠ¤ëŸ¬ìš´ ë°œìŒê³¼ ì–µì–‘ íŒ¨í„´ í•™ìŠµ"
        ]