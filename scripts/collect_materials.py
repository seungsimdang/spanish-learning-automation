#!/usr/bin/env python3
"""
Collect Spanish learning materials: articles and podcast episodes with content analysis.
"""
import os
import sys
import requests
import feedparser
from datetime import datetime, timedelta
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
from spanish_vocabulary import search_vocabulary

def is_episode_recent(published_date, max_days_old=2):
    """ì—í”¼ì†Œë“œê°€ ìµœê·¼ ë©°ì¹  ì´ë‚´ì— ë°œí–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸"""
    try:
        if not published_date:
            return True  # ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ í—ˆìš©
        
        # feedparserê°€ íŒŒì‹±í•œ ë‚ ì§œë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        if hasattr(published_date, 'tm_year'):  # struct_time ê°ì²´ì¸ ê²½ìš°
            episode_date = datetime(*published_date[:6])
        elif isinstance(published_date, str):
            # ë¬¸ìì—´ì¸ ê²½ìš° íŒŒì‹± ì‹œë„
            from email.utils import parsedate_tz
            import calendar
            parsed = parsedate_tz(published_date)
            if parsed:
                episode_date = datetime(*parsed[:6])
            else:
                return True  # íŒŒì‹± ì‹¤íŒ¨ì‹œ í—ˆìš©
        else:
            return True
        
        current_date = datetime.now()
        days_diff = (current_date - episode_date).days
        
        print(f"DEBUG: ì—í”¼ì†Œë“œ ë‚ ì§œ í™•ì¸ - ë°œí–‰ì¼: {episode_date.strftime('%Y-%m-%d')}, í˜„ì¬: {current_date.strftime('%Y-%m-%d')}, ì°¨ì´: {days_diff}ì¼")
        
        return days_diff <= max_days_old
        
    except Exception as e:
        print(f"ë‚ ì§œ í™•ì¸ ì˜¤ë¥˜: {e}")
        return True  # ì˜¤ë¥˜ ì‹œ í—ˆìš©

def is_duplicate_content(title, existing_titles=[]):
    """ì œëª© ì¤‘ë³µ í™•ì¸"""
    if not existing_titles:
        return False
    
    # ì œëª© ì •ê·œí™” (íŠ¹ìˆ˜ë¬¸ì, ê³µë°± ì œê±°)
    normalized_title = re.sub(r'[^\w\s]', '', title.lower()).strip()
    
    for existing_title in existing_titles:
        normalized_existing = re.sub(r'[^\w\s]', '', existing_title.lower()).strip()
        
        # 80% ì´ìƒ ìœ ì‚¬í•˜ë©´ ì¤‘ë³µìœ¼ë¡œ íŒë‹¨
        similarity = len(set(normalized_title.split()) & set(normalized_existing.split())) / max(len(normalized_title.split()), len(normalized_existing.split()))
        
        if similarity >= 0.8:
            print(f"DEBUG: ì¤‘ë³µ ì½˜í…ì¸  ê°ì§€ - ê¸°ì¡´: {existing_title}, ìƒˆë¡œìš´: {title} (ìœ ì‚¬ë„: {similarity:.2f})")
            return True
    
    return False

def analyze_text_difficulty(content):
    """Analyze text difficulty and return appropriate CEFR level"""
    if not content:
        return "B2"  # ê¸°ë³¸ê°’
    
    # í…ìŠ¤íŠ¸ ê¸¸ì´ë¡œ ê¸°ë³¸ íŒë‹¨
    word_count = len(content.split())
    sentence_count = len([s for s in content.split('.') if s.strip()])
    avg_sentence_length = word_count / max(sentence_count, 1)
    
    # ë³µì¡í•œ ë¬¸ë²• êµ¬ì¡° í™•ì¸ (ê°€ì¤‘ì¹˜)
    complexity_score = 0
    
    # ì ‘ì†ë²• (subjunctive) íŒ¨í„´ë“¤
    subjunctive_patterns = [
        r'\b(sea|seas|seamos|sean)\b',  # ser ì ‘ì†ë²•
        r'\b(tenga|tengas|tengamos|tengan)\b',  # tener ì ‘ì†ë²•  
        r'\b(haga|hagas|hagamos|hagan)\b',  # hacer ì ‘ì†ë²•
        r'\b(vaya|vayas|vayamos|vayan)\b',  # ir ì ‘ì†ë²•
        r'\bque\s+\w+[ae]s?\b',  # que + ì ‘ì†ë²• íŒ¨í„´
        r'\bsi\s+\w+[ai]era\b',  # si + ì ‘ì†ë²• ê³¼ê±°
        r'\bojalÃ¡\b',  # ojalÃ¡ (ì ‘ì†ë²• ì‹ í˜¸)
        r'\bes\s+importante\s+que\b',  # ê°ì •/ì˜ê²¬ í‘œí˜„ + que
        r'\bespero\s+que\b',
        r'\bdudo\s+que\b'
    ]
    
    for pattern in subjunctive_patterns:
        complexity_score += len(re.findall(pattern, content, re.IGNORECASE))
    
    # ë³µì¡í•œ ì‹œì œë“¤
    complex_tenses = [
        r'\b\w+ado\s+sido\b',  # ì™„ë£Œí˜•
        r'\b\w+ido\s+sido\b',
        r'\bhabÃ­a\s+\w+[adi]o\b',  # ê³¼ê±°ì™„ë£Œ
        r'\bhabrÃ¡\s+\w+[adi]o\b',  # ë¯¸ë˜ì™„ë£Œ
        r'\bestaba\s+\w+ndo\b',  # ê³¼ê±°ì§„í–‰
        r'\bestarÃ­a\s+\w+ndo\b'  # ì¡°ê±´ë²• ì§„í–‰
    ]
    
    for pattern in complex_tenses:
        complexity_score += len(re.findall(pattern, content, re.IGNORECASE))
    
    # ê³ ê¸‰ ì–´íœ˜ (ì¶”ìƒì , í•™ìˆ ì  ì–´íœ˜)
    advanced_vocab = [
        r'\b(perspectiva|anÃ¡lisis|consecuencia|implicaciÃ³n|estrategia)\b',
        r'\b(implementar|consolidar|optimizar|contextualizar)\b',
        r'\b(paradigma|metodologÃ­a|epistemologÃ­a|ontologÃ­a)\b',
        r'\b(inherente|intrÃ­nseco|subyacente|tangible|intangible)\b',
        r'\b(heterogÃ©neo|homogÃ©neo|multifacÃ©tico|polifacÃ©tico)\b'
    ]
    
    for pattern in advanced_vocab:
        complexity_score += len(re.findall(pattern, content, re.IGNORECASE)) * 2  # ê³ ê¸‰ì–´íœ˜ëŠ” ê°€ì¤‘ì¹˜ 2ë°°
    
    # ë³µì¡í•œ ì—°ê²°ì‚¬ë“¤
    complex_connectors = [
        r'\bsin\s+embargo\b', r'\bno\s+obstante\b', r'\ba\s+pesar\s+de\b',
        r'\ben\s+cuanto\s+a\b', r'\brespecto\s+a\b', r'\bcon\s+respecto\s+a\b',
        r'\bpor\s+consiguiente\b', r'\bpor\s+ende\b', r'\basimismo\b',
        r'\bademÃ¡s\s+de\b', r'\baparte\s+de\b', r'\bexcepto\b', r'\bsalvo\b'
    ]
    
    for pattern in complex_connectors:
        complexity_score += len(re.findall(pattern, content, re.IGNORECASE))
    
    # ìˆ˜ë™í˜•
    passive_patterns = [
        r'\bfue\s+\w+[adi]o\b', r'\bfueron\s+\w+[adi]os\b',
        r'\bes\s+\w+[adi]o\b', r'\bson\s+\w+[adi]os\b',
        r'\bserÃ¡\s+\w+[adi]o\b', r'\bserÃ­an\s+\w+[adi]os\b'
    ]
    
    for pattern in passive_patterns:
        complexity_score += len(re.findall(pattern, content, re.IGNORECASE))
    
    # ì •ê·œí™”ëœ ë³µì¡ë„ ì ìˆ˜ ê³„ì‚°
    normalized_score = complexity_score / max(word_count / 100, 1)  # 100ë‹¨ì–´ë‹¹ ë³µì¡ë„
    
    print(f"DEBUG: ë‚œì´ë„ ë¶„ì„ - ë‹¨ì–´ìˆ˜: {word_count}, ë³µì¡ë„ ì ìˆ˜: {complexity_score}, ì •ê·œí™” ì ìˆ˜: {normalized_score:.2f}")
    
    # ë‚œì´ë„ íŒì •
    if normalized_score >= 3.0 or avg_sentence_length > 25:
        return "C1"
    elif normalized_score >= 1.5 or avg_sentence_length > 20:
        return "B2+"
    elif normalized_score >= 0.8:
        return "B2"
    else:
        return "B1+"

def search_apple_podcasts_episode(podcast_name, episode_title, apple_base):
    """Search for exact episode URL using Apple iTunes Search API"""
    try:
        import urllib.parse
        
        # Radio Ambulanteì˜ ì •í™•í•œ iTunes ID
        if 'Radio Ambulante' in podcast_name:
            podcast_id = "527614348"
        else:
            # ë‹¤ë¥¸ íŒŸìºìŠ¤íŠ¸ì˜ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
            return apple_base
        
        # ê²€ìƒ‰ì–´ë¥¼ ì—¬ëŸ¬ ë°©ì‹ìœ¼ë¡œ ì‹œë„
        search_terms = []
        
        # 1. ì „ì²´ ì œëª©ìœ¼ë¡œ ê²€ìƒ‰
        search_terms.append(episode_title)
        
        # 2. "The Network:" ë¶€ë¶„ë§Œìœ¼ë¡œ ê²€ìƒ‰ (Radio Ambulante ì‹œë¦¬ì¦ˆ)
        if ':' in episode_title:
            main_part = episode_title.split(':')[0].strip()
            search_terms.append(main_part)
            
            # ë¶€ì œëª© ë¶€ë¶„ë„ ì¶”ê°€
            subtitle = episode_title.split(':', 1)[1].strip()
            search_terms.append(subtitle)
        
        # 3. Radio Ambulante + í‚¤ì›Œë“œ ì¡°í•©
        keywords = episode_title.lower().split()
        important_words = [w for w in keywords if len(w) > 3 and w not in ['the', 'and', 'of', 'in', 'to', 'for']]
        if important_words:
            search_terms.append(f"Radio Ambulante {' '.join(important_words[:2])}")
        
        print(f"Apple ê²€ìƒ‰ì–´ë“¤: {search_terms}")
        
        for search_term in search_terms:
            encoded_term = urllib.parse.quote(search_term)
            search_url = f"https://itunes.apple.com/search?term={encoded_term}&media=podcast&entity=podcastEpisode&limit=50"
            
            print(f"Apple iTunes Search API í˜¸ì¶œ: {search_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                results = data.get('results', [])
                
                print(f"iTunes ê²€ìƒ‰ ê²°ê³¼ ({search_term}): {len(results)}ê°œ ì—í”¼ì†Œë“œ ë°œê²¬")
                
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ Radio Ambulante ì—í”¼ì†Œë“œ ì°¾ê¸°
                for result in results:
                    collection_name = result.get('collectionName', '').lower()
                    track_name = result.get('trackName', '')
                    
                    print(f"  ê²€í†  ì¤‘: {track_name} (ì»¬ë ‰ì…˜: {collection_name})")
                    
                    # Radio Ambulante íŒŸìºìŠ¤íŠ¸ì¸ì§€ í™•ì¸
                    if 'radio ambulante' in collection_name:
                        # ì œëª© ìœ ì‚¬ë„ í™•ì¸ - ë” ê´€ëŒ€í•œ ë§¤ì¹­
                        title_words = episode_title.lower().split()
                        track_words = track_name.lower().split()
                        
                        # ì£¼ìš” ë‹¨ì–´ê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
                        common_words = set(title_words) & set(track_words)
                        if len(common_words) >= 2 or any(word in track_name.lower() for word in title_words if len(word) > 4):
                            episode_url = result.get('episodeUrl')
                            track_id = result.get('trackId')
                            
                            if episode_url:
                                print(f"âœ… Apple Podcastsì—ì„œ ì—í”¼ì†Œë“œ ë°œê²¬: {track_name}")
                                print(f"   Direct URL: {episode_url}")
                                return episode_url
                            elif track_id:
                                apple_url = f"https://podcasts.apple.com/kr/podcast/radio-ambulante/id{podcast_id}?i={track_id}"
                                print(f"âœ… Apple Podcastsì—ì„œ ì—í”¼ì†Œë“œ ë°œê²¬: {track_name}")
                                print(f"   Track URL: {apple_url}")
                                return apple_url
                
            else:
                print(f"iTunes Search API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
                
        print(f"âŒ Apple Podcastsì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í•¨")
        # ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ë©”ì¸ íŒŸìºìŠ¤íŠ¸ í˜ì´ì§€ ë°˜í™˜
        return apple_base
        
    except Exception as e:
        print(f"Apple Podcasts ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return apple_base

def generate_apple_podcast_link(podcast_name, apple_base, episode_link, episode_number, episode_title=""):
    """Generate optimized Apple Podcasts link by podcast type"""
    
    # íŒŸìºìŠ¤íŠ¸ë³„ ë§í¬ ìƒì„± ì „ëµ
    if 'Radio Ambulante' in podcast_name or 'npr.org' in episode_link:
        # Radio AmbulanteëŠ” ì—í”¼ì†Œë“œë³„ ì§ì ‘ ë§í¬ ìƒì„± ì‹œë„
        if episode_link and 'radioambulante.org' in episode_link:
            # ì›ë³¸ ì—í”¼ì†Œë“œ ë§í¬ê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„ ìš°ì„  ì‚¬ìš©
            return episode_link
        else:
            # Apple iTunes Search APIë¥¼ ì‚¬ìš©í•´ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œ ì°¾ê¸°
            if episode_title:
                apple_url = search_apple_podcasts_episode(podcast_name, episode_title, apple_base)
                # ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ì„ ë•Œë§Œ Apple URL ì‚¬ìš© (apple_baseì™€ ë‹¤ë¥¸ ê²½ìš°)
                if apple_url != apple_base and validate_url(apple_url):
                    print(f"âœ… Apple Podcastsì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œ ë§í¬ ì°¾ìŒ: {apple_url}")
                    return apple_url
                else:
                    print(f"âŒ Apple Podcastsì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í•¨, ì—í”¼ì†Œë“œ URL ì‚¬ìš©")
                    # Appleì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ì›ë³¸ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                    return episode_link
            
            # ì—í”¼ì†Œë“œ ì œëª©ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë§í¬ ì‚¬ìš©
            print(f"âŒ ì—í”¼ì†Œë“œ ì œëª©ì´ ì—†ì–´ Apple ê²€ìƒ‰ ë¶ˆê°€, ì—í”¼ì†Œë“œ URL ì‚¬ìš©")
            return episode_link

    elif 'Hoy Hablamos' in podcast_name:
        # Hoy HablamosëŠ” ì—í”¼ì†Œë“œ ë²ˆí˜¸ ê¸°ë°˜ìœ¼ë¡œ ë§í¬ ìƒì„±
        if episode_number and episode_number != 'N/A':
            try:
                # ì—í”¼ì†Œë“œ ë²ˆí˜¸ë¥¼ ìˆ«ìë¡œ ë³€í™˜
                ep_num = int(episode_number)
                return f"{apple_base}?i=1000{ep_num:06d}"  # Appleì˜ ì—í”¼ì†Œë“œ ID íŒ¨í„´
            except:
                pass
        return apple_base
    elif 'SpanishWithVicente' in podcast_name:
        # SpanishWithVicenteëŠ” ì—í”¼ì†Œë“œ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if episode_number and episode_number != 'N/A':
            return f"{apple_base}?i={episode_number}"
        else:
            return apple_base
    elif 'DELE' in podcast_name:
        # DELE PodcastëŠ” ë©”ì¸ ë§í¬ ì‚¬ìš©
        return apple_base
    else:
        # ê¸°ë³¸ ì „ëµ: ì—í”¼ì†Œë“œ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ì¶”ê°€
        if episode_number and episode_number != 'N/A':
            return f"{apple_base}?i={episode_number}"
        else:
            return apple_base

def get_article_content(url):
    """Get actual article content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì‚¬ì´íŠ¸ë³„ ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
        content = ""
        
        if '20minutos.es' in url:
            # 20minutos ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('div', class_='article-text') or soup.find('div', class_='content')
            if article_body:
                paragraphs = article_body.find_all(['p', 'div'])
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
        
        elif 'elpais.com' in url:
            # El PaÃ­s ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('div', {'data-dtm-region': 'articulo_cuerpo'}) or \
                         soup.find('div', class_='a_c clearfix') or \
                         soup.find('div', class_='articulo-cuerpo')
            if article_body:
                paragraphs = article_body.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
        
        # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (fallback)
        if not content:
            # ì¼ë°˜ì ì¸ article íƒœê·¸ë‚˜ main íƒœê·¸ì—ì„œ ì¶”ì¶œ
            article = soup.find('article') or soup.find('main')
            if article:
                paragraphs = article.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()][:10])  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨ë§Œ
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if len(content) < 200:
            all_paragraphs = soup.find_all('p')
            content = ' '.join([p.get_text().strip() for p in all_paragraphs if len(p.get_text().strip()) > 50][:8])
        
        return content[:2000]  # ì²˜ìŒ 2000ìë§Œ ë°˜í™˜
        
    except Exception as e:
        print(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def extract_vocabulary_from_content(content, difficulty="B2"):
    """Extract vocabulary from article content based on difficulty level"""
    if not content:
        print("DEBUG: ë‚´ìš©ì´ ì—†ì–´ì„œ ì–´íœ˜ ì¶”ì¶œ ë¶ˆê°€")
        return []
    
    print(f"DEBUG: ì–´íœ˜ ì¶”ì¶œ ì¤‘... ë‚´ìš© ê¸¸ì´: {len(content)}, ë‚œì´ë„: {difficulty}")
    print(f"DEBUG: ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {content[:200]}...")
    
    # ìƒˆë¡œìš´ ì–´íœ˜ ëª¨ë“ˆ ì‚¬ìš©
    found_vocabulary = search_vocabulary(content, difficulty, max_results=8)
    
    print(f"DEBUG: ì´ ì¶”ì¶œëœ ì–´íœ˜ ê°œìˆ˜: {len(found_vocabulary)}")
    for vocab in found_vocabulary:
        print(f"DEBUG: ì°¾ì€ ì–´íœ˜ - {vocab}")
    
    return found_vocabulary

def extract_category_from_content(title, content):
    """Extract category from title and content"""
    full_text = (title + " " + content).lower()
    
    keywords = {
        'ì •ì¹˜': ['gobierno', 'polÃ­tica', 'elecciones', 'parlamento', 'ministro', 'rey', 'presidente', 'votaciÃ³n', 'congreso'],
        'ê²½ì œ': ['economÃ­a', 'banco', 'euro', 'empleo', 'crisis', 'mercado', 'dinero', 'trabajo', 'empresa', 'inversiÃ³n'],
        'ì‚¬íšŒ': ['sociedad', 'educaciÃ³n', 'sanidad', 'vivienda', 'familia', 'salud', 'poblaciÃ³n', 'ciudadanos'],
        'ìŠ¤í¬ì¸ ': ['fÃºtbol', 'real madrid', 'barcelona', 'liga', 'deporte', 'partido', 'atletico', 'champions'],
        'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³vil', 'digital', 'app', 'inteligencia', 'innovaciÃ³n'],
        'ë¬¸í™”': ['cultura', 'arte', 'mÃºsica', 'teatro', 'festival', 'libro', 'cine', 'exposiciÃ³n'],
        'êµ­ì œ': ['internacional', 'mundial', 'europa', 'amÃ©rica', 'china', 'estados unidos', 'uniÃ³n europea']
    }
    
    category_scores = {}
    for category, words in keywords.items():
        score = sum(1 for word in words if word in full_text)
        if score > 0:
            category_scores[category] = score
    
    if category_scores:
        return max(category_scores, key=category_scores.get)
    return 'ì¼ë°˜'

def extract_episode_number(title):
    patterns = [
        r'Ep\.?\s*(\d+)',
        r'Episode\s*(\d+)',
        r'#(\d+)',
        r'(\d{3,4})'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, title, re.IGNORECASE)
        if match:
            return match.group(1)
    return None

def extract_duration_from_feed(entry):
    # iTunes ë“€ë ˆì´ì…˜ ë¨¼ì € í™•ì¸
    if hasattr(entry, 'itunes_duration'):
        duration = entry.itunes_duration
        # ì´ˆ ë‹¨ìœ„ì¸ ê²½ìš° ë¶„:ì´ˆë¡œ ë³€í™˜
        if duration.isdigit():
            total_seconds = int(duration)
            minutes = total_seconds // 60
            seconds = total_seconds % 60
            return f"{minutes}:{seconds:02d}"
        return duration
    
    # ìš”ì•½ì—ì„œ ì¬ìƒì‹œê°„ ì¶”ì¶œ ì‹œë„
    summary = entry.get('summary', '') + entry.get('description', '')
    duration_patterns = [
        r'(\d+)\s*min',
        r'(\d+)\s*ë¶„',
        r'(\d+):(\d+)',
        r'Duration:\s*(\d+)'
    ]
    
    for pattern in duration_patterns:
        match = re.search(pattern, summary)
        if match:
            if ':' in pattern:
                return f"{match.group(1)}:{match.group(2)}"
            else:
                return f"{match.group(1)}ë¶„"
    
    return "15-25ë¶„"

def extract_topic_keywords(title, summary=""):
    content = (title + " " + summary).lower()
    
    topic_keywords = {
        'ë¬¸ë²•': ['gramÃ¡tica', 'verbos', 'subjuntivo', 'pretÃ©rito', 'sintaxis'],
        'ë¬¸í™”': ['cultura', 'tradiciÃ³n', 'costumbres', 'historia', 'arte'],
        'ìš”ë¦¬': ['cocina', 'comida', 'receta', 'gastronomÃ­a', 'plato'],
        'ì—¬í–‰': ['viajes', 'turismo', 'ciudades', 'lugares', 'destinos'],
        'ì§ì—…': ['trabajo', 'empleo', 'profesiÃ³n', 'carrera', 'oficina'],
        'ê°€ì¡±': ['familia', 'padres', 'hijos', 'matrimonio', 'casa'],
        'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³viles', 'digital', 'aplicaciones'],
        'ì •ì¹˜': ['polÃ­tica', 'gobierno', 'elecciones', 'democracia'],
        'ê²½ì œ': ['economÃ­a', 'dinero', 'banco', 'trabajo', 'crisis', 'preferentes', 'ahorros'],
        'ì‚¬íšŒ': ['sociedad', 'gente', 'problemas', 'cambios', 'vida'],
        'ê±´ê°•': ['salud', 'medicina', 'hospital', 'enfermedad', 'mÃ©dico'],
        'êµìœ¡': ['educaciÃ³n', 'estudiantes', 'universidad', 'aprender']
    }
    
    for topic, keywords in topic_keywords.items():
        if any(keyword in content for keyword in keywords):
            return topic
    return 'ì¼ë°˜ ì£¼ì œ'

def create_detailed_memo(content_type, data, weekday_name):
    if content_type == "article":
        category = data.get('category', 'ì¼ë°˜')
        vocabulary = data.get('vocabulary', [])
        difficulty = data.get('difficulty', 'B2')  # ë™ì ìœ¼ë¡œ ë¶„ì„ëœ ë‚œì´ë„ ì‚¬ìš©
        
        print(f"DEBUG: ë©”ëª¨ ìƒì„± - ì¹´í…Œê³ ë¦¬: {category}, ë‚œì´ë„: {difficulty}")
        
        vocab_text = ""
        if vocabulary:
            # ì–´íœ˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°œì„ ëœ í˜•íƒœë¡œ í‘œì‹œ
            vocab_list = []
            for vocab in vocabulary[:4]:  # ì²˜ìŒ 4ê°œë§Œ í‘œì‹œ
                if '(' in vocab:
                    word = vocab.split('(')[0].strip()
                    meaning = vocab.split('(')[1].replace(')', '').strip()
                    vocab_list.append(f"{word}({meaning})")
                else:
                    vocab_list.append(vocab)
            vocab_text = f"ğŸ“š í•µì‹¬ ì–´íœ˜: {', '.join(vocab_list)} "
        
        return (f"ğŸ“° {category} ë¶„ì•¼ ê¸°ì‚¬ ({difficulty} ìˆ˜ì¤€) "
               f"ğŸ“… ë°œí–‰: {data.get('published', 'ì˜¤ëŠ˜')} "
               f"ğŸ¯ í•™ìŠµëª©í‘œ: 15ë¶„ ë…í•´, {difficulty} ìˆ˜ì¤€ ì–´íœ˜ ì •ë¦¬ "
               f"{vocab_text}"
               f"ğŸ“ ê¶Œì¥: ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ì„ í†µí•œ ë§ì¶¤ ì–´íœ˜ í•™ìŠµ")

    elif content_type == "podcast":
        podcast_name = data.get('podcast_name', '').replace(' (ë°±ì—…)', '')  # ë°±ì—… í…ìŠ¤íŠ¸ ì œê±°
        duration = data.get('duration', '15-25ë¶„')
        topic = data.get('topic', 'ì¼ë°˜ ì£¼ì œ')
        episode_num = data.get('episode_number', '')
        episode_title = data.get('title', '')  # ì •í™•í•œ ì—í”¼ì†Œë“œ ì œëª© ì¶”ê°€
        
        # 'N/A'ë‚˜ ë¹ˆ ê°’ ì²˜ë¦¬
        if episode_num == 'N/A' or not episode_num:
            episode_num = ''
        
        # ì£¼ì œ ì •ë¦¬ (ì›í•˜ì§€ ì•ŠëŠ” ê°’ë“¤ ì œê±°)
        if topic in ['ì¼ë°˜ ì£¼ì œ', 'N/A', '']:
            topic = 'ìŠ¤í˜ì¸ì–´ í•™ìŠµ'
        
        # íŒŸìºìŠ¤íŠ¸ ì´ë¦„ ì •ë¦¬ (ë°±ì—… í‘œì‹œë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°)
        clean_podcast_name = podcast_name.replace(" (ë°±ì—…)", "").strip()
        
        # ì£¼ì œì— ë”°ë¥¸ í•™ìŠµëª©í‘œ ì„¤ì •
        learning_goals = {
            'ê²½ì œ': 'ê¸ˆìœµ ì–´íœ˜',
            'ì •ì¹˜': 'ì •ì¹˜ ìš©ì–´',
            'ë¬¸í™”': 'ë¬¸í™” í‘œí˜„',
            'ì‚¬íšŒ': 'ì‚¬íšŒ ì´ìŠˆ ì–´íœ˜',
            'êµìœ¡': 'êµìœ¡ ê´€ë ¨ ì–´íœ˜',
            'ê±´ê°•': 'ì˜ë£Œ ìš©ì–´',
            'ê¸°ìˆ ': 'ê¸°ìˆ  ìš©ì–´',
            'ë¬¸ë²•': 'ë¬¸ë²• êµ¬ì¡°',
            'ìŠ¤í˜ì¸ì–´ í•™ìŠµ': 'ì¼ìƒ ì–´íœ˜'
        }
        goal = learning_goals.get(topic, 'í•µì‹¬ ì–´íœ˜')
        
        # ì¬ìƒì‹œê°„ì— ë”°ë¥¸ ì²­ì·¨ ê³„íš ì„¤ì •
        if ':' in duration:
            try:
                minutes, seconds = duration.split(':')
                total_minutes = int(minutes)
                if total_minutes > 30:
                    listen_plan = f"(30ë¶„ ì²­ì·¨ ëª©í‘œ)"
                elif total_minutes > 20:
                    listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
                else:
                    listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
            except:
                listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
        else:
            listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
        
        # ì—í”¼ì†Œë“œ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ í‘œì‹œ, ì—†ìœ¼ë©´ ìƒëµ
        episode_text = f"Ep.{episode_num} - " if episode_num else ""
        
        # ì •í™•í•œ ì—í”¼ì†Œë“œ ì œëª© ì¶”ê°€ (Apple Podcastsì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡)
        search_info = ""
        if episode_title:
            # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½
            short_title = episode_title[:50] + "..." if len(episode_title) > 50 else episode_title
            search_info = f"ğŸ” ê²€ìƒ‰ì–´: \"{short_title}\" "
        
        # Radio Ambulanteì¸ ê²½ìš° ì›¹ì‚¬ì´íŠ¸ URL ì •ë³´ ì¶”ê°€
        url_info = ""
        if 'Radio Ambulante' in clean_podcast_name:
            episode_url = data.get('url', '')
            if 'radioambulante.org' in episode_url:
                url_info = f"ğŸŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì²­ì·¨ ê°€ëŠ¥ "
            elif 'npr.org' in episode_url:
                url_info = f"ğŸ“» NPRì—ì„œ ì²­ì·¨ ê°€ëŠ¥ "
            
            # Apple Podcastsì—ì„œ ìˆ˜ë™ ê²€ìƒ‰ì„ ìœ„í•œ ì •ë³´ ì¶”ê°€
            if episode_title:
                # ì œëª©ì—ì„œ ë¶€ì œëª© ì¶”ì¶œ (ì½œë¡  ì´í›„ ë¶€ë¶„)
                if ':' in episode_title:
                    main_title = episode_title.split(':')[0].strip()
                    subtitle = episode_title.split(':', 1)[1].strip()
                    url_info += f"ğŸ Apple Podcasts ê²€ìƒ‰: \"{main_title}\" ë˜ëŠ” \"{subtitle}\" "
                else:
                    url_info += f"ğŸ Apple Podcasts ê²€ìƒ‰: \"{episode_title}\" "
        
        return (f"ğŸ§ {clean_podcast_name} {episode_text}{weekday_name} ìŠ¤í˜ì¸ íŒŸìºìŠ¤íŠ¸ "
               f"ğŸ“º ì—í”¼ì†Œë“œ: \"{episode_title}\" "
               f"â±ï¸ ì¬ìƒì‹œê°„: {duration} {listen_plan} "
               f"ğŸ¯ í•™ìŠµëª©í‘œ: {goal} 5ê°œ ì •ë¦¬ "
               f"ğŸŒ ì£¼ì œ: {topic} "
               f"{search_info}"
               f"{url_info}"
               f"ğŸ“ ê¶Œì¥: í•µì‹¬ ì–´íœ˜ì— ì§‘ì¤‘í•˜ì—¬ ì²­ì·¨")

def extract_radio_ambulante_url(entry):
    """Extract actual Radio Ambulante website URL"""
    try:
        # ì—í”¼ì†Œë“œ ì œëª©ì—ì„œ ìŠ¬ëŸ¬ê·¸ ìƒì„± ì‹œë„
        title = entry.title.lower()
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ ë³€í™˜
        import re
        slug = re.sub(r'[^\w\s-]', '', title)
        slug = re.sub(r'[-\s]+', '-', slug).strip('-')
        
        # Radio Ambulante ì›¹ì‚¬ì´íŠ¸ URL ìƒì„±
        radio_ambulante_url = f"https://radioambulante.org/audio/{slug}"
        
        # URL ìœ íš¨ì„± í™•ì¸
        if validate_url(radio_ambulante_url):
            return radio_ambulante_url
        
        # ìŠ¬ëŸ¬ê·¸ ìƒì„± ì‹¤íŒ¨ ì‹œ ìš”ì•½ì—ì„œ ë§í¬ ì°¾ê¸°
        summary = entry.get('summary', '') + entry.get('description', '')
        url_match = re.search(r'https://radioambulante\.org/audio/[^\s<>"]+', summary)
        if url_match:
            found_url = url_match.group(0)
            if validate_url(found_url):
                return found_url
                
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ None ë°˜í™˜
        return None
        
    except Exception as e:
        print(f"Radio Ambulante URL ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def validate_url(url, timeout=5):
    """Validate URL quickly"""
    try:
        if not url or not (url.startswith('http://') or url.startswith('https://')):
            return False
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)
        return response.status_code < 400
    except:
        return False

def main():
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
    reading_source = os.environ.get('READING_SOURCE', '')
    preset_difficulty = os.environ.get('READING_DIFFICULTY', 'B2')  # ê¸°ë³¸ê°’ìœ¼ë¡œë§Œ ì‚¬ìš©
    podcast_rss = os.environ.get('PODCAST_RSS', '')
    podcast_name = os.environ.get('PODCAST_NAME', '')
    weekday_name = os.environ.get('WEEKDAY_NAME', '')
    podcast_apple_base = os.environ.get('PODCAST_APPLE_BASE', '')
    
    article_data = None
    podcast_data = None  # ëª…ì‹œì ìœ¼ë¡œ Noneìœ¼ë¡œ ì´ˆê¸°í™”

    print(f"=== í•™ìŠµ ìë£Œ ìˆ˜ì§‘ ì‹œì‘ ===")
    print(f"ë…í•´ ì†ŒìŠ¤: {reading_source}")
    print(f"íŒŸìºìŠ¤íŠ¸: {podcast_name}")
    print(f"íŒŸìºìŠ¤íŠ¸ RSS: {podcast_rss}")
    print(f"ìš”ì¼: {weekday_name}")
    print(f"====================")

    # ê¸°ì‚¬ ìˆ˜ì§‘ ë° ì‹¤ì œ ë‚´ìš© ë¶„ì„
    try:
        if reading_source == "20minutos":
            feed_url = "https://www.20minutos.es/rss/"
        elif "El PaÃ­s" in reading_source:
            if "ì‚¬ì„¤" in reading_source:
                feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/opinion"
            else:
                feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada"
        
        print(f"RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {feed_url}")
        feed = feedparser.parse(feed_url)
        
        if feed.entries:
            latest = feed.entries[0]
            article_url = latest.link
            clean_title = latest.title.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
            
            print(f"ê¸°ì‚¬ URL ì ‘ì† ì¤‘: {article_url}")
            # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            article_content = get_article_content(article_url)
            
            if article_content:
                print(f"ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ ì¤‘... (ë‚´ìš© ê¸¸ì´: {len(article_content)}ì)")
                
                # ë™ì  ë‚œì´ë„ ë¶„ì„
                analyzed_difficulty = analyze_text_difficulty(article_content)
                print(f"ë¶„ì„ëœ ë‚œì´ë„: {analyzed_difficulty}")
                
                # ë¶„ì„ëœ ë‚œì´ë„ë¡œ ì–´íœ˜ ì¶”ì¶œ
                vocabulary = extract_vocabulary_from_content(article_content, analyzed_difficulty)
                category = extract_category_from_content(clean_title, article_content)
                
                print(f"ì¶”ì¶œëœ ì–´íœ˜: {vocabulary}")
                print(f"ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")
                
                article_data = {
                    'title': clean_title,
                    'url': article_url,
                    'published': latest.get('published', ''),
                    'category': category,
                    'vocabulary': vocabulary,
                    'difficulty': analyzed_difficulty,  # ë™ì ìœ¼ë¡œ ë¶„ì„ëœ ë‚œì´ë„ ì‚¬ìš©
                    'content_preview': article_content[:200] + "..." if len(article_content) > 200 else article_content
                }
            else:
                print("ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ RSS ìš”ì•½ ì‚¬ìš©")
                # ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
                summary = latest.get('summary', '')
                analyzed_difficulty = analyze_text_difficulty(summary) if summary else preset_difficulty
                vocabulary = extract_vocabulary_from_content(summary, analyzed_difficulty)
                category = extract_category_from_content(clean_title, summary)
                
                article_data = {
                    'title': clean_title,
                    'url': article_url,
                    'published': latest.get('published', ''),
                    'category': category,
                    'vocabulary': vocabulary,
                    'difficulty': analyzed_difficulty,
                    'content_preview': summary
                }
                
        else:
            print("RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
    except Exception as e:
        print(f"ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

    # íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
    try:
        print(f"íŒŸìºìŠ¤íŠ¸ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {podcast_rss}")
        feed = feedparser.parse(podcast_rss)
        
        print(f"í”¼ë“œ íŒŒì‹± ê²°ê³¼:")
        print(f"- í”¼ë“œ ì œëª©: {feed.feed.get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"- ì—í”¼ì†Œë“œ ê°œìˆ˜: {len(feed.entries)}")
        print(f"- í”¼ë“œ ìƒíƒœ: {getattr(feed, 'status', 'N/A')}")
        
        if hasattr(feed, 'bozo') and feed.bozo:
            print(f"- RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {getattr(feed, 'bozo_exception', 'Unknown')}")
            
        # í”¼ë“œ ìƒíƒœê°€ 404ì´ê±°ë‚˜ ì—í”¼ì†Œë“œê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ ë°±ì—… í”¼ë“œ ì‹œë„
        if (hasattr(feed, 'status') and feed.status == 404) or len(feed.entries) == 0:
            print(f"âš ï¸  ë©”ì¸ RSS í”¼ë“œ ì‚¬ìš© ë¶ˆê°€ (ìƒíƒœ: {getattr(feed, 'status', 'N/A')}, ì—í”¼ì†Œë“œ: {len(feed.entries)})")
            print("ë°±ì—… RSS í”¼ë“œë“¤ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            # ë°±ì—… RSS í”¼ë“œë“¤ ì‹œë„ - ë” ì •í™•í•œ í”¼ë“œ URLë“¤ ì‚¬ìš©
            backup_feeds = []
            
            # ìš”ì¼ì— ë”°ë¼ ì ì ˆí•œ ë°±ì—… í”¼ë“œë“¤ ì„¤ì •
            if weekday_name == "ìˆ˜ìš”ì¼":
                # ìˆ˜ìš”ì¼ì€ SpanishWithVicenteì´ì§€ë§Œ í”¼ë“œê°€ ì‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ì˜µì…˜ë“¤ ì‹œë„
                backup_feeds = [
                    ("https://www.hoyhablamos.com/podcast.rss", "Hoy Hablamos", "https://podcasts.apple.com/kr/podcast/hoy-hablamos-podcast-diario-para-aprender-espaÃ±ol-learn/id1201483158"),
                    ("https://feeds.npr.org/510311/podcast.xml", "Radio Ambulante", "https://podcasts.apple.com/kr/podcast/radio-ambulante/id527614348"),
                    ("https://anchor.fm/s/f4f4a4f0/podcast/rss", "DELE Podcast", "https://podcasts.apple.com/us/podcast/examen-dele/id1705001626")
                ]
            else:
                # ë‹¤ë¥¸ ìš”ì¼ë“¤ì˜ ì¼ë°˜ì ì¸ ë°±ì—… í”¼ë“œë“¤
                backup_feeds = [
                    ("https://www.hoyhablamos.com/podcast.rss", "Hoy Hablamos", "https://podcasts.apple.com/kr/podcast/hoy-hablamos-podcast-diario-para-aprender-espaÃ±ol-learn/id1201483158"),
                    ("https://feeds.npr.org/510311/podcast.xml", "Radio Ambulante", "https://podcasts.apple.com/kr/podcast/radio-ambulante/id527614348"),
                    ("https://anchor.fm/s/f4f4a4f0/podcast/rss", "DELE Podcast", "https://podcasts.apple.com/us/podcast/examen-dele/id1705001626"),
                    ("https://feeds.feedburner.com/SpanishWithVicente", "SpanishWithVicente (ëŒ€ì²´ í”¼ë“œ)", "https://podcasts.apple.com/kr/podcast/spanish-with-vicente/id1493547273")
                ]
            
            for backup_url, backup_podcast_name, backup_apple_base in backup_feeds:
                try:
                    print(f"ë°±ì—… í”¼ë“œ ì‹œë„: {backup_url} ({backup_podcast_name})")
                    backup_feed = feedparser.parse(backup_url)
                    
                    # ë°±ì—… í”¼ë“œ ìƒíƒœ í™•ì¸
                    backup_status = getattr(backup_feed, 'status', 'N/A')
                    print(f"ë°±ì—… í”¼ë“œ ìƒíƒœ: {backup_status}, ì—í”¼ì†Œë“œ ê°œìˆ˜: {len(backup_feed.entries)}")
                    
                    if backup_feed.entries:
                        # ë°±ì—… í”¼ë“œì—ì„œë„ ìµœê·¼ ì—í”¼ì†Œë“œ í™•ì¸
                        recent_episodes = []
                        for entry in backup_feed.entries[:3]:  # ë°±ì—…ì—ì„œëŠ” 3ê°œë§Œ í™•ì¸
                            if is_episode_recent(entry.get('published_parsed')):
                                recent_episodes.append(entry)
                        
                        if not recent_episodes:
                            recent_episodes = [backup_feed.entries[0]]  # ìµœì‹  ì—í”¼ì†Œë“œë¼ë„ ì‚¬ìš©
                        
                        latest = recent_episodes[0]
                        print(f"ë°±ì—… í”¼ë“œì—ì„œ ì„ íƒëœ ì—í”¼ì†Œë“œ:")
                        print(f"  ì œëª©: {latest.title}")
                        print(f"  ë°œí–‰ì¼: {latest.get('published', 'N/A')}")
                        print(f"  RSS ì—í”¼ì†Œë“œ URL: {latest.link}")
                        
                        episode_number = extract_episode_number(latest.title)
                        duration = extract_duration_from_feed(latest)
                        topic = extract_topic_keywords(latest.title, latest.get('summary', ''))
                        
                        # Radio Ambulanteì¸ ê²½ìš° ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ URL ì‹œë„
                        if 'Radio Ambulante' in backup_podcast_name:
                            radio_ambulante_url = extract_radio_ambulante_url(latest)
                            if radio_ambulante_url:
                                print(f"  Radio Ambulante ì›¹ì‚¬ì´íŠ¸ URL: {radio_ambulante_url}")
                                episode_link = radio_ambulante_url
                            else:
                                print(f"  Radio Ambulante ì›¹ì‚¬ì´íŠ¸ URL ì¶”ì¶œ ì‹¤íŒ¨, RSS URL ì‚¬ìš©")
                                episode_link = latest.link
                        else:
                            episode_link = latest.link
                        
                        # Apple Podcasts ë§í¬ ìƒì„± - ì—í”¼ì†Œë“œ ì œëª© í¬í•¨
                        apple_link = generate_apple_podcast_link(backup_podcast_name, backup_apple_base, episode_link, episode_number, latest.title)
                        
                        # Radio Ambulanteì˜ ê²½ìš° Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URLì„ ë©”ì¸ URLë¡œ ì‚¬ìš©
                        final_episode_url = episode_link
                        if 'Radio Ambulante' in backup_podcast_name:
                            # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
                            if apple_link != backup_apple_base and validate_url(apple_link):
                                # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ìœ¼ë©´ Apple URLì„ ì‚¬ìš©
                                print(f"  âœ… Apple Podcastsì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œ ì°¾ìŒ, Apple URL ì‚¬ìš©")
                                final_episode_url = apple_link
                            else:
                                # Appleì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ì›ë³¸ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                                print(f"  âŒ Apple Podcastsì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í•¨, NPR URL ì‚¬ìš©")
                                final_episode_url = episode_link
                                apple_link = backup_apple_base  # Apple ë§í¬ëŠ” ë©”ì¸ í˜ì´ì§€ë¡œ ì„¤ì •
                        else:
                            # ë‹¤ë¥¸ íŒŸìºìŠ¤íŠ¸ëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€
                            if not validate_url(episode_link):
                                print(f"  âš ï¸  ì—í”¼ì†Œë“œ ë§í¬ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {episode_link}")
                                final_episode_url = apple_link if validate_url(apple_link) else backup_apple_base
                                
                            if not validate_url(apple_link):
                                print(f"  âš ï¸  Apple ë§í¬ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {apple_link}")
                                apple_link = backup_apple_base
                        
                        podcast_data = {
                            'title': latest.title,
                            'url': final_episode_url,  # Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                            'apple_link': apple_link,
                            'published': latest.get('published', ''),
                            'duration': duration,
                            'episode_number': episode_number or 'N/A',
                            'topic': topic,
                            'podcast_name': backup_podcast_name,
                            'summary': latest.get('summary', '')[:200]
                        }
                        
                        print(f"âœ… ë°±ì—… í”¼ë“œ ì„±ê³µ! ì‚¬ìš©ëœ í”¼ë“œ: {backup_podcast_name}")
                        print(f"   ì—í”¼ì†Œë“œ: {latest.title}")
                        print(f"   ìµœì¢… ì—í”¼ì†Œë“œ URL: {final_episode_url}")
                        print(f"   Apple URL: {apple_link}")
                        print(f"   URL ê²€ì¦ ê²°ê³¼ - ì—í”¼ì†Œë“œ: {'âœ…' if validate_url(final_episode_url) else 'âŒ'}, Apple: {'âœ…' if validate_url(apple_link) else 'âŒ'}")
                        break
                except Exception as backup_e:
                    print(f"ë°±ì—… í”¼ë“œ ì˜¤ë¥˜ ({backup_podcast_name}): {backup_e}")
                    continue
        
        elif feed.entries:
            print(f"í”¼ë“œì—ì„œ ìµœì‹  ì—í”¼ì†Œë“œ í™•ì¸ ì¤‘...")
            
            # ìµœê·¼ ë©°ì¹  ì´ë‚´ì˜ ì—í”¼ì†Œë“œë§Œ í•„í„°ë§
            recent_episodes = []
            for entry in feed.entries[:5]:  # ìµœê·¼ 5ê°œ ì—í”¼ì†Œë“œë§Œ í™•ì¸
                print(f"  ì—í”¼ì†Œë“œ í™•ì¸: {entry.title}")
                print(f"    ë°œí–‰ì¼: {entry.get('published', 'N/A')}")
                
                if is_episode_recent(entry.get('published_parsed')):
                    recent_episodes.append(entry)
                    print(f"    âœ… ìµœê·¼ ì—í”¼ì†Œë“œë¡œ í™•ì¸ë¨")
                else:
                    print(f"    âŒ ì˜¤ë˜ëœ ì—í”¼ì†Œë“œ")
            
            if not recent_episodes:
                print("âš ï¸  ìµœê·¼ ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ê°€ì¥ ìµœì‹  ì—í”¼ì†Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                recent_episodes = [feed.entries[0]]
            
            latest = recent_episodes[0]
            print(f"ì„ íƒëœ ì—í”¼ì†Œë“œ: {latest.title}")
            print(f"- ë§í¬: {latest.link}")
            print(f"- ë°œí–‰ì¼: {latest.get('published', 'N/A')}")
            print(f"- ìš”ì•½ ê¸¸ì´: {len(latest.get('summary', ''))}")
            
            episode_number = extract_episode_number(latest.title)
            duration = extract_duration_from_feed(latest)
            topic = extract_topic_keywords(latest.title, latest.get('summary', ''))
            
            episode_link = latest.link
            
            # ì—í”¼ì†Œë“œ ë§í¬ ìœ íš¨ì„± ê²€ì¦
            if not validate_url(episode_link):
                print(f"âš ï¸  ì—í”¼ì†Œë“œ ë§í¬ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {episode_link}")
                episode_link = podcast_apple_base  # ê¸°ë³¸ê°’ìœ¼ë¡œ Apple Podcasts ì‚¬ìš©
            
            # Apple Podcasts ë§í¬ ìƒì„± - ì—í”¼ì†Œë“œ ì œëª© í¬í•¨
            apple_link = generate_apple_podcast_link(podcast_name, podcast_apple_base, episode_link, episode_number, latest.title)
            
            # Radio Ambulanteì˜ ê²½ìš° Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URLì„ ë©”ì¸ URLë¡œ ì‚¬ìš©
            final_episode_url = episode_link
            if 'Radio Ambulante' in podcast_name:
                # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
                if apple_link != podcast_apple_base and validate_url(apple_link):
                    # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ìœ¼ë©´ Apple URLì„ ì‚¬ìš©
                    print(f"âœ… Apple Podcastsì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œ ì°¾ìŒ, Apple URL ì‚¬ìš©")
                    final_episode_url = apple_link
                else:
                    # Appleì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ì›ë³¸ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                    print(f"âŒ Apple Podcastsì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í•¨, NPR URL ì‚¬ìš©")
                    final_episode_url = episode_link
                    apple_link = podcast_apple_base  # Apple ë§í¬ëŠ” ë©”ì¸ í˜ì´ì§€ë¡œ ì„¤ì •
            else:
                # ë‹¤ë¥¸ íŒŸìºìŠ¤íŠ¸ëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€
                if not validate_url(apple_link):
                    print(f"âš ï¸  Apple Podcasts ë§í¬ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ, ê¸°ë³¸ ë§í¬ ì‚¬ìš©")
                    apple_link = podcast_apple_base
            
            podcast_data = {
                'title': latest.title,
                'url': final_episode_url,  # Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                'apple_link': apple_link,
                'published': latest.get('published', ''),
                'duration': duration,
                'episode_number': episode_number or 'N/A',
                'topic': topic,
                'podcast_name': podcast_name,
                'summary': latest.get('summary', '')[:200]
            }
            
            print(f"íŒŸìºìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
            print(f"- ì—í”¼ì†Œë“œ ë²ˆí˜¸: {episode_number}")
            print(f"- ì¬ìƒì‹œê°„: {duration}")
            print(f"- ì£¼ì œ: {topic}")
            print(f"- ìµœì¢… ì—í”¼ì†Œë“œ URL: {final_episode_url}")
            print(f"- Apple Podcasts URL: {apple_link}")
            print(f"- URL ìœ íš¨ì„± - ì—í”¼ì†Œë“œ: {'âœ…' if validate_url(final_episode_url) else 'âŒ'}, Apple: {'âœ…' if validate_url(apple_link) else 'âŒ'}")
            
        else:
            print("ë©”ì¸ í”¼ë“œì— ì—í”¼ì†Œë“œê°€ ì—†ìŒ - ì´ ê²½ìš°ëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë¨")
            
    except Exception as e:
        print(f"íŒŸìºìŠ¤íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    # í•™ìŠµ ìë£Œ ì •ë³´ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì¶œë ¥
    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
        if article_data:
            f.write(f"article_title={article_data['title']}\n")
            f.write(f"article_url={article_data['url']}\n")
            f.write(f"article_category={article_data['category']}\n")
            f.write(f"article_vocabulary={', '.join(article_data['vocabulary'])}\n")
            f.write(f"article_difficulty={article_data['difficulty']}\n")  # ë™ì  ë‚œì´ë„ ì¶œë ¥
            f.write(f"article_memo={create_detailed_memo('article', article_data, weekday_name)}\n")
        
        if podcast_data:
            f.write(f"podcast_title={podcast_data['title']}\n")
            f.write(f"podcast_url={podcast_data['url']}\n")
            f.write(f"podcast_apple={podcast_data['apple_link']}\n")
            f.write(f"podcast_duration={podcast_data['duration']}\n")
            f.write(f"podcast_topic={podcast_data['topic']}\n")
            f.write(f"podcast_memo={create_detailed_memo('podcast', podcast_data, weekday_name)}\n")

    print("í•™ìŠµ ìë£Œ ìˆ˜ì§‘ ì™„ë£Œ!")
    if article_data:
        print(f"âœ… ê¸°ì‚¬: {article_data['title']}")
        print(f"   ì¹´í…Œê³ ë¦¬: {article_data['category']}")
        print(f"   ë‚œì´ë„: {article_data['difficulty']}")  # ë™ì  ë‚œì´ë„ ì¶œë ¥
        print(f"   ì–´íœ˜: {article_data['vocabulary']}")
    else:
        print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨")
        
    if podcast_data:
        print(f"âœ… íŒŸìºìŠ¤íŠ¸: {podcast_data['title']}")
        print(f"   ì£¼ì œ: {podcast_data['topic']}")
        print(f"   ì¬ìƒì‹œê°„: {podcast_data['duration']}")
    else:
        print(f"âŒ íŒŸìºìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
        print(f"   RSS URL: {podcast_rss}")
        print(f"   íŒŸìºìŠ¤íŠ¸ëª…: {podcast_name}")

if __name__ == "__main__":
    main()
