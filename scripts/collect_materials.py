#!/usr/bin/env python3
"""
Collect Spanish learning materials: articles and podcast episodes with content analysis.
"""
import os
import sys
import requests
import feedparser
from datetime import datetime
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time

def get_article_content(url):
    """ì‹¤ì œ ê¸°ì‚¬ URLì— ì ‘ì†í•´ì„œ ë³¸ë¬¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì‚¬ì´íŠ¸ë³„ ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
        content = ""
        
        if '20minutos.es' in url:
            # 20minutos ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('div', class_='article-text') or soup.find('div', class_='content')
            if article_body:
                paragraphs = article_body.find_all(['p', 'div'])
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
        
        elif 'elpais.com' in url:
            # El PaÃ­s ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('div', {'data-dtm-region': 'articulo_cuerpo'}) or \
                         soup.find('div', class_='a_c clearfix') or \
                         soup.find('div', class_='articulo-cuerpo')
            if article_body:
                paragraphs = article_body.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
        
        # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (fallback)
        if not content:
            # ì¼ë°˜ì ì¸ article íƒœê·¸ë‚˜ main íƒœê·¸ì—ì„œ ì¶”ì¶œ
            article = soup.find('article') or soup.find('main')
            if article:
                paragraphs = article.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()][:10])  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨ë§Œ
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if len(content) < 200:
            all_paragraphs = soup.find_all('p')
            content = ' '.join([p.get_text().strip() for p in all_paragraphs if len(p.get_text().strip()) > 50][:8])
        
        return content[:2000]  # ì²˜ìŒ 2000ìë§Œ ë°˜í™˜
        
    except Exception as e:
        print(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def extract_vocabulary_from_content(content, difficulty="B2"):
    """ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš©ì—ì„œ í•™ìŠµì ìˆ˜ì¤€ì— ë§ëŠ” í•µì‹¬ ì–´íœ˜ë¥¼ ì¶”ì¶œ"""
    if not content:
        return []
    
    # ìˆ˜ì¤€ë³„ ì–´íœ˜ ì¶”ì¶œ ê¸°ì¤€
    if difficulty == "B2":
        # B2 ìˆ˜ì¤€: ì¤‘ê¸‰ ì–´íœ˜, ì¼ìƒì ì´ì§€ ì•Šì€ í‘œí˜„ë“¤
        target_patterns = [
            # ë™ì‚¬ë¥˜
            r'\b(desarrollar|establecer|implementar|generar|promover|fortalecer|consolidar|impulsar|fomentar|garantizar)\b',
            # ëª…ì‚¬ë¥˜  
            r'\b(iniciativa|propuesta|medida|estrategia|polÃ­tica|programa|proyecto|inversiÃ³n|desarrollo|crecimiento)\b',
            # í˜•ìš©ì‚¬ë¥˜
            r'\b(fundamental|esencial|crucial|significativo|relevante|considerable|notable|destacado|principal|prioritario)\b',
            # ì—°ê²°ì–´
            r'\b(ademÃ¡s|asimismo|por tanto|sin embargo|no obstante|en consecuencia|por consiguiente|en definitiva)\b',
            # ê¸°ê´€/ì¡°ì§ ê´€ë ¨
            r'\b(entidad|organismo|instituciÃ³n|administraciÃ³n|departamento|ministerio|ayuntamiento|comunidad)\b'
        ]
    elif difficulty == "C1":
        # C1 ìˆ˜ì¤€: ê³ ê¸‰ ì–´íœ˜, ì „ë¬¸ì /í•™ìˆ ì  í‘œí˜„ë“¤
        target_patterns = [
            # ê³ ê¸‰ ë™ì‚¬
            r'\b(implementar|consolidar|incrementar|optimizar|diversificar|potenciar|materializar|vehicular|canalizar)\b',
            # ì „ë¬¸ ëª…ì‚¬
            r'\b(sostenibilidad|competitividad|rentabilidad|eficiencia|transparencia|gobernanza|paradigma|metodologÃ­a)\b',
            # ê³ ê¸‰ í˜•ìš©ì‚¬
            r'\b(innovador|sostenible|competitivo|eficiente|transparente|inclusivo|participativo|colaborativo)\b',
            # í•™ìˆ ì  ì—°ê²°ì–´
            r'\b(en este sentido|cabe destacar|es preciso|conviene seÃ±alar|resulta evidente|se constata)\b',
            # ì „ë¬¸ ë¶„ì•¼ ìš©ì–´
            r'\b(digitalizaciÃ³n|transformaciÃ³n|modernizaciÃ³n|reestructuraciÃ³n|reconversiÃ³n|reorientaciÃ³n)\b'
        ]
    else:
        # ê¸°ë³¸ B2 íŒ¨í„´ ì‚¬ìš©
        target_patterns = [
            r'\b(desarrollar|establecer|implementar|medida|estrategia|fundamental|ademÃ¡s|sin embargo)\b'
        ]
    
    found_vocab = []
    content_lower = content.lower()
    
    # ì–´íœ˜ ì¶”ì¶œ ë° ì˜ë¯¸ ë§¤í•‘
    vocab_meanings = {
        # B2 ìˆ˜ì¤€ ì–´íœ˜
        'desarrollar': 'ê°œë°œí•˜ë‹¤, ë°œì „ì‹œí‚¤ë‹¤',
        'establecer': 'ì„¤ë¦½í•˜ë‹¤, í™•ë¦½í•˜ë‹¤',
        'implementar': 'ì‹œí–‰í•˜ë‹¤, êµ¬í˜„í•˜ë‹¤',
        'generar': 'ìƒì„±í•˜ë‹¤, ë§Œë“¤ì–´ë‚´ë‹¤',
        'promover': 'ì´‰ì§„í•˜ë‹¤, ì¥ë ¤í•˜ë‹¤',
        'fortalecer': 'ê°•í™”í•˜ë‹¤',
        'consolidar': 'í†µí•©í•˜ë‹¤, ê²¬ê³ íˆ í•˜ë‹¤',
        'impulsar': 'ì¶”ì§„í•˜ë‹¤, ì´‰ì§„í•˜ë‹¤',
        'fomentar': 'ì¥ë ¤í•˜ë‹¤, ì´‰ì§„í•˜ë‹¤',
        'garantizar': 'ë³´ì¥í•˜ë‹¤',
        'iniciativa': 'ê³„íš, ì£¼ë„ê¶Œ',
        'propuesta': 'ì œì•ˆ',
        'medida': 'ì¡°ì¹˜, ëŒ€ì±…',
        'estrategia': 'ì „ëµ',
        'polÃ­tica': 'ì •ì±…',
        'programa': 'í”„ë¡œê·¸ë¨',
        'proyecto': 'í”„ë¡œì íŠ¸',
        'inversiÃ³n': 'íˆ¬ì',
        'desarrollo': 'ë°œì „, ê°œë°œ',
        'crecimiento': 'ì„±ì¥',
        'fundamental': 'ê¸°ë³¸ì ì¸, ê·¼ë³¸ì ì¸',
        'esencial': 'í•„ìˆ˜ì ì¸',
        'crucial': 'ì¤‘ìš”í•œ, ê²°ì •ì ì¸',
        'significativo': 'ì˜ë¯¸ìˆëŠ”, ì¤‘ìš”í•œ',
        'relevante': 'ê´€ë ¨ìˆëŠ”, ì¤‘ìš”í•œ',
        'considerable': 'ìƒë‹¹í•œ',
        'notable': 'ì£¼ëª©í•  ë§Œí•œ',
        'destacado': 'ë›°ì–´ë‚œ, ë‘ë“œëŸ¬ì§„',
        'principal': 'ì£¼ìš”í•œ',
        'prioritario': 'ìš°ì„ ì ì¸',
        'ademÃ¡s': 'ê²Œë‹¤ê°€, ë˜í•œ',
        'asimismo': 'ë§ˆì°¬ê°€ì§€ë¡œ',
        'por tanto': 'ë”°ë¼ì„œ',
        'sin embargo': 'ê·¸ëŸ¬ë‚˜',
        'no obstante': 'ê·¸ëŸ¼ì—ë„ ë¶ˆêµ¬í•˜ê³ ',
        'en consecuencia': 'ê²°ê³¼ì ìœ¼ë¡œ',
        'por consiguiente': 'ë”°ë¼ì„œ',
        'en definitiva': 'ê²°êµ­',
        'entidad': 'ê¸°ê´€, ë‹¨ì²´',
        'organismo': 'ê¸°ê´€, ì¡°ì§',
        'instituciÃ³n': 'ê¸°ê´€, ì œë„',
        'administraciÃ³n': 'í–‰ì •ë¶€',
        'departamento': 'ë¶€ì„œ',
        'ministerio': 'ë¶€ (ì •ë¶€ê¸°ê´€)',
        'ayuntamiento': 'ì‹œì²­',
        'comunidad': 'ì§€ì—­ì‚¬íšŒ, ê³µë™ì²´',
        
        # C1 ìˆ˜ì¤€ ì–´íœ˜
        'incrementar': 'ì¦ê°€ì‹œí‚¤ë‹¤',
        'optimizar': 'ìµœì í™”í•˜ë‹¤',
        'diversificar': 'ë‹¤ì–‘í™”í•˜ë‹¤',
        'potenciar': 'ê°•í™”í•˜ë‹¤, ì ì¬ë ¥ì„ í‚¤ìš°ë‹¤',
        'materializar': 'ì‹¤í˜„í•˜ë‹¤',
        'vehicular': 'ì „ë‹¬í•˜ë‹¤, ìˆ˜ë‹¨ì´ ë˜ë‹¤',
        'canalizar': 'ê²½ë¡œë¥¼ ì œê³µí•˜ë‹¤',
        'sostenibilidad': 'ì§€ì†ê°€ëŠ¥ì„±',
        'competitividad': 'ê²½ìŸë ¥',
        'rentabilidad': 'ìˆ˜ìµì„±',
        'eficiencia': 'íš¨ìœ¨ì„±',
        'transparencia': 'íˆ¬ëª…ì„±',
        'gobernanza': 'ê±°ë²„ë„ŒìŠ¤, í†µì¹˜',
        'paradigma': 'íŒ¨ëŸ¬ë‹¤ì„',
        'metodologÃ­a': 'ë°©ë²•ë¡ ',
        'innovador': 'í˜ì‹ ì ì¸',
        'sostenible': 'ì§€ì†ê°€ëŠ¥í•œ',
        'competitivo': 'ê²½ìŸì ì¸',
        'eficiente': 'íš¨ìœ¨ì ì¸',
        'transparente': 'íˆ¬ëª…í•œ',
        'inclusivo': 'í¬ìš©ì ì¸',
        'participativo': 'ì°¸ì—¬ì ì¸',
        'colaborativo': 'í˜‘ë ¥ì ì¸',
        'digitalizaciÃ³n': 'ë””ì§€í„¸í™”',
        'transformaciÃ³n': 'ë³€í™”, ë³€í˜',
        'modernizaciÃ³n': 'í˜„ëŒ€í™”',
        'reestructuraciÃ³n': 'êµ¬ì¡°ì¡°ì •',
        'reconversiÃ³n': 'ì „í™˜',
        'reorientaciÃ³n': 'ë°©í–¥ ì „í™˜'
    }
    
    # íŒ¨í„´ ë§¤ì¹­ìœ¼ë¡œ ì–´íœ˜ ì°¾ê¸°
    for pattern in target_patterns:
        matches = re.findall(pattern, content_lower, re.IGNORECASE)
        for match in matches:
            if match.lower() in vocab_meanings and match.lower() not in [v.split(' (')[0].lower() for v in found_vocab]:
                meaning = vocab_meanings[match.lower()]
                found_vocab.append(f"{match} ({meaning})")
    
    # ì¤‘ë³µ ì œê±° ë° ìµœëŒ€ 5ê°œ ë°˜í™˜
    unique_vocab = []
    seen = set()
    for vocab in found_vocab:
        word = vocab.split(' (')[0].lower()
        if word not in seen:
            unique_vocab.append(vocab)
            seen.add(word)
            if len(unique_vocab) >= 5:
                break
    
    return unique_vocab

def extract_category_from_content(title, content):
    """ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
    full_text = (title + " " + content).lower()
    
    keywords = {
        'ì •ì¹˜': ['gobierno', 'polÃ­tica', 'elecciones', 'parlamento', 'ministro', 'rey', 'presidente', 'votaciÃ³n', 'congreso'],
        'ê²½ì œ': ['economÃ­a', 'banco', 'euro', 'empleo', 'crisis', 'mercado', 'dinero', 'trabajo', 'empresa', 'inversiÃ³n'],
        'ì‚¬íšŒ': ['sociedad', 'educaciÃ³n', 'sanidad', 'vivienda', 'familia', 'salud', 'poblaciÃ³n', 'ciudadanos'],
        'ìŠ¤í¬ì¸ ': ['fÃºtbol', 'real madrid', 'barcelona', 'liga', 'deporte', 'partido', 'atletico', 'champions'],
        'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³vil', 'digital', 'app', 'inteligencia', 'innovaciÃ³n'],
        'ë¬¸í™”': ['cultura', 'arte', 'mÃºsica', 'teatro', 'festival', 'libro', 'cine', 'exposiciÃ³n'],
        'êµ­ì œ': ['internacional', 'mundial', 'europa', 'amÃ©rica', 'china', 'estados unidos', 'uniÃ³n europea']
    }
    
    category_scores = {}
    for category, words in keywords.items():
        score = sum(1 for word in words if word in full_text)
        if score > 0:
            category_scores[category] = score
    
    if category_scores:
        return max(category_scores, key=category_scores.get)
    return 'ì¼ë°˜'

def extract_episode_number(title):
    patterns = [
        r'Ep\.?\s*(\d+)',
        r'Episode\s*(\d+)',
        r'#(\d+)',
        r'(\d{3,4})'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, title, re.IGNORECASE)
        if match:
            return match.group(1)
    return None

def extract_duration_from_feed(entry):
    # iTunes ë“€ë ˆì´ì…˜ ë¨¼ì € í™•ì¸
    if hasattr(entry, 'itunes_duration'):
        duration = entry.itunes_duration
        # ì´ˆ ë‹¨ìœ„ì¸ ê²½ìš° ë¶„:ì´ˆë¡œ ë³€í™˜
        if duration.isdigit():
            total_seconds = int(duration)
            minutes = total_seconds // 60
            seconds = total_seconds % 60
            return f"{minutes}:{seconds:02d}"
        return duration
    
    # ìš”ì•½ì—ì„œ ì¬ìƒì‹œê°„ ì¶”ì¶œ ì‹œë„
    summary = entry.get('summary', '') + entry.get('description', '')
    duration_patterns = [
        r'(\d+)\s*min',
        r'(\d+)\s*ë¶„',
        r'(\d+):(\d+)',
        r'Duration:\s*(\d+)'
    ]
    
    for pattern in duration_patterns:
        match = re.search(pattern, summary)
        if match:
            if ':' in pattern:
                return f"{match.group(1)}:{match.group(2)}"
            else:
                return f"{match.group(1)}ë¶„"
    
    return "15-25ë¶„"

def extract_topic_keywords(title, summary=""):
    content = (title + " " + summary).lower()
    
    topic_keywords = {
        'ë¬¸ë²•': ['gramÃ¡tica', 'verbos', 'subjuntivo', 'pretÃ©rito', 'sintaxis'],
        'ë¬¸í™”': ['cultura', 'tradiciÃ³n', 'costumbres', 'historia', 'arte'],
        'ìš”ë¦¬': ['cocina', 'comida', 'receta', 'gastronomÃ­a', 'plato'],
        'ì—¬í–‰': ['viajes', 'turismo', 'ciudades', 'lugares', 'destinos'],
        'ì§ì—…': ['trabajo', 'empleo', 'profesiÃ³n', 'carrera', 'oficina'],
        'ê°€ì¡±': ['familia', 'padres', 'hijos', 'matrimonio', 'casa'],
        'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³viles', 'digital', 'aplicaciones'],
        'ì •ì¹˜': ['polÃ­tica', 'gobierno', 'elecciones', 'democracia'],
        'ê²½ì œ': ['economÃ­a', 'dinero', 'banco', 'trabajo', 'crisis', 'preferentes', 'ahorros'],
        'ì‚¬íšŒ': ['sociedad', 'gente', 'problemas', 'cambios', 'vida'],
        'ê±´ê°•': ['salud', 'medicina', 'hospital', 'enfermedad', 'mÃ©dico'],
        'êµìœ¡': ['educaciÃ³n', 'estudiantes', 'universidad', 'aprender']
    }
    
    for topic, keywords in topic_keywords.items():
        if any(keyword in content for keyword in keywords):
            return topic
    return 'ì¼ë°˜ ì£¼ì œ'

def create_detailed_memo(content_type, data, weekday_name):
    if content_type == "article":
        category = data.get('category', 'ì¼ë°˜')
        vocabulary = data.get('vocabulary', [])
        difficulty = data.get('difficulty', 'B2')
        
        vocab_text = ""
        if vocabulary:
            vocab_list = ", ".join(vocabulary[:3])  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            vocab_text = f"ğŸ“š í•µì‹¬ ì–´íœ˜: {vocab_list} "
        
        return (f"ğŸ“° {category} ë¶„ì•¼ ê¸°ì‚¬ ({difficulty} ìˆ˜ì¤€) "
               f"ğŸ“… ë°œí–‰: {data.get('published', 'ì˜¤ëŠ˜')} "
               f"ğŸ¯ í•™ìŠµëª©í‘œ: 15ë¶„ ë…í•´, {difficulty} ìˆ˜ì¤€ ì–´íœ˜ ì •ë¦¬ "
               f"{vocab_text}"
               f"ğŸ“ ê¶Œì¥: ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ì„ í†µí•œ ë§ì¶¤ ì–´íœ˜ í•™ìŠµ")

    elif content_type == "podcast":
        podcast_name = data.get('podcast_name', '')
        duration = data.get('duration', '15-25ë¶„')
        topic = data.get('topic', 'ì¼ë°˜ ì£¼ì œ')
        episode_num = data.get('episode_number', '')
        
        # ì£¼ì œì— ë”°ë¥¸ í•™ìŠµëª©í‘œ ì„¤ì •
        learning_goals = {
            'ê²½ì œ': 'ê¸ˆìœµ ì–´íœ˜',
            'ì •ì¹˜': 'ì •ì¹˜ ìš©ì–´',
            'ë¬¸í™”': 'ë¬¸í™” í‘œí˜„',
            'ì‚¬íšŒ': 'ì‚¬íšŒ ì´ìŠˆ ì–´íœ˜',
            'êµìœ¡': 'êµìœ¡ ê´€ë ¨ ì–´íœ˜',
            'ê±´ê°•': 'ì˜ë£Œ ìš©ì–´',
            'ê¸°ìˆ ': 'ê¸°ìˆ  ìš©ì–´',
            'ë¬¸ë²•': 'ë¬¸ë²• êµ¬ì¡°'
        }
        goal = learning_goals.get(topic, 'í•µì‹¬ ì–´íœ˜')
        
        # ì¬ìƒì‹œê°„ì— ë”°ë¥¸ ì²­ì·¨ ê³„íš ì„¤ì •
        if ':' in duration:
            try:
                minutes, seconds = duration.split(':')
                total_minutes = int(minutes)
                if total_minutes > 30:
                    listen_plan = f"(30ë¶„ ì²­ì·¨ ëª©í‘œ)"
                elif total_minutes > 20:
                    listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
                else:
                    listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
            except:
                listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
        else:
            listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
        
        return (f"ğŸ§ {podcast_name} Ep.{episode_num} - {weekday_name} ìŠ¤í˜ì¸ íŒŸìºìŠ¤íŠ¸ "
               f"â±ï¸ ì¬ìƒì‹œê°„: {duration} {listen_plan} "
               f"ğŸ¯ í•™ìŠµëª©í‘œ: {goal} 5ê°œ ì •ë¦¬ "
               f"ğŸŒ ì£¼ì œ: {topic} "
               f"ğŸ“ ê¶Œì¥: í•µì‹¬ ì–´íœ˜ì— ì§‘ì¤‘í•˜ì—¬ ì²­ì·¨")

def main():
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
    reading_source = os.environ.get('READING_SOURCE', '')
    reading_difficulty = os.environ.get('READING_DIFFICULTY', 'B2')
    podcast_rss = os.environ.get('PODCAST_RSS', '')
    podcast_name = os.environ.get('PODCAST_NAME', '')
    weekday_name = os.environ.get('WEEKDAY_NAME', '')
    podcast_apple_base = os.environ.get('PODCAST_APPLE_BASE', '')
    
    article_data = None
    podcast_data = None  # ëª…ì‹œì ìœ¼ë¡œ Noneìœ¼ë¡œ ì´ˆê¸°í™”

    print(f"=== í•™ìŠµ ìë£Œ ìˆ˜ì§‘ ì‹œì‘ ===")
    print(f"ë…í•´ ì†ŒìŠ¤: {reading_source}")
    print(f"íŒŸìºìŠ¤íŠ¸: {podcast_name}")
    print(f"íŒŸìºìŠ¤íŠ¸ RSS: {podcast_rss}")
    print(f"ìš”ì¼: {weekday_name}")
    print(f"====================")

    # ê¸°ì‚¬ ìˆ˜ì§‘ ë° ì‹¤ì œ ë‚´ìš© ë¶„ì„
    try:
        if reading_source == "20minutos":
            feed_url = "https://www.20minutos.es/rss/"
        elif "El PaÃ­s" in reading_source:
            if "ì‚¬ì„¤" in reading_source:
                feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/opinion"
            else:
                feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada"
        
        print(f"RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {feed_url}")
        feed = feedparser.parse(feed_url)
        
        if feed.entries:
            latest = feed.entries[0]
            article_url = latest.link
            clean_title = latest.title.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
            
            print(f"ê¸°ì‚¬ URL ì ‘ì† ì¤‘: {article_url}")
            # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            article_content = get_article_content(article_url)
            
            if article_content:
                print(f"ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ ì¤‘... (ë‚´ìš© ê¸¸ì´: {len(article_content)}ì)")
                # ì‹¤ì œ ë‚´ìš©ì—ì„œ ì–´íœ˜ ì¶”ì¶œ
                vocabulary = extract_vocabulary_from_content(article_content, reading_difficulty)
                category = extract_category_from_content(clean_title, article_content)
                
                print(f"ì¶”ì¶œëœ ì–´íœ˜: {vocabulary}")
                print(f"ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")
                
                article_data = {
                    'title': clean_title,
                    'url': article_url,
                    'published': latest.get('published', ''),
                    'category': category,
                    'vocabulary': vocabulary,
                    'difficulty': reading_difficulty,
                    'content_preview': article_content[:200] + "..." if len(article_content) > 200 else article_content
                }
            else:
                print("ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ RSS ìš”ì•½ ì‚¬ìš©")
                # ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
                summary = latest.get('summary', '')
                vocabulary = extract_vocabulary_from_content(summary, reading_difficulty)
                category = extract_category_from_content(clean_title, summary)
                
                article_data = {
                    'title': clean_title,
                    'url': article_url,
                    'published': latest.get('published', ''),
                    'category': category,
                    'vocabulary': vocabulary,
                    'difficulty': reading_difficulty,
                    'content_preview': summary
                }
                
        else:
            print("RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
    except Exception as e:
        print(f"ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")

    # íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
    try:
        print(f"íŒŸìºìŠ¤íŠ¸ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {podcast_rss}")
        feed = feedparser.parse(podcast_rss)
        
        print(f"í”¼ë“œ íŒŒì‹± ê²°ê³¼:")
        print(f"- í”¼ë“œ ì œëª©: {feed.feed.get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"- ì—í”¼ì†Œë“œ ê°œìˆ˜: {len(feed.entries)}")
        print(f"- í”¼ë“œ ìƒíƒœ: {getattr(feed, 'status', 'N/A')}")
        
        if hasattr(feed, 'bozo') and feed.bozo:
            print(f"- RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {getattr(feed, 'bozo_exception', 'Unknown')}")
        
        if feed.entries:
            latest = feed.entries[0]
            print(f"ìµœì‹  ì—í”¼ì†Œë“œ ì •ë³´:")
            print(f"- ì œëª©: {latest.title}")
            print(f"- ë§í¬: {latest.link}")
            print(f"- ë°œí–‰ì¼: {latest.get('published', 'N/A')}")
            print(f"- ìš”ì•½ ê¸¸ì´: {len(latest.get('summary', ''))}")
            
            episode_number = extract_episode_number(latest.title)
            duration = extract_duration_from_feed(latest)
            topic = extract_topic_keywords(latest.title, latest.get('summary', ''))
            
            episode_link = latest.link
            
            # Apple Podcasts ë§í¬ ìƒì„± ê°œì„ 
            if 'npr.org' in episode_link or 'radioambulante' in episode_link:
                apple_link = podcast_apple_base
            else:
                apple_link = f"{podcast_apple_base}?i={episode_number}" if episode_number else podcast_apple_base
            
            podcast_data = {
                'title': latest.title,
                'url': episode_link,
                'apple_link': apple_link,
                'published': latest.get('published', ''),
                'duration': duration,
                'episode_number': episode_number or 'N/A',
                'topic': topic,
                'podcast_name': podcast_name,
                'summary': latest.get('summary', '')[:200]
            }
            
            print(f"íŒŸìºìŠ¤íŠ¸ ë°ì´í„° ìƒì„± ì™„ë£Œ:")
            print(f"- ì—í”¼ì†Œë“œ ë²ˆí˜¸: {episode_number}")
            print(f"- ì¬ìƒì‹œê°„: {duration}")
            print(f"- ì£¼ì œ: {topic}")
            
        else:
            print("íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            print("ë‹¤ë¥¸ RSS í”¼ë“œë¥¼ ì‹œë„í•©ë‹ˆë‹¤...")
            
            # ë°±ì—… RSS í”¼ë“œë“¤ ì‹œë„
            backup_feeds = [
                "https://feeds.feedburner.com/hoyhablamos",  # Hoy Hablamos
                "https://feeds.npr.org/510311/podcast.xml",   # Radio Ambulante
                "https://anchor.fm/s/f4f4a4f0/podcast/rss"    # DELE Podcast
            ]
            
            for backup_url in backup_feeds:
                try:
                    print(f"ë°±ì—… í”¼ë“œ ì‹œë„: {backup_url}")
                    backup_feed = feedparser.parse(backup_url)
                    if backup_feed.entries:
                        latest = backup_feed.entries[0]
                        episode_number = extract_episode_number(latest.title)
                        duration = extract_duration_from_feed(latest)
                        topic = extract_topic_keywords(latest.title, latest.get('summary', ''))
                        
                        podcast_data = {
                            'title': latest.title,
                            'url': latest.link,
                            'apple_link': podcast_apple_base,
                            'published': latest.get('published', ''),
                            'duration': duration,
                            'episode_number': episode_number or 'N/A',
                            'topic': topic,
                            'podcast_name': f"{podcast_name} (ë°±ì—…)",
                            'summary': latest.get('summary', '')[:200]
                        }
                        print(f"ë°±ì—… í”¼ë“œì—ì„œ ì—í”¼ì†Œë“œ ì°¾ìŒ: {latest.title}")
                        break
                except Exception as backup_e:
                    print(f"ë°±ì—… í”¼ë“œ ì˜¤ë¥˜: {backup_e}")
                    continue
            
    except Exception as e:
        print(f"íŒŸìºìŠ¤íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    # í•™ìŠµ ìë£Œ ì •ë³´ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì¶œë ¥
    with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
        if article_data:
            f.write(f"article_title={article_data['title']}\n")
            f.write(f"article_url={article_data['url']}\n")
            f.write(f"article_category={article_data['category']}\n")
            f.write(f"article_vocabulary={', '.join(article_data['vocabulary'])}\n")
            f.write(f"article_memo={create_detailed_memo('article', article_data, weekday_name)}\n")
        
        if podcast_data:
            f.write(f"podcast_title={podcast_data['title']}\n")
            f.write(f"podcast_url={podcast_data['url']}\n")
            f.write(f"podcast_apple={podcast_data['apple_link']}\n")
            f.write(f"podcast_duration={podcast_data['duration']}\n")
            f.write(f"podcast_topic={podcast_data['topic']}\n")
            f.write(f"podcast_memo={create_detailed_memo('podcast', podcast_data, weekday_name)}\n")

    print("í•™ìŠµ ìë£Œ ìˆ˜ì§‘ ì™„ë£Œ!")
    if article_data:
        print(f"âœ… ê¸°ì‚¬: {article_data['title']}")
        print(f"   ì¹´í…Œê³ ë¦¬: {article_data['category']}")
        print(f"   ì–´íœ˜: {article_data['vocabulary']}")
    else:
        print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨")
        
    if podcast_data:
        print(f"âœ… íŒŸìºìŠ¤íŠ¸: {podcast_data['title']}")
        print(f"   ì£¼ì œ: {podcast_data['topic']}")
        print(f"   ì¬ìƒì‹œê°„: {podcast_data['duration']}")
    else:
        print(f"âŒ íŒŸìºìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
        print(f"   RSS URL: {podcast_rss}")
        print(f"   íŒŸìºìŠ¤íŠ¸ëª…: {podcast_name}")

if __name__ == "__main__":
    main()
