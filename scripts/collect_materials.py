#!/usr/bin/env python3
"""
Collect Spanish learning materials: articles and podcast episodes with LLM-powered content analysis.
"""
import os
import sys
import requests
import feedparser
import re
import time
import random
import traceback
import urllib.parse
from datetime import datetime, timedelta
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# LLM ë¶„ì„ê¸° ì„í¬íŠ¸
try:
    from llm_analyzer import SpanishLLMAnalyzer
    LLM_AVAILABLE = True
except ImportError:
    print("âš ï¸ LLM ë¶„ì„ê¸°ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. LLMì´ í•„ìˆ˜ì…ë‹ˆë‹¤.")
    LLM_AVAILABLE = False

def is_episode_recent(published_date, max_days_old=30, allow_old=True):
    """
    ì—í”¼ì†Œë“œê°€ ìµœê·¼ ë©°ì¹  ì´ë‚´ì— ë°œí–‰ë˜ì—ˆëŠ”ì§€ í™•ì¸
    ëª¨ë“  ì—í”¼ì†Œë“œ í—ˆìš© (í•™ìŠµ ëª©ì )
    """
    return True  # ëª¨ë“  ì—í”¼ì†Œë“œ í—ˆìš©



def analyze_text_difficulty(content):
    """Analyze text difficulty using LLM"""
    if not content:
        return "B2"  # ê¸°ë³¸ê°’
    
    if not LLM_AVAILABLE or not os.environ.get('OPENAI_API_KEY'):
        print("âš ï¸ LLM ë¶„ì„ê¸°ê°€ í•„ìš”í•©ë‹ˆë‹¤. ê¸°ë³¸ ë‚œì´ë„ B2ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return "B2"
    
    try:
        analyzer = SpanishLLMAnalyzer()
        return analyzer.analyze_text_difficulty(content)
    except Exception as e:
        print(f"LLM ë‚œì´ë„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return "B2"  # ê¸°ë³¸ê°’

def search_apple_podcasts_episode(podcast_name, episode_title, apple_base):
    """Search for exact episode URL using Apple iTunes Search API"""
    try:
        import urllib.parse
        
        print(f"    ğŸ” iTunes Search APIë¡œ {podcast_name} ì—í”¼ì†Œë“œ ê²€ìƒ‰ ì¤‘...")
        
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¡œ ì‹œë„
        search_terms = []
        
        # 1. íŒŸìºìŠ¤íŠ¸ ì´ë¦„ + ì—í”¼ì†Œë“œ ì œëª©
        search_terms.append(f"{podcast_name} {episode_title}")
        
        # 2. ì—í”¼ì†Œë“œ ì œëª©ë§Œìœ¼ë¡œë„ ê²€ìƒ‰
        search_terms.append(episode_title)
        
        # 3. íŒŸìºìŠ¤íŠ¸ ì´ë¦„ë§Œìœ¼ë¡œ ê²€ìƒ‰ (ì—í”¼ì†Œë“œê°€ ë„ˆë¬´ êµ¬ì²´ì ì¼ ë•Œ)
        search_terms.append(podcast_name)
        
        # íŠ¹ë³„í•œ ê²€ìƒ‰ì–´ íŒ¨í„´ ì¶”ê°€ (ëª¨ë“  íŒŸìºìŠ¤íŠ¸ì— ì ìš©)
        if ':' in episode_title:
            # ì½œë¡ ìœ¼ë¡œ êµ¬ë¶„ëœ ì œëª©ì˜ ê²½ìš° (ì˜ˆ: "The Network: Episode Title")
            main_part = episode_title.split(':')[0].strip()
            search_terms.append(main_part)
            subtitle = episode_title.split(':', 1)[1].strip()
            search_terms.append(subtitle)
            search_terms.append(f"{podcast_name} {main_part}")
            search_terms.append(f"{podcast_name} {subtitle}")
        
        # ì¤‘ìš”í•œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œí•˜ì—¬ ê²€ìƒ‰ (ëª¨ë“  íŒŸìºìŠ¤íŠ¸ì— ì ìš©)
        keywords = episode_title.lower().split()
        important_words = [w for w in keywords if len(w) > 3 and w not in ['the', 'and', 'of', 'in', 'to', 'for', 'with', 'episode', 'ep']]
        if important_words and len(important_words) >= 2:
            search_terms.append(f"{podcast_name} {' '.join(important_words[:2])}")
        
        print(f"    ğŸ” ê²€ìƒ‰ì–´ë“¤: {search_terms[:5]}...")  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
        
        for search_term in search_terms:
            encoded_term = urllib.parse.quote(search_term)
            search_url = f"https://itunes.apple.com/search?term={encoded_term}&media=podcast&entity=podcastEpisode&limit=50"
            
            print(f"    ğŸ“¡ iTunes Search API í˜¸ì¶œ: {search_url}")
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(search_url, headers=headers, timeout=10)
            if response.status_code == 200:
                data = response.json()
                results = data.get('results', [])
                
                print(f"    ğŸ“Š iTunes ê²€ìƒ‰ ê²°ê³¼ ({search_term}): {len(results)}ê°œ ì—í”¼ì†Œë“œ ë°œê²¬")
                
                # ê²€ìƒ‰ ê²°ê³¼ì—ì„œ í•´ë‹¹ íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ì°¾ê¸°
                for result in results:
                    collection_name = result.get('collectionName', '').lower()
                    track_name = result.get('trackName', '')
                    track_view_url = result.get('trackViewUrl', '')
                    
                    print(f"    ğŸ“º ê²€í†  ì¤‘: {track_name} (ì»¬ë ‰ì…˜: {collection_name})")
                    
                    # íŒŸìºìŠ¤íŠ¸ ì´ë¦„ ë§¤ì¹­ í™•ì¸ (í†µí•©ëœ ë¡œì§)
                    podcast_match = False
                    
                    # ì •í™•í•œ ì´ë¦„ ë§¤ì¹­ ìš°ì„ 
                    if podcast_name.lower().replace(' ', '') in collection_name.replace(' ', ''):
                        podcast_match = True
                    # í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­
                    elif any(name.lower() in collection_name for name in podcast_name.split() if len(name) > 3):
                        podcast_match = True
                    
                    if podcast_match:
                        # í†µí•©ëœ ì—í”¼ì†Œë“œ ì œëª© ë§¤ì¹­ ë¡œì§
                        title_match = False
                        
                        title_words = episode_title.lower().split()
                        track_words = track_name.lower().split()
                        
                        # 1. ê³µí†µ ë‹¨ì–´ ë§¤ì¹­ (ëª¨ë“  íŒŸìºìŠ¤íŠ¸ì— ì ìš©)
                        common_words = set(title_words) & set(track_words)
                        if len(common_words) >= 2:
                            title_match = True
                        
                        # 2. ì¤‘ìš”í•œ ë‹¨ì–´ ë§¤ì¹­ (ëª¨ë“  íŒŸìºìŠ¤íŠ¸ì— ì ìš©)
                        elif any(word in track_name.lower() for word in title_words if len(word) > 4):
                            title_match = True
                        
                        # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤ì¹­ (ëª¨ë“  íŒŸìºìŠ¤íŠ¸ì— ì ìš©)
                        else:
                            important_words = [word for word in title_words if len(word) > 3 and word not in ['the', 'and', 'of', 'in', 'to', 'for', 'with', 'episode', 'ep']]
                            if important_words:
                                matches = sum(1 for word in important_words if word in track_name.lower())
                                if matches >= min(2, len(important_words)):
                                    title_match = True
                        
                        if title_match and track_view_url:
                            print(f"    âœ… Apple Podcast ì •í™•í•œ ì—í”¼ì†Œë“œ URL ë°œê²¬: {track_view_url}")
                            return track_view_url
                
            else:
                print(f"    âŒ iTunes Search API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
        
        print(f"    âš ï¸ ëª¨ë“  ê²€ìƒ‰ì–´ë¡œ ì‹œë„í–ˆì§€ë§Œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í•¨")
        return apple_base
                
    except Exception as e:
        print(f"    âŒ iTunes Search ì˜¤ë¥˜: {e}")
        return apple_base

def generate_apple_podcast_link(podcast_name, apple_base, episode_link, episode_number, episode_title=""):
    """Generate optimized Apple Podcasts link by podcast type"""
    
    # ëª¨ë“  íŒŸìºìŠ¤íŠ¸ì— ëŒ€í•œ í†µí•©ëœ ë§í¬ ìƒì„± ì „ëµ
    if 'Radio Ambulante' in podcast_name or 'npr.org' in episode_link:
        # Radio AmbulanteëŠ” ì›ë³¸ ì›¹ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ìš°ì„  ì‚¬ìš©
        if episode_link and 'radioambulante.org' in episode_link and validate_url(episode_link):
            print(f"    âœ… Radio Ambulante ì›ë³¸ ì›¹ì‚¬ì´íŠ¸ ë§í¬ ì‚¬ìš©: {episode_link}")
            return episode_link
        else:
            # iTunes Search API ì‹œë„
            if episode_title:
                apple_url = search_apple_podcasts_episode(podcast_name, episode_title, apple_base)
                if apple_url != apple_base and validate_url(apple_url):
                    print(f"    âœ… iTunes Search APIì—ì„œ Radio Ambulante ì—í”¼ì†Œë“œ ë°œê²¬: {apple_url}")
                    return apple_url
            
            # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´ ì›ë³¸ ë§í¬ ë˜ëŠ” ê¸°ë³¸ Apple ë§í¬ ë°˜í™˜
            if episode_link and validate_url(episode_link):
                return episode_link
            else:
                return apple_base
    
    elif 'SpanishPodcast' in podcast_name:
        # SpanishPodcastëŠ” ì›ë³¸ ì›¹ì‚¬ì´íŠ¸ ë§í¬ë¥¼ ìš°ì„  ì‚¬ìš©
        if episode_link and validate_url(episode_link):
            print(f"    âœ… SpanishPodcast ì›ë³¸ ì›¹ì‚¬ì´íŠ¸ ë§í¬ ì‚¬ìš©: {episode_link}")
            return episode_link
        else:
            print(f"    âš ï¸ SpanishPodcast ì›ë³¸ ë§í¬ ìœ íš¨í•˜ì§€ ì•ŠìŒ, iTunes Search API ì‹œë„")
            # ì›ë³¸ ë§í¬ê°€ ìœ íš¨í•˜ì§€ ì•Šìœ¼ë©´ iTunes Search API ì‹œë„
            if episode_title:
                apple_url = search_apple_podcasts_episode(podcast_name, episode_title, apple_base)
                if apple_url != apple_base and validate_url(apple_url):
                    print(f"    âœ… iTunes Search APIì—ì„œ SpanishPodcast ì—í”¼ì†Œë“œ ë°œê²¬: {apple_url}")
                    return apple_url
            
            # iTunes Searchë„ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ Apple Podcasts ë§í¬ ë°˜í™˜
            print(f"    ğŸ”„ iTunes Search APIë„ ì‹¤íŒ¨, ê¸°ë³¸ Apple Podcasts ë§í¬ ì‚¬ìš©: {apple_base}")
            return apple_base

    elif 'Hoy Hablamos' in podcast_name:
        # iTunes Search API ìš°ì„  ì‹œë„
        if episode_title:
            apple_url = search_apple_podcasts_episode(podcast_name, episode_title, apple_base)
            if apple_url != apple_base and validate_url(apple_url):
                print(f"    âœ… iTunes Search APIì—ì„œ Hoy Hablamos ì—í”¼ì†Œë“œ ë°œê²¬: {apple_url}")
                return apple_url
        
        # iTunes Searchê°€ ì‹¤íŒ¨í•˜ë©´ ì—í”¼ì†Œë“œ ë²ˆí˜¸ ê¸°ë°˜ìœ¼ë¡œ ë§í¬ ìƒì„± ì‹œë„
        if episode_number and episode_number != 'N/A':
            try:
                ep_num = int(episode_number)
                generated_url = f"{apple_base}?i=1000{ep_num:06d}"
                print(f"    ğŸ”„ ì—í”¼ì†Œë“œ ë²ˆí˜¸ ê¸°ë°˜ URL ìƒì„±: {generated_url}")
                if validate_url(generated_url):
                    return generated_url
            except:
                pass
        
        print(f"    ğŸ”„ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ Apple Podcasts ë§í¬ ì‚¬ìš©: {apple_base}")
        return apple_base
        
    elif 'SpanishWithVicente' in podcast_name:
        # iTunes Search API ìš°ì„  ì‹œë„
        if episode_title:
            apple_url = search_apple_podcasts_episode(podcast_name, episode_title, apple_base)
            if apple_url != apple_base and validate_url(apple_url):
                print(f"    âœ… iTunes Search APIì—ì„œ SpanishWithVicente ì—í”¼ì†Œë“œ ë°œê²¬: {apple_url}")
                return apple_url
        
        # iTunes Searchê°€ ì‹¤íŒ¨í•˜ë©´ ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ê°€ ì‹œë„
        if episode_number and episode_number != 'N/A':
            generated_url = f"{apple_base}?i={episode_number}"
            print(f"    ğŸ”„ ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ê°€ URL: {generated_url}")
            if validate_url(generated_url):
                return generated_url
        
        print(f"    ğŸ”„ ëª¨ë“  ì‹œë„ ì‹¤íŒ¨, ê¸°ë³¸ Apple Podcasts ë§í¬ ì‚¬ìš©: {apple_base}")
        return apple_base
        
    elif 'DELE' in podcast_name:
        # iTunes Search API ìš°ì„  ì‹œë„
        if episode_title:
            apple_url = search_apple_podcasts_episode(podcast_name, episode_title, apple_base)
            if apple_url != apple_base and validate_url(apple_url):
                print(f"    âœ… iTunes Search APIì—ì„œ DELE ì—í”¼ì†Œë“œ ë°œê²¬: {apple_url}")
                return apple_url
        
        # iTunes Searchê°€ ì‹¤íŒ¨í•˜ë©´ ë©”ì¸ ë§í¬ ì‚¬ìš©
        print(f"    ğŸ”„ iTunes Search ì‹¤íŒ¨, ê¸°ë³¸ Apple Podcasts ë§í¬ ì‚¬ìš©: {apple_base}")
        return apple_base
    else:
        # ê¸°ë³¸ ì „ëµ: iTunes Search APIë¡œ ì •í™•í•œ ì—í”¼ì†Œë“œ ì°¾ê¸°
        if episode_title:
            print(f"    ğŸ” iTunes Search APIë¡œ {podcast_name} ì—í”¼ì†Œë“œ ê²€ìƒ‰ ì¤‘...")
            
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ì–´ë¡œ ì‹œë„
            search_terms = []
            
            # 1. íŒŸìºìŠ¤íŠ¸ ì´ë¦„ + ì—í”¼ì†Œë“œ ì œëª©
            search_terms.append(f"{podcast_name} {episode_title}")
            
            # 2. ì—í”¼ì†Œë“œ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ ë²ˆí˜¸ë¡œë„ ê²€ìƒ‰
            if episode_number and episode_number != 'N/A':
                search_terms.append(f"{podcast_name} {episode_number}")
                search_terms.append(f"{podcast_name} Episode {episode_number}")
                search_terms.append(f"{podcast_name} Ep {episode_number}")
            
            # 3. ì—í”¼ì†Œë“œ ì œëª©ë§Œìœ¼ë¡œë„ ê²€ìƒ‰
            search_terms.append(episode_title)
            
            for search_term in search_terms:
                try:
                    encoded_term = urllib.parse.quote(search_term)
                    search_url = f"https://itunes.apple.com/search?term={encoded_term}&media=podcast&entity=podcastEpisode&limit=20"
                    
                    print(f"    ğŸ” ê²€ìƒ‰ì–´: {search_term}")
                    
                    headers = {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                    }
                    
                    response = requests.get(search_url, headers=headers, timeout=10)
                    if response.status_code == 200:
                        data = response.json()
                        results = data.get('results', [])
                        
                        print(f"    ğŸ“Š iTunes ê²€ìƒ‰ ê²°ê³¼: {len(results)}ê°œ ì—í”¼ì†Œë“œ ë°œê²¬")
                        
                        for result in results:
                            result_title = result.get('trackName', '').lower()
                            collection_name = result.get('collectionName', '').lower()
                            track_view_url = result.get('trackViewUrl', '')
                            
                            print(f"    ğŸ“º ê²€í†  ì¤‘: {result.get('trackName', '')} (ì»¬ë ‰ì…˜: {collection_name})")
                            
                            # íŒŸìºìŠ¤íŠ¸ ì´ë¦„ ë§¤ì¹­ í™•ì¸
                            podcast_match = False
                            if 'spanishpodcast' in podcast_name.lower() and 'spanishpodcast' in collection_name:
                                podcast_match = True
                            elif any(name.lower() in collection_name for name in podcast_name.split() if len(name) > 3):
                                podcast_match = True
                            
                            if podcast_match:
                                # ì—í”¼ì†Œë“œ ì œëª© ë§¤ì¹­ í™•ì¸
                                title_match = False
                                
                                # ì—í”¼ì†Œë“œ ë²ˆí˜¸ ë§¤ì¹­
                                if episode_number and episode_number != 'N/A':
                                    if episode_number in result_title or f"episode {episode_number}" in result_title or f"ep {episode_number}" in result_title:
                                        title_match = True
                                
                                # ì œëª© í‚¤ì›Œë“œ ë§¤ì¹­
                                if not title_match:
                                    title_words = episode_title.lower().split()
                                    important_words = [word for word in title_words if len(word) > 3 and word not in ['the', 'and', 'of', 'in', 'to', 'for', 'with', 'episode', 'ep']]
                                    if important_words:
                                        matches = sum(1 for word in important_words if word in result_title)
                                        if matches >= min(2, len(important_words)):
                                            title_match = True
                                
                                if title_match and track_view_url:
                                    print(f"    âœ… Apple Podcast ì •í™•í•œ ì—í”¼ì†Œë“œ URL ë°œê²¬: {track_view_url}")
                                    return track_view_url
                        
                        # ì´ ê²€ìƒ‰ì–´ë¡œ ì°¾ì•˜ìœ¼ë©´ ë” ì´ìƒ ì‹œë„í•˜ì§€ ì•ŠìŒ
                        if results:
                            print(f"    âš ï¸ iTunesì—ì„œ ì •í™•í•œ ë§¤ì¹­ì„ ì°¾ì§€ ëª»í•¨ (ê²€ìƒ‰ì–´: {search_term})")
                            break
                            
                    else:
                        print(f"    âŒ iTunes Search API ì˜¤ë¥˜: {response.status_code}")
                        
                except Exception as e:
                    print(f"    âŒ iTunes Search ì˜¤ë¥˜ (ê²€ìƒ‰ì–´: {search_term}): {e}")
                    continue
            
            print(f"    âš ï¸ ëª¨ë“  ê²€ìƒ‰ì–´ë¡œ ì‹œë„í–ˆì§€ë§Œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í•¨")
        
        # ëª¨ë“  ì‹œë„ê°€ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ Apple Podcasts ë§í¬ ë°˜í™˜
        print(f"    ğŸ”„ ê¸°ë³¸ Apple Podcasts ë§í¬ ì‚¬ìš©: {apple_base}")
        
        # Apple Podcasts ë§í¬ ìœ íš¨ì„± ê²€ì¦
        if validate_url(apple_base):
            return apple_base
        else:
            print(f"    âŒ ê¸°ë³¸ Apple Podcasts ë§í¬ë„ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {apple_base}")
            
            # ì§€ì—­ ì½”ë“œ ë³€ê²½ ì‹œë„ (us -> kr, kr -> us)
            if '/us/' in apple_base:
                alternative_url = apple_base.replace('/us/', '/kr/')
                print(f"    ğŸ”„ ì§€ì—­ ì½”ë“œ ë³€ê²½ ì‹œë„ (us -> kr): {alternative_url}")
                if validate_url(alternative_url):
                    return alternative_url
            elif '/kr/' in apple_base:
                alternative_url = apple_base.replace('/kr/', '/us/')
                print(f"    ğŸ”„ ì§€ì—­ ì½”ë“œ ë³€ê²½ ì‹œë„ (kr -> us): {alternative_url}")
                if validate_url(alternative_url):
                    return alternative_url
            
            # ìµœì¢…ì ìœ¼ë¡œ ì›ë³¸ ë§í¬ ë°˜í™˜
            print(f"    âš ï¸ ëª¨ë“  Apple Podcasts ë§í¬ ì‹œë„ ì‹¤íŒ¨, ì›ë³¸ ë§í¬ ë°˜í™˜")
            return apple_base

def get_article_content(url):
    """Get actual article content from URL"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = 'utf-8'
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ì‚¬ì´íŠ¸ë³„ ë³¸ë¬¸ ì¶”ì¶œ ë¡œì§
        content = ""
        
        if '20minutos.es' in url:
            # 20minutos ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('div', class_='article-text') or soup.find('div', class_='content')
            if article_body:
                paragraphs = article_body.find_all(['p', 'div'])
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
        
        elif 'elpais.com' in url:
            # El PaÃ­s ë³¸ë¬¸ ì¶”ì¶œ
            article_body = soup.find('div', {'data-dtm-region': 'articulo_cuerpo'}) or \
                         soup.find('div', class_='a_c clearfix') or \
                         soup.find('div', class_='articulo-cuerpo')
            if article_body:
                paragraphs = article_body.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])
        
        # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ (fallback)
        if not content:
            # ì¼ë°˜ì ì¸ article íƒœê·¸ë‚˜ main íƒœê·¸ì—ì„œ ì¶”ì¶œ
            article = soup.find('article') or soup.find('main')
            if article:
                paragraphs = article.find_all('p')
                content = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()][:10])  # ì²˜ìŒ 10ê°œ ë¬¸ë‹¨ë§Œ
        
        # ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ë‹¤ë¥¸ ë°©ë²• ì‹œë„
        if len(content) < 200:
            all_paragraphs = soup.find_all('p')
            content = ' '.join([p.get_text().strip() for p in all_paragraphs if len(p.get_text().strip()) > 50][:8])
        
        return content[:2000]  # ì²˜ìŒ 2000ìë§Œ ë°˜í™˜
        
    except Exception as e:
        print(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def extract_category_from_content(title, content):
    """Extract category from title and content"""
    full_text = (title + " " + content).lower()
    
    keywords = {
        'ì •ì¹˜': ['gobierno', 'polÃ­tica', 'elecciones', 'parlamento', 'ministro', 'rey', 'presidente', 'votaciÃ³n', 'congreso'],
        'ê²½ì œ': ['economÃ­a', 'banco', 'euro', 'empleo', 'crisis', 'mercado', 'dinero', 'trabajo', 'empresa', 'inversiÃ³n'],
        'ì‚¬íšŒ': ['sociedad', 'educaciÃ³n', 'sanidad', 'vivienda', 'familia', 'salud', 'poblaciÃ³n', 'ciudadanos'],
        'ìŠ¤í¬ì¸ ': ['fÃºtbol', 'real madrid', 'barcelona', 'liga', 'deporte', 'partido', 'atletico', 'champions'],
        'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³vil', 'digital', 'app', 'inteligencia', 'innovaciÃ³n'],
        'ë¬¸í™”': ['cultura', 'arte', 'mÃºsica', 'teatro', 'festival', 'libro', 'cine', 'exposiciÃ³n'],
        'êµ­ì œ': ['internacional', 'mundial', 'europa', 'amÃ©rica', 'china', 'estados unidos', 'uniÃ³n europea']
    }
    
    category_scores = {}
    for category, words in keywords.items():
        score = sum(1 for word in words if word in full_text)
        if score > 0:
            category_scores[category] = score
    
    if category_scores:
        return max(category_scores, key=category_scores.get)
    return 'ì¼ë°˜'

def extract_episode_number(title):
    patterns = [
        r'Ep\.?\s*(\d+)',
        r'Episode\s*(\d+)',
        r'#(\d+)',
        r'(\d{3,4})'
    ]
    
    for pattern in patterns:
        match = re.search(pattern, title, re.IGNORECASE)
        if match:
            return match.group(1)
    return None

def extract_duration_from_feed(entry):
    # iTunes ë“€ë ˆì´ì…˜ ë¨¼ì € í™•ì¸
    if hasattr(entry, 'itunes_duration'):
        duration = entry.itunes_duration
        # ì´ˆ ë‹¨ìœ„ì¸ ê²½ìš° ë¶„:ì´ˆë¡œ ë³€í™˜
        if duration.isdigit():
            total_seconds = int(duration)
            minutes = total_seconds // 60
            seconds = total_seconds % 60
            return f"{minutes}:{seconds:02d}"
        return duration
    
    # ìš”ì•½ì—ì„œ ì¬ìƒì‹œê°„ ì¶”ì¶œ ì‹œë„
    summary = entry.get('summary', '') + entry.get('description', '')
    duration_patterns = [
        r'(\d+)\s*min',
        r'(\d+)\s*ë¶„',
        r'(\d+):(\d+)',
        r'Duration:\s*(\d+)'
    ]
    
    for pattern in duration_patterns:
        match = re.search(pattern, summary)
        if match:
            if ':' in pattern:
                return f"{match.group(1)}:{match.group(2)}"
            else:
                return f"{match.group(1)}ë¶„"
    
    return "15-25ë¶„"

def extract_topic_keywords(title, summary=""):
    content = (title + " " + summary).lower()
    
    topic_keywords = {
        'ë¬¸ë²•': ['gramÃ¡tica', 'verbos', 'subjuntivo', 'pretÃ©rito', 'sintaxis'],
        'ë¬¸í™”': ['cultura', 'tradiciÃ³n', 'costumbres', 'historia', 'arte'],
        'ìš”ë¦¬': ['cocina', 'comida', 'receta', 'gastronomÃ­a', 'plato'],
        'ì—¬í–‰': ['viajes', 'turismo', 'ciudades', 'lugares', 'destinos'],
        'ì§ì—…': ['trabajo', 'empleo', 'profesiÃ³n', 'carrera', 'oficina'],
        'ê°€ì¡±': ['familia', 'padres', 'hijos', 'matrimonio', 'casa'],
        'ê¸°ìˆ ': ['tecnologÃ­a', 'internet', 'mÃ³viles', 'digital', 'aplicaciones'],
        'ì •ì¹˜': ['polÃ­tica', 'gobierno', 'elecciones', 'democracia'],
        'ê²½ì œ': ['economÃ­a', 'dinero', 'banco', 'trabajo', 'crisis', 'preferentes', 'ahorros'],
        'ì‚¬íšŒ': ['sociedad', 'gente', 'problemas', 'cambios', 'vida'],
        'ê±´ê°•': ['salud', 'medicina', 'hospital', 'enfermedad', 'mÃ©dico'],
        'êµìœ¡': ['educaciÃ³n', 'estudiantes', 'universidad', 'aprender']
    }
    
    for topic, keywords in topic_keywords.items():
        if any(keyword in content for keyword in keywords):
            return topic
    return 'ì¼ë°˜ ì£¼ì œ'

def create_detailed_memo(content_type, data, weekday_name):
    if content_type == "article":
        category = data.get('category', 'ì¼ë°˜')
        difficulty = data.get('difficulty', 'B2')
        content = data.get('content_preview', '')
        
        # ë ˆë²¨ë³„ ë¬¸ë²• í¬ì¸íŠ¸ ì¶”ì¶œ
        grammar_points = extract_grammar_points_from_content(content, difficulty)
        
        # ë¬¸ë²• í¬ì¸íŠ¸ í…ìŠ¤íŠ¸ ìƒì„±
        grammar_text = ""
        if grammar_points:
            grammar_text = f"ğŸ“ {difficulty} ë¬¸ë²•: {' | '.join(grammar_points)} "
        
        return (f"ğŸ“° {category} ë¶„ì•¼ ê¸°ì‚¬ ({difficulty} ìˆ˜ì¤€) "
               f"ğŸ“… ë°œí–‰: {data.get('published', 'ì˜¤ëŠ˜')} "
               f"ğŸ¯ í•™ìŠµëª©í‘œ: 15ë¶„ ë…í•´, {difficulty} ìˆ˜ì¤€ ë¬¸ë²• ë¶„ì„ "
               f"{grammar_text}"
               f"ğŸ¤– AI ë¶„ì„ "
               f"ğŸ“– ê¶Œì¥: ë¬¸ë²• êµ¬ì¡° ë¶„ì„ì„ í†µí•œ ë…í•´ ì‹¤ë ¥ í–¥ìƒ")

    elif content_type == "podcast":
        podcast_name = data.get('podcast_name', '')
        duration = data.get('duration', '15-25ë¶„')
        topic = data.get('topic', 'ì¼ë°˜ ì£¼ì œ')
        episode_num = data.get('episode_number', '')
        episode_title = data.get('title', '')
        summary = data.get('summary', '')
        difficulty = data.get('difficulty', 'B2')
        
        # íŒŸìºìŠ¤íŠ¸ ì´ë¦„ì—ì„œ íŠ¹ìˆ˜ í‘œì‹œ ì œê±°í•˜ì—¬ ì •í™•í•œ ì´ë¦„ ì–»ê¸°
        clean_podcast_name = podcast_name.replace(" (ë°±ì—…)", "").replace(" (ëŒ€ì•ˆ)", "").replace(" (ì¤‘ë³µ ê°€ëŠ¥)", "").strip()
        
        # 'N/A'ë‚˜ ë¹ˆ ê°’ ì²˜ë¦¬
        if episode_num == 'N/A' or not episode_num:
            episode_num = ''
        
        # ì£¼ì œ ì •ë¦¬ (ì›í•˜ì§€ ì•ŠëŠ” ê°’ë“¤ ì œê±°)
        if topic in ['ì¼ë°˜ ì£¼ì œ', 'N/A', '']:
            topic = 'ìŠ¤í˜ì¸ì–´ í•™ìŠµ'
        
        # íŠ¹ë³„ ìƒíƒœ í‘œì‹œ
        status_info = ""
        if "(ëŒ€ì•ˆ)" in podcast_name:
            status_info = "ğŸ”„ ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ë¡œ ì„ íƒë¨ "
        elif "(ì¤‘ë³µ ê°€ëŠ¥)" in podcast_name:
            status_info = "âš ï¸ ì¤‘ë³µ ê°€ëŠ¥ì„± ìˆìŒ "
        
        # íŒŸìºìŠ¤íŠ¸ transcriptì—ì„œ êµ¬ì–´ì²´ í‘œí˜„ ë¶„ì„
        expressions = []
        episode_url = data.get('url', '')
        
        # ì‹¤ì œ transcriptë‚˜ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
        transcript_content = get_podcast_transcript_or_content(episode_url, episode_title)
        
        if transcript_content:
            print(f"    ğŸ“ ì‹¤ì œ ì½˜í…ì¸ ì—ì„œ êµ¬ì–´ì²´ í‘œí˜„ ë¶„ì„ ì¤‘...")
            expressions = extract_vocabulary_expressions_from_transcript(transcript_content, difficulty)
        else:
            print(f"    âŒ ì–´ë–¤ ì†ŒìŠ¤ì—ì„œë„ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            expressions = []
        
        # Apple Podcastì—ì„œ ì •í™•í•œ URLì„ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
        found_apple_url = get_found_apple_url()
        if found_apple_url:
            print(f"    ğŸ Apple Podcast ì •í™•í•œ URL ë°œê²¬: {found_apple_url}")
            # ë°ì´í„°ì— ì •í™•í•œ Apple URL ì—…ë°ì´íŠ¸
            data['apple_link'] = found_apple_url
        
        # ì£¼ì œì— ë”°ë¥¸ í•™ìŠµëª©í‘œ ì„¤ì •
        learning_goals = {
            'ê²½ì œ': 'ê¸ˆìœµ í‘œí˜„',
            'ì •ì¹˜': 'ì •ì¹˜ í‘œí˜„',
            'ë¬¸í™”': 'ë¬¸í™” í‘œí˜„',
            'ì‚¬íšŒ': 'ì‚¬íšŒ ì´ìŠˆ í‘œí˜„',
            'êµìœ¡': 'êµìœ¡ ê´€ë ¨ í‘œí˜„',
            'ê±´ê°•': 'ì˜ë£Œ í‘œí˜„',
            'ê¸°ìˆ ': 'ê¸°ìˆ  í‘œí˜„',
            'ë¬¸ë²•': 'ë¬¸ë²• êµ¬ì¡°',
            'ìŠ¤í˜ì¸ì–´ í•™ìŠµ': 'ì¼ìƒ í‘œí˜„'
        }
        goal = learning_goals.get(topic, 'í•µì‹¬ í‘œí˜„')
        
        # ì¬ìƒì‹œê°„ì— ë”°ë¥¸ ì²­ì·¨ ê³„íš ì„¤ì •
        if ':' in duration:
            try:
                minutes, seconds = duration.split(':')
                total_minutes = int(minutes)
                if total_minutes > 30:
                    listen_plan = f"(30ë¶„ ì²­ì·¨ ëª©í‘œ)"
                elif total_minutes > 20:
                    listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
                else:
                    listen_plan = f"(ì „ì²´ {duration} ì²­ì·¨)"
            except:
                listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
        else:
            listen_plan = "(25ë¶„ ì²­ì·¨ ëª©í‘œ)"
        
        # ì—í”¼ì†Œë“œ ë²ˆí˜¸ê°€ ìˆìœ¼ë©´ í‘œì‹œ, ì—†ìœ¼ë©´ ìƒëµ
        episode_text = f"Ep.{episode_num} - " if episode_num else ""
        
        # êµ¬ì–´ì²´ í‘œí˜„ í…ìŠ¤íŠ¸ ìƒì„±
        expression_text = ""
        if expressions:
            expression_text = f"ğŸ¯ {difficulty} êµ¬ì–´ì²´: {' | '.join(expressions)} "
        else:
            expression_text = f"ğŸ¯ {difficulty} êµ¬ì–´ì²´: ì‹¤ì œ ì½˜í…ì¸  ë¶„ì„ í•„ìš”"
        
        # ì •í™•í•œ ì—í”¼ì†Œë“œ ì œëª© ì¶”ê°€ (Apple Podcastsì—ì„œ ê²€ìƒ‰í•  ìˆ˜ ìˆë„ë¡)
        search_info = ""
        if episode_title:
            # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ì¶•ì•½
            short_title = episode_title[:50] + "..." if len(episode_title) > 50 else episode_title
            search_info = f"ğŸ” ê²€ìƒ‰ì–´: \"{short_title}\" "
        
        # Radio Ambulanteì¸ ê²½ìš°ì—ë§Œ ì›¹ì‚¬ì´íŠ¸ URL ì •ë³´ ì¶”ê°€
        url_info = ""
        if 'Radio Ambulante' in clean_podcast_name:
            episode_url = data.get('url', '')
            if 'radioambulante.org' in episode_url:
                url_info = f"ğŸŒ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì§ì ‘ ì²­ì·¨ ê°€ëŠ¥ "
            elif 'npr.org' in episode_url:
                url_info = f"ğŸ“» NPRì—ì„œ ì²­ì·¨ ê°€ëŠ¥ "
            
            # Apple Podcastsì—ì„œ ìˆ˜ë™ ê²€ìƒ‰ì„ ìœ„í•œ ì •ë³´ ì¶”ê°€
            if episode_title:
                # ì œëª©ì—ì„œ ë¶€ì œëª© ì¶”ì¶œ (ì½œë¡  ì´í›„ ë¶€ë¶„)
                if ':' in episode_title:
                    main_title = episode_title.split(':')[0].strip()
                    subtitle = episode_title.split(':', 1)[1].strip()
                    url_info += f"ğŸ Apple Podcasts ê²€ìƒ‰: \"{main_title}\" ë˜ëŠ” \"{subtitle}\" "
                else:
                    url_info += f"ğŸ Apple Podcasts ê²€ìƒ‰: \"{episode_title}\" "
        
        return (f"ğŸ§ {clean_podcast_name} {episode_text}{weekday_name} ìŠ¤í˜ì¸ì–´ íŒŸìºìŠ¤íŠ¸ "
               f"{status_info}"
               f"ğŸ“º ì—í”¼ì†Œë“œ: \"{episode_title}\" "
               f"â±ï¸ ì¬ìƒì‹œê°„: {duration} {listen_plan} "
               f"ğŸ¯ í•™ìŠµëª©í‘œ: {goal} 5ê°œ ì •ë¦¬ "
               f"ğŸŒ ì£¼ì œ: {topic} "
               f"{expression_text}"
               f"ğŸ¤– AI ë¶„ì„ "
               f"{search_info}"
               f"{url_info}"
               f"ğŸ“» ê¶Œì¥: êµ¬ì–´ì²´ í‘œí˜„ì— ì§‘ì¤‘í•˜ì—¬ ì²­ì·¨")

def extract_radio_ambulante_url(entry):
    """Extract actual Radio Ambulante website URL"""
    try:
        # ì—í”¼ì†Œë“œ ì œëª©ì—ì„œ ìŠ¬ëŸ¬ê·¸ ìƒì„± ì‹œë„
        title = entry.title.lower()
        # íŠ¹ìˆ˜ ë¬¸ì ì œê±° ë° ê³µë°±ì„ í•˜ì´í”ˆìœ¼ë¡œ ë³€í™˜
        import re
        slug = re.sub(r'[^\w\s-]', '', title)
        slug = re.sub(r'[-\s]+', '-', slug).strip('-')
        
        # Radio Ambulante ì›¹ì‚¬ì´íŠ¸ URL ìƒì„±
        radio_ambulante_url = f"https://radioambulante.org/audio/{slug}"
        
        # URL ìœ íš¨ì„± í™•ì¸
        if validate_url(radio_ambulante_url):
            return radio_ambulante_url
        
        # ìŠ¬ëŸ¬ê·¸ ìƒì„± ì‹¤íŒ¨ ì‹œ ìš”ì•½ì—ì„œ ë§í¬ ì°¾ê¸°
        summary = entry.get('summary', '') + entry.get('description', '')
        url_match = re.search(r'https://radioambulante\.org/audio/[^\s<>"]+', summary)
        if url_match:
            found_url = url_match.group(0)
            if validate_url(found_url):
                return found_url
                
        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ì‹œ None ë°˜í™˜
        return None
        
    except Exception as e:
        print(f"Radio Ambulante URL ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return None

def validate_url(url, timeout=5):
    """Validate URL quickly"""
    try:
        if not url or not (url.startswith('http://') or url.startswith('https://')):
            return False
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        response = requests.head(url, headers=headers, timeout=timeout, allow_redirects=True)
        return response.status_code < 400
    except:
        return False

def get_alternative_podcasts(current_weekday, current_podcast_name):
    """í˜„ì¬ ìš”ì¼ê³¼ íŒŸìºìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ ëª©ë¡ ë°˜í™˜"""
    all_podcasts = {
        "SpanishPodcast": {
            "name": "SpanishPodcast",
            "rss": "https://feeds.feedburner.com/SpanishPodcast",
            "apple_base": "https://podcasts.apple.com/us/podcast/spanishpodcast/id70077665",
            "region": "ìŠ¤í˜ì¸"
        },
        "Radio Ambulante": {
            "name": "Radio Ambulante", 
            "rss": "https://feeds.simplecast.com/54nAGcIl",  # í™•ì¸ëœ Radio Ambulante í”¼ë“œ
            "apple_base": "https://podcasts.apple.com/kr/podcast/radio-ambulante/id527614348",
            "region": "ì¤‘ë‚¨ë¯¸"
        },
        "EspaÃ±olistos": {
            "name": "EspaÃ±olistos",
            "rss": "https://creators.spotify.com/pod/show/espanolistos/rss",
            "apple_base": "https://podcasts.apple.com/us/podcast/espa%C3%B1olistos/id1508733186",
            "region": "ë‚¨ë¯¸"
        },
        "SpanishPodcast (ê¸ˆìš”ì¼)": {
            "name": "SpanishPodcast (ê¸ˆìš”ì¼)",
            "rss": "https://feeds.feedburner.com/SpanishPodcast",
            "apple_base": "https://podcasts.apple.com/us/podcast/spanishpodcast/id70077665",
            "region": "ìŠ¤í˜ì¸"
        }
    }
    
    # í˜„ì¬ íŒŸìºìŠ¤íŠ¸ë¥¼ ì œì™¸í•œ ëŒ€ì•ˆë“¤ ë°˜í™˜
    alternatives = []
    for name, info in all_podcasts.items():
        if name != current_podcast_name:
            alternatives.append((name, info))
    
    return alternatives



def try_alternative_podcast(alternatives, weekday_name):
    """ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ë“¤ì„ ì‹œë„í•´ì„œ ì¤‘ë³µë˜ì§€ ì•Šì€ ì—í”¼ì†Œë“œ ì°¾ê¸°"""
    for alt_name, alt_info in alternatives:
        try:
            print(f"\nğŸ”„ ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ ì‹œë„: {alt_name}")
            print(f"   RSS: {alt_info['rss']}")
            
            feed = feedparser.parse(alt_info['rss'])
            
            if not feed.entries:
                print(f"   âŒ {alt_name}: ì—í”¼ì†Œë“œê°€ ì—†ìŒ")
                continue
                
            # ìµœê·¼ ì—í”¼ì†Œë“œë“¤ í™•ì¸
            for entry in feed.entries[:3]:  # ìµœê·¼ 3ê°œë§Œ í™•ì¸
                episode_title = entry.title
                print(f"   ğŸ“ ì—í”¼ì†Œë“œ í™•ì¸: {episode_title}")
                
                # ë‚ ì§œ ì²´í¬
                if not is_episode_recent(entry.get('published_parsed')):
                    print(f"      âŒ ì˜¤ë˜ëœ ì—í”¼ì†Œë“œ")
                    continue
                
                print(f"   âœ… {alt_name}ì—ì„œ ìƒˆë¡œìš´ ì—í”¼ì†Œë“œ ë°œê²¬!")
                
                # ì—í”¼ì†Œë“œ ë°ì´í„° ìƒì„±
                episode_number = extract_episode_number(episode_title)
                duration = extract_duration_from_feed(entry)
                topic = extract_topic_keywords(episode_title, entry.get('summary', ''))
                
                episode_link = entry.link
                
                # Radio Ambulanteì¸ ê²½ìš° ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ URL ì‹œë„
                if 'Radio Ambulante' in alt_name:
                    radio_ambulante_url = extract_radio_ambulante_url(entry)
                    if radio_ambulante_url:
                        episode_link = radio_ambulante_url
                
                # Apple Podcasts ë§í¬ ìƒì„±
                apple_link = generate_apple_podcast_link(alt_name, alt_info['apple_base'], episode_link, episode_number, episode_title)
                
                # Radio Ambulanteì˜ ê²½ìš° Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URLì„ ë©”ì¸ URLë¡œ ì‚¬ìš©
                final_episode_url = episode_link
                if 'Radio Ambulante' in alt_name:
                    if apple_link != alt_info['apple_base'] and validate_url(apple_link):
                        final_episode_url = apple_link
                    else:
                        final_episode_url = episode_link
                        apple_link = alt_info['apple_base']
                else:
                    if not validate_url(episode_link):
                        final_episode_url = apple_link if validate_url(apple_link) else alt_info['apple_base']
                    if not validate_url(apple_link):
                        apple_link = alt_info['apple_base']
                
                # ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„
                alt_summary = entry.get('summary', '')
                alt_difficulty = analyze_text_difficulty(alt_summary) if alt_summary else "B2"
                
                podcast_data = {
                    'title': episode_title,
                    'url': final_episode_url,
                    'apple_link': apple_link,
                    'published': entry.get('published', ''),
                    'duration': duration,
                    'episode_number': episode_number or 'N/A',
                    'topic': topic,
                    'podcast_name': f"{alt_name} (ëŒ€ì•ˆ)",  # ëŒ€ì•ˆì„ì„ í‘œì‹œ
                    'summary': entry.get('summary', '')[:200],
                    'difficulty': alt_difficulty  # ë‚œì´ë„ ì •ë³´ ì¶”ê°€
                }
                
                print(f"   ğŸ“Š ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ ë°ì´í„°:")
                print(f"      ì—í”¼ì†Œë“œ: {episode_title}")
                print(f"      URL: {final_episode_url}")
                print(f"      Apple: {apple_link}")
                
                return podcast_data
                
        except Exception as e:
            print(f"   âŒ {alt_name} ì‹œë„ ì¤‘ ì˜¤ë¥˜: {e}")
            continue
    
    print("\nâŒ ëª¨ë“  ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ì—ì„œë„ ìƒˆë¡œìš´ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    return None

def is_spanish_content_by_transcript(episode_url, title, summary=""):
    """
    ì‹¤ì œ transcriptë¥¼ ê°€ì ¸ì™€ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ì¸ì§€ ì •í™•í•˜ê²Œ íŒë‹¨
    """
    print(f"    ğŸ” transcript ë¶„ì„ì„ í†µí•œ ì–¸ì–´ ê°ì§€ ì‹œì‘...")
    
    # ì‹¤ì œ transcriptë‚˜ ìƒì„¸ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
    transcript_content = get_podcast_transcript_or_content(episode_url, title)
    
    if transcript_content:
        print(f"    ğŸ“ transcript ë‚´ìš© ë¶„ì„ ì¤‘ (ê¸¸ì´: {len(transcript_content)}ì)")
        
        # ìŠ¤í˜ì¸ì–´ íŠ¹ì§•ì ì¸ íŒ¨í„´ë“¤
        spanish_patterns = [
            r'\b(el|la|los|las)\s+\w+',  # ê´€ì‚¬
            r'\b(que|con|por|para|de|en|sin)\b',  # ì „ì¹˜ì‚¬
            r'\b(es|son|estÃ¡|estÃ¡n|fue|fueron)\b',  # ë™ì‚¬ ser/estar
            r'\b(muy|mÃ¡s|menos|tambiÃ©n|ademÃ¡s)\b',  # ë¶€ì‚¬
            r'\b(pero|aunque|porque|cuando|mientras)\b',  # ì ‘ì†ì‚¬
            r'\b(aÃ±o|aÃ±os|dÃ­a|dÃ­as|tiempo|vida|gente)\b',  # ì¼ë°˜ì ì¸ ëª…ì‚¬
            r'\b(espaÃ±ol|espaÃ±ola|latinos|latina)\b',  # ì–¸ì–´/ì§€ì—­ ê´€ë ¨
            r'Ã±',  # ìŠ¤í˜ì¸ì–´ íŠ¹ìˆ˜ë¬¸ì
            r'Â¿[^?]*\?',  # ìŠ¤í˜ì¸ì–´ ì˜ë¬¸ë¬¸
            r'Â¡[^!]*!'   # ìŠ¤í˜ì¸ì–´ ê°íƒ„ë¬¸
        ]
        
        # ì˜ì–´ íŠ¹ì§•ì ì¸ íŒ¨í„´ë“¤
        english_patterns = [
            r'\b(the|a|an)\s+\w+',  # ì˜ì–´ ê´€ì‚¬
            r'\b(and|or|but|so|because|when|while)\b',  # ì˜ì–´ ì ‘ì†ì‚¬
            r'\b(is|are|was|were|have|has|had)\b',  # ì˜ì–´ ë™ì‚¬
            r'\b(this|that|these|those|here|there)\b',  # ì˜ì–´ ì§€ì‹œì–´
            r'\b(very|more|most|also|really|actually)\b',  # ì˜ì–´ ë¶€ì‚¬
            r'\b(people|time|year|years|life|work)\b',  # ì¼ë°˜ì ì¸ ì˜ì–´ ëª…ì‚¬
            r'\b(you|I|we|they|he|she|it)\b',  # ì˜ì–´ ëŒ€ëª…ì‚¬
            r"'(s|re|ve|ll|t|d)\b"  # ì˜ì–´ ì¶•ì•½í˜•
        ]
        
        spanish_score = 0
        english_score = 0
        
        # ìŠ¤í˜ì¸ì–´ íŒ¨í„´ ì¹´ìš´íŠ¸
        for pattern in spanish_patterns:
            matches = len(re.findall(pattern, transcript_content, re.IGNORECASE))
            spanish_score += matches
        
        # ì˜ì–´ íŒ¨í„´ ì¹´ìš´íŠ¸
        for pattern in english_patterns:
            matches = len(re.findall(pattern, transcript_content, re.IGNORECASE))
            english_score += matches
        
        print(f"    ğŸ“Š ì–¸ì–´ ë¶„ì„ ê²°ê³¼:")
        print(f"       ìŠ¤í˜ì¸ì–´ ì ìˆ˜: {spanish_score}")
        print(f"       ì˜ì–´ ì ìˆ˜: {english_score}")
        
        # ì ìˆ˜ ë¹„êµë¡œ ì–¸ì–´ íŒë‹¨
        if spanish_score > english_score * 1.5:  # ìŠ¤í˜ì¸ì–´ê°€ í™•ì‹¤íˆ ìš°ì„¸
            print(f"    âœ… ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ë¡œ í™•ì¸ë¨")
            return True
        elif english_score > spanish_score * 1.5:  # ì˜ì–´ê°€ í™•ì‹¤íˆ ìš°ì„¸
            print(f"    ğŸš« ì˜ì–´ ì½˜í…ì¸ ë¡œ í™•ì¸ë¨")
            return False
        else:
            print(f"    âš ï¸ ì–¸ì–´ íŒë‹¨ì´ ì• ë§¤í•¨ - ì œëª©ë¡œ ì¬íŒë‹¨")
            # ì• ë§¤í•œ ê²½ìš° ì œëª©ë¡œ íŒë‹¨
            return is_spanish_content_by_title(title, summary)
    else:
        print(f"    âš ï¸ transcriptë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ ì œëª©ë¡œ íŒë‹¨")
        # transcriptë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ ì œëª©ìœ¼ë¡œ íŒë‹¨
        return is_spanish_content_by_title(title, summary)

def is_spanish_content_by_title(title, summary=""):
    """
    ì œëª©ê³¼ ìš”ì•½ìœ¼ë¡œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ì¸ì§€ íŒë‹¨ (fallback ë°©ë²•)
    """
    content = (title + " " + summary).lower()
    
    # Radio Ambulante í™•ì¸
    if 'radio ambulante' in content:
        return True
    
    # ëª…ë°±í•œ ì˜ì–´ ì œëª© íŒ¨í„´ë“¤
    english_title_patterns = [
        r'\bthe\s+network\b',  # "The Network" ì‹œë¦¬ì¦ˆ
        r'\bbreaking\s+bread\b',
        r'\blife\s+kit\b',
        r'\ball\s+things\s+considered\b',
        r'\bmorning\s+edition\b',
        r'\bweekend\s+edition\b',
        r'\bfresh\s+air\b',
        r'\bon\s+the\s+media\b'
    ]
    
    # ì˜ì–´ ì œëª© íŒ¨í„´ í™•ì¸
    for pattern in english_title_patterns:
        if re.search(pattern, content):
            print(f"    ğŸš« ì˜ì–´ ì œëª© íŒ¨í„´ ë°œê²¬: {pattern}")
            return False
    
    # ìŠ¤í˜ì¸ì–´ ì§€í‘œë“¤
    spanish_indicators = [
        'espaÃ±ol', 'espaÃ±ola', 'latina', 'latino', 'hispanic', 'hispano',
        'mÃ©xico', 'argentina', 'colombia', 'venezuela', 'perÃº', 'chile',
        'guatemala', 'ecuador', 'bolivia', 'paraguay', 'uruguay',
        'centroamÃ©rica', 'sudamÃ©rica', 'latinoamÃ©rica'
    ]
    
    # ì˜ì–´ ì§€í‘œë“¤
    english_indicators = [
        'english', 'american', 'politics', 'election', 'congress',
        'president biden', 'white house', 'washington', 'democrat', 'republican',
        'network', 'breaking bread', 'life kit', 'things considered'
    ]
    
    # ì˜ì–´ ì½˜í…ì¸  í™•ì‹¤í•œ ê²½ìš°
    if any(keyword in content for keyword in english_indicators):
        print(f"    ğŸš« ì˜ì–´ ì½˜í…ì¸  ì§€í‘œ ë°œê²¬")
        return False
    
    # ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  í™•ì‹¤í•œ ê²½ìš°
    if any(keyword in content for keyword in spanish_indicators):
        print(f"    âœ… ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ì§€í‘œ ë°œê²¬")
        return True
    
    # ê¸°ë³¸ì ìœ¼ë¡œ NPR í”¼ë“œëŠ” ì˜ì–´ë¡œ ê°„ì£¼ (ì• ë§¤í•œ ê²½ìš°)
    print(f"    âš ï¸ ì• ë§¤í•œ ê²½ìš° - NPR í”¼ë“œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ ì˜ì–´ë¡œ ê°„ì£¼")
    return False

def extract_grammar_points_from_content(content, difficulty="B2"):
    """
    ê¸°ì‚¬ ë‚´ìš©ì—ì„œ ë ˆë²¨ë³„ ë¬¸ë²• í¬ì¸íŠ¸ ì¶”ì¶œ (LLM ì „ìš©)
    """
    if not content:
        return []
    
    if not LLM_AVAILABLE or not os.environ.get('OPENAI_API_KEY'):
        print("âš ï¸ LLM ë¶„ì„ê¸°ê°€ í•„ìš”í•©ë‹ˆë‹¤. OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return []
    
    try:
        analyzer = SpanishLLMAnalyzer()
        return analyzer.analyze_article_grammar(content, difficulty)
    except Exception as e:
        print(f"LLM ë¬¸ë²• ë¶„ì„ ì˜¤ë¥˜: {e}")
        return []

def extract_vocabulary_expressions_from_transcript(transcript, difficulty="B2"):
    """
    íŒŸìºìŠ¤íŠ¸ transcriptì—ì„œ ë ˆë²¨ë³„ êµ¬ì–´ì²´ í‘œí˜„ ì¶”ì¶œ (LLM ì „ìš©)
    """
    if not transcript:
        print("âš ï¸ transcript ë‚´ìš©ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        return []
    
    if not LLM_AVAILABLE or not os.environ.get('OPENAI_API_KEY'):
        print("âš ï¸ LLM ë¶„ì„ê¸°ê°€ í•„ìš”í•©ë‹ˆë‹¤. OPENAI_API_KEYë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        return []
    
    try:
        print(f"    ğŸ” LLM ë¶„ì„ ì‹œì‘ - ì½˜í…ì¸  ê¸¸ì´: {len(transcript)}ì, ë‚œì´ë„: {difficulty}")
        print(f"    ğŸ“„ ë¶„ì„í•  ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°: {transcript[:200]}...")
        
        analyzer = SpanishLLMAnalyzer()
        result = analyzer.analyze_podcast_colloquialisms(transcript, difficulty)
        
        if result:
            print(f"    âœ… LLM ë¶„ì„ ì„±ê³µ! ì¶”ì¶œëœ êµ¬ì–´ì²´ í‘œí˜„ {len(result)}ê°œ:")
            for i, expr in enumerate(result, 1):
                print(f"       {i}. {expr}")
            return result
        else:
            print("    âš ï¸ LLMì—ì„œ êµ¬ì–´ì²´ í‘œí˜„ì„ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
            return []
    except Exception as e:
        print(f"    âŒ LLM êµ¬ì–´ì²´ í‘œí˜„ ë¶„ì„ ì˜¤ë¥˜: {e}")
        return []

def get_podcast_transcript_or_content(episode_url, episode_title):
    """
    íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œ URLì—ì„œ transcriptë‚˜ ìƒì„¸ ë‚´ìš©ì„ ê°€ì ¸ì˜¤ê¸°
    ì›ë³¸ URLì„ ìš°ì„ ì ìœ¼ë¡œ í™•ì¸ í›„ ë‹¤ë¥¸ ì†ŒìŠ¤ ê²€ìƒ‰
    """
    print(f"    ğŸ” ì½˜í…ì¸  ê²€ìƒ‰ ì‹œì‘ - ì›ë³¸ URL ìš°ì„  í™•ì¸...")
    
    # ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
    globals()['found_apple_url'] = None
    
    # 1. ë¨¼ì € ì›ë³¸ URLì—ì„œ transcript ì‹œë„ (ê°€ì¥ ìš°ì„ ìˆœìœ„)
    print(f"    ğŸ“„ ì›ë³¸ URLì—ì„œ transcript ì¶”ì¶œ ì‹œë„: {episode_url}")
    content = try_extract_from_url(episode_url, episode_title)
    if content:
        print(f"    âœ… ì›ë³¸ URLì—ì„œ ì½˜í…ì¸  ë°œê²¬! (ê¸¸ì´: {len(content)}ì)")
        return content
    
    print(f"    âš ï¸ ì›ë³¸ URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•¨ - ë‹¤ë¥¸ ì†ŒìŠ¤ ê²€ìƒ‰ ì‹œì‘...")
    
    # 2. Radio Ambulante ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰
    if 'Radio Ambulante' in episode_title or 'radioambulante.org' in episode_url:
        print(f"    ğŸŒ Radio Ambulante ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²€ìƒ‰...")
        content = search_radio_ambulante_website(episode_title)
        if content:
            return content
    
    # 3. YouTubeì—ì„œ ê°™ì€ ì—í”¼ì†Œë“œ ê²€ìƒ‰
    print(f"    ğŸ“º YouTubeì—ì„œ ìë§‰ ê²€ìƒ‰...")
    content = search_youtube_transcript(episode_title)
    if content:
        return content
    
    # 4. íŒŸìºìŠ¤íŠ¸ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ê²€ìƒ‰
    print(f"    ğŸ“ íŒŸìºìŠ¤íŠ¸ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ê²€ìƒ‰...")
    content = search_podcast_website(episode_title, episode_url)
    if content:
        return content
    
    # 5. Apple Podcastsì—ì„œ ì—í”¼ì†Œë“œ ì„¤ëª… ê²€ìƒ‰
    print(f"    ğŸ Apple Podcastsì—ì„œ ì—í”¼ì†Œë“œ ì„¤ëª… ê²€ìƒ‰...")
    content = search_apple_podcast_description(episode_title)
    if content:
        return content
    
    print(f"    âŒ ëª¨ë“  ì†ŒìŠ¤ì—ì„œ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•¨")
    return ""

def get_found_apple_url():
    """Apple Podcast ê²€ìƒ‰ì—ì„œ ë°œê²¬ëœ URL ë°˜í™˜"""
    return globals().get('found_apple_url', None)

def try_extract_from_url(episode_url, episode_title):
    """ì›ë³¸ URLì—ì„œ transcript ì¶”ì¶œ ì‹œë„"""
    try:
        print(f"    ğŸ“„ {episode_url} ì ‘ì† ì¤‘...")
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(episode_url, headers=headers, timeout=10)
        if response.status_code != 200:
            print(f"    âŒ HTTP ì˜¤ë¥˜: {response.status_code}")
            return ""
        
        print(f"    ğŸ“‹ í˜ì´ì§€ íŒŒì‹± ì¤‘...")
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë²”ìš© transcript ì¶”ì¶œ ë¡œì§
        print(f"    ğŸ” í˜ì´ì§€ì—ì„œ transcript/ì½˜í…ì¸  ì¶”ì¶œ ì¤‘...")
        
        # 1. transcript ê´€ë ¨ ë²„íŠ¼ì´ë‚˜ ë§í¬ì—ì„œ ì‹¤ì œ transcript URL ì°¾ê¸°
        print(f"    ğŸ” transcript ë²„íŠ¼/ë§í¬ì—ì„œ URL ì¶”ì¶œ ì‹œë„...")
        transcript_buttons = soup.find_all(['a', 'button'], string=re.compile(r'(transcript|transcripciÃ³n|ver transcripciÃ³n)', re.IGNORECASE))
        for button in transcript_buttons:
            href = button.get('href')
            onclick = button.get('onclick', '')
            data_url = button.get('data-url', '')
            
            # ê°€ëŠ¥í•œ transcript URLë“¤
            possible_urls = []
            if href and not href.startswith('#'):
                from urllib.parse import urljoin
                # ìƒëŒ€ URLì„ ì ˆëŒ€ URLë¡œ ë³€í™˜
                absolute_url = urljoin(episode_url, href)
                possible_urls.append(absolute_url)
                print(f"    ğŸ”— transcript ë§í¬ ë°œê²¬: {href} â†’ {absolute_url}")
            
            if data_url:
                from urllib.parse import urljoin
                absolute_data_url = urljoin(episode_url, data_url)
                possible_urls.append(absolute_data_url)
                print(f"    ğŸ”— data-url ë°œê²¬: {data_url} â†’ {absolute_data_url}")
            
            # onclickì—ì„œ URL ì¶”ì¶œ
            if onclick:
                url_match = re.search(r'["\']([^"\']*transcript[^"\']*)["\']', onclick)
                if url_match:
                    onclick_url = url_match.group(1)
                    from urllib.parse import urljoin
                    absolute_onclick_url = urljoin(episode_url, onclick_url)
                    possible_urls.append(absolute_onclick_url)
                    print(f"    ğŸ”— onclick URL ë°œê²¬: {onclick_url} â†’ {absolute_onclick_url}")
            
            # ì¶”ì¶œëœ URLë“¤ ì‹œë„
            for transcript_url in possible_urls:
                try:
                    print(f"    ğŸ” transcript URL ì‹œë„: {transcript_url}")
                    transcript_response = requests.get(transcript_url, headers=headers, timeout=10)
                    if transcript_response.status_code == 200:
                        transcript_content = transcript_response.text.strip()
                        # HTMLì¸ ê²½ìš° í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
                        if transcript_content.startswith('<'):
                            transcript_soup = BeautifulSoup(transcript_content, 'html.parser')
                            transcript_content = transcript_soup.get_text().strip()
                        
                        if len(transcript_content) > 100:
                            print(f"    âœ… transcript URLì—ì„œ ì½˜í…ì¸  ë°œê²¬! (ê¸¸ì´: {len(transcript_content)}ì)")
                            return transcript_content[:3000]
                        else:
                            print(f"    âš ï¸ transcript ë‚´ìš©ì´ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(transcript_content)}ì)")
                    else:
                        print(f"    âŒ HTTP ì˜¤ë¥˜: {transcript_response.status_code}")
                except Exception as e:
                    print(f"    âŒ transcript URL ì ‘ê·¼ ì‹¤íŒ¨: {e}")
                    continue
        
        # 2. í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ JavaScript ë³€ìˆ˜ë‚˜ JSON ë°ì´í„°ë¡œ embeddedëœ transcript ì°¾ê¸°
        print(f"    ğŸ” JavaScript/JSON ë°ì´í„°ì—ì„œ transcript ê²€ìƒ‰...")
        page_content = response.text
        
        # JavaScript ë³€ìˆ˜ì—ì„œ transcript ì¶”ì¶œ íŒ¨í„´ë“¤
        js_patterns = [
            r'transcript["\']?\s*:\s*["\']([^"\']{200,})["\']',
            r'transcription["\']?\s*:\s*["\']([^"\']{200,})["\']',
            r'content["\']?\s*:\s*["\']([^"\']{200,})["\']',
            r'text["\']?\s*:\s*["\']([^"\']{200,})["\']'
        ]
        
        for pattern in js_patterns:
            matches = re.findall(pattern, page_content, re.IGNORECASE | re.DOTALL)
            for match in matches:
                # HTML ì—”í‹°í‹° ë””ì½”ë”© ë° ì •ë¦¬
                clean_text = match.replace('\\n', '\n').replace('\\t', ' ').replace('\\"', '"')
                if len(clean_text) > 200 and any(word in clean_text.lower() for word in ['el ', 'la ', 'es ', 'que ', 'con ']):
                    print(f"    âœ… JavaScript ë°ì´í„°ì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ë°œê²¬! (ê¸¸ì´: {len(clean_text)}ì)")
                    return clean_text[:3000]
        
        # 3. í¬ê´„ì ì¸ CSS ì…€ë ‰í„°ë¡œ ì½˜í…ì¸  ì¶”ì¶œ
        print(f"    ğŸ” CSS ì…€ë ‰í„°ë¡œ ì½˜í…ì¸  ì¶”ì¶œ...")
        
        # transcript ê´€ë ¨ ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        priority_selectors = [
            '.transcript', '.transcription', '.episode-transcript', '.transcript-content',
            '#transcript', '#transcription', '[data-transcript]', '[class*="transcript"]'
        ]
        
        # ì¼ë°˜ì ì¸ ì½˜í…ì¸  ì…€ë ‰í„°ë“¤
        content_selectors = [
            '.episode-content', '.episode-description', '.show-notes', '.episode-notes',
            '.post-content', '.entry-content', '.content', '.description', '.summary',
            'article', 'main', '.story-content', '.episode-body'
        ]
        
        # ìš°ì„ ìˆœìœ„ ì…€ë ‰í„°ë“¤ ë¨¼ì € ì‹œë„
        for selector in priority_selectors:
            elements = soup.select(selector)
            if elements:
                content = ' '.join([elem.get_text().strip() for elem in elements])
                if len(content) > 100:
                    print(f"    âœ… ìš°ì„ ìˆœìœ„ ì…€ë ‰í„°ì—ì„œ ì½˜í…ì¸  ë°œê²¬! (ì…€ë ‰í„°: {selector}, ê¸¸ì´: {len(content)}ì)")
                    return content[:3000]
        
        # ì¼ë°˜ ì½˜í…ì¸  ì…€ë ‰í„°ë“¤ ì‹œë„
        for selector in content_selectors:
            elements = soup.select(selector)
            if elements:
                content = ' '.join([elem.get_text().strip() for elem in elements])
                if len(content) > 200:  # ì¼ë°˜ ì½˜í…ì¸ ëŠ” ë” ê¸´ í…ìŠ¤íŠ¸ë§Œ í—ˆìš©
                    print(f"    âœ… ì¼ë°˜ ì…€ë ‰í„°ì—ì„œ ì½˜í…ì¸  ë°œê²¬! (ì…€ë ‰í„°: {selector}, ê¸¸ì´: {len(content)}ì)")
                    return content[:3000]
        
        # 4. í˜ì´ì§€ì˜ ëª¨ë“  ë¬¸ë‹¨ì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  í•„í„°ë§
        print(f"    ğŸ” í˜ì´ì§€ ì „ì²´ì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ê²€ìƒ‰...")
        all_paragraphs = soup.find_all('p')
        spanish_content = []
        
        for p in all_paragraphs:
            text = p.get_text().strip()
            if len(text) > 30:  # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ ì œì™¸
                # ìŠ¤í˜ì¸ì–´ íŒ¨í„´ í™•ì¸ (ë” í¬ê´„ì )
                spanish_patterns = ['el ', 'la ', 'es ', 'que ', 'con ', 'por ', 'para ', 'de ', 'en ', 'un ', 'una ']
                if any(pattern in text.lower() for pattern in spanish_patterns):
                    # ë„¤ë¹„ê²Œì´ì…˜ì´ë‚˜ ë©”ë‰´ í…ìŠ¤íŠ¸ ì œì™¸
                    if not any(nav_word in text.lower() for nav_word in ['inicio', 'contacto', 'sobre', 'menu', 'copyright', 'Â©']):
                        spanish_content.append(text)
        
        if spanish_content:
            content = ' '.join(spanish_content)
            if len(content) > 200:
                print(f"    âœ… í˜ì´ì§€ì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ë°œê²¬! (ê¸¸ì´: {len(content)}ì)")
                return content[:3000]
        
        print(f"    âŒ ì›ë³¸ URLì—ì„œ ì¶©ë¶„í•œ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í•¨")
        return ""
        
    except Exception as e:
        print(f"    âŒ ì›ë³¸ URL transcript ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        return ""

def search_radio_ambulante_website(episode_title):
    """Radio Ambulante ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì—í”¼ì†Œë“œ ê²€ìƒ‰"""
    try:
        # ì—í”¼ì†Œë“œ ì œëª©ì—ì„œ ìŠ¬ëŸ¬ê·¸ ìƒì„±
        import re
        title_clean = re.sub(r'[^\w\s-]', '', episode_title.lower())
        slug = re.sub(r'[-\s]+', '-', title_clean).strip('-')
        
        # ì—¬ëŸ¬ ê°€ëŠ¥í•œ URL íŒ¨í„´ ì‹œë„
        possible_urls = [
            f"https://radioambulante.org/audio/{slug}",
            f"https://radioambulante.org/episodes/{slug}",
            f"https://radioambulante.org/podcast/{slug}"
        ]
        
        for url in possible_urls:
            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
                
                response = requests.get(url, headers=headers, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Radio Ambulante íŠ¹í™” ì…€ë ‰í„°ë“¤
                    selectors = [
                        '.episode-transcript',
                        '.transcript-content',
                        '.episode-content p',
                        '.story-content p',
                        '.post-content p',
                        '.entry-content p'
                    ]
                    
                    for selector in selectors:
                        elements = soup.select(selector)
                        if elements:
                            content = ' '.join([elem.get_text().strip() for elem in elements])
                            if len(content) > 200:
                                print(f"    âœ… Radio Ambulante ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì½˜í…ì¸  ë°œê²¬ (ê¸¸ì´: {len(content)}ì)")
                                return content[:3000]
                            
            except Exception as e:
                continue
        
        return ""
        
    except Exception as e:
        print(f"    âŒ Radio Ambulante ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def search_youtube_transcript(episode_title):
    """YouTubeì—ì„œ ê°™ì€ ì—í”¼ì†Œë“œì˜ ìë§‰ ê²€ìƒ‰"""
    try:
        # YouTube ê²€ìƒ‰ URL ìƒì„±
        search_query = f"{episode_title} transcript"
        search_url = f"https://www.youtube.com/results?search_query={urllib.parse.quote(search_query)}"
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(search_url, headers=headers, timeout=10)
        if response.status_code == 200:
            # YouTube ê²€ìƒ‰ ê²°ê³¼ì—ì„œ ë¹„ë””ì˜¤ ID ì¶”ì¶œ
            video_id_pattern = r'"videoId":"([^"]+)"'
            video_ids = re.findall(video_id_pattern, response.text)
            
            if video_ids:
                # ì²« ë²ˆì§¸ ë¹„ë””ì˜¤ì˜ ì„¤ëª… ê°€ì ¸ì˜¤ê¸° ì‹œë„
                video_url = f"https://www.youtube.com/watch?v={video_ids[0]}"
                video_response = requests.get(video_url, headers=headers, timeout=10)
                
                if video_response.status_code == 200:
                    soup = BeautifulSoup(video_response.content, 'html.parser')
                    
                    # ë¹„ë””ì˜¤ ì„¤ëª… ì¶”ì¶œ
                    description_selectors = [
                        '[data-content]',
                        '.description',
                        '#description'
                    ]
                    
                    for selector in description_selectors:
                        elements = soup.select(selector)
                        if elements:
                            content = ' '.join([elem.get_text().strip() for elem in elements])
                            if len(content) > 200:
                                print(f"    âœ… YouTube ì—í”¼ì†Œë“œ ì„¤ëª… ë°œê²¬ (ê¸¸ì´: {len(content)}ì)")
                                return content[:3000]
        
        return ""
        
    except Exception as e:
        print(f"    âŒ YouTube ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def search_podcast_website(episode_title, episode_url):
    """íŒŸìºìŠ¤íŠ¸ ê³µì‹ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ê²€ìƒ‰"""
    try:
        # URLì—ì„œ ë„ë©”ì¸ ì¶”ì¶œ
        from urllib.parse import urlparse
        parsed_url = urlparse(episode_url)
        domain = parsed_url.netloc
        
        # ë„ë©”ì¸ë³„ íŠ¹í™” ê²€ìƒ‰
        if 'spanishpodcast.org' in domain:
            return search_spanishpodcast_website(episode_title)
        elif 'espanolistos.com' in domain:
            return search_espanolistos_website(episode_title)
        else:
            # ì¼ë°˜ì ì¸ íŒŸìºìŠ¤íŠ¸ ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰
            return search_general_podcast_website(episode_url)
        
    except Exception as e:
        print(f"    âŒ íŒŸìºìŠ¤íŠ¸ ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def search_spanishpodcast_website(episode_title):
    """SpanishPodcast ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ê²€ìƒ‰"""
    try:
        # SpanishPodcast ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ë¡œì§
        base_url = "https://www.spanishpodcast.org"
        
        # ì—í”¼ì†Œë“œ ë²ˆí˜¸ ì¶”ì¶œ
        episode_num = extract_episode_number(episode_title)
        if episode_num:
            episode_url = f"{base_url}/podcasts/{episode_num}.html"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(episode_url, headers=headers, timeout=10)
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                
                # SpanishPodcast íŠ¹í™” ì…€ë ‰í„°ë“¤
                selectors = [
                    '.episode-content',
                    '.show-notes',
                    '.description',
                    'article p',
                    '.content p'
                ]
                
                for selector in selectors:
                    elements = soup.select(selector)
                    if elements:
                        content = ' '.join([elem.get_text().strip() for elem in elements])
                        if len(content) > 200:
                            print(f"    âœ… SpanishPodcast ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ë°œê²¬ (ê¸¸ì´: {len(content)}ì)")
                            return content[:3000]
        
        return ""
        
    except Exception as e:
        print(f"    âŒ SpanishPodcast ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def search_espanolistos_website(episode_title):
    """EspaÃ±olistos ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ê²€ìƒ‰"""
    try:
        # EspaÃ±olistos ì›¹ì‚¬ì´íŠ¸ëŠ” Spotify ê¸°ë°˜ì´ë¯€ë¡œ ë‹¤ë¥¸ ì ‘ê·¼ í•„ìš”
        # ì¼ë°˜ì ì¸ ê²€ìƒ‰ ì‹œë„
        search_query = f"site:espanolistos.com {episode_title}"
        
        # êµ¬ê¸€ ê²€ìƒ‰ ì‹œë®¬ë ˆì´ì…˜ì€ ë³µì¡í•˜ë¯€ë¡œ ì¼ë‹¨ íŒ¨ìŠ¤
        return ""
        
    except Exception as e:
        print(f"    âŒ EspaÃ±olistos ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def search_general_podcast_website(episode_url):
    """ì¼ë°˜ì ì¸ íŒŸìºìŠ¤íŠ¸ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ê²€ìƒ‰"""
    try:
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(episode_url, headers=headers, timeout=10)
        if response.status_code == 200:
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì¼ë°˜ì ì¸ ì‡¼ë…¸íŠ¸ ì…€ë ‰í„°ë“¤
            selectors = [
                '.show-notes',
                '.episode-notes',
                '.description',
                '.summary',
                '.content',
                'article',
                '.post-content'
            ]
            
            for selector in selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text().strip() for elem in elements])
                    if len(content) > 200:
                        print(f"    âœ… ì¼ë°˜ íŒŸìºìŠ¤íŠ¸ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì‡¼ë…¸íŠ¸ ë°œê²¬ (ê¸¸ì´: {len(content)}ì)")
                        return content[:3000]
        
        return ""
        
    except Exception as e:
        print(f"    âŒ ì¼ë°˜ íŒŸìºìŠ¤íŠ¸ ì›¹ì‚¬ì´íŠ¸ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def search_apple_podcast_description(episode_title):
    """Apple Podcastsì—ì„œ ì—í”¼ì†Œë“œ ì„¤ëª… ê²€ìƒ‰í•˜ê³  URLë„ ë°˜í™˜"""
    try:
        # iTunes Search APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì—í”¼ì†Œë“œ ì„¤ëª… ê°€ì ¸ì˜¤ê¸°
        search_term = episode_title
        encoded_term = urllib.parse.quote(search_term)
        search_url = f"https://itunes.apple.com/search?term={encoded_term}&media=podcast&entity=podcastEpisode&limit=5"
        
        print(f"    ğŸ” iTunes Search API í˜¸ì¶œ: {search_url}")
        
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        }
        
        response = requests.get(search_url, headers=headers, timeout=10)
        if response.status_code == 200:
            data = response.json()
            results = data.get('results', [])
            
            print(f"    ğŸ“Š iTunes Search ê²°ê³¼: {len(results)}ê°œ ì—í”¼ì†Œë“œ ë°œê²¬")
            
            for i, result in enumerate(results, 1):
                result_title = result.get('trackName', '').lower()
                track_view_url = result.get('trackViewUrl', '')
                print(f"    ğŸ“º ê²°ê³¼ {i}: {result.get('trackName', 'N/A')}")
                
                if any(word in result_title for word in episode_title.lower().split() if len(word) > 3):
                    description = result.get('description', '') or result.get('longDescription', '')
                    if description and len(description) > 200:
                        print(f"    âœ… Apple Podcastsì—ì„œ ì—í”¼ì†Œë“œ ì„¤ëª… ë°œê²¬ (ê¸¸ì´: {len(description)}ì)")
                        print(f"    ğŸ“„ ì„¤ëª… ë¯¸ë¦¬ë³´ê¸°: {description[:150]}...")
                        
                        # URLë„ í•¨ê»˜ ì €ì¥ (ì „ì—­ ë³€ìˆ˜ë‚˜ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ)
                        if track_view_url:
                            print(f"    ğŸ”— í•´ë‹¹ ì—í”¼ì†Œë“œ URL: {track_view_url}")
                            # ì „ì—­ ë³€ìˆ˜ì— ì €ì¥í•˜ì—¬ ë‚˜ì¤‘ì— ì‚¬ìš©
                            globals()['found_apple_url'] = track_view_url
                        
                        return description[:3000]
                    else:
                        print(f"    âš ï¸ ì„¤ëª…ì´ ë„ˆë¬´ ì§§ìŒ (ê¸¸ì´: {len(description) if description else 0}ì)")
        else:
            print(f"    âŒ iTunes Search API í˜¸ì¶œ ì‹¤íŒ¨: {response.status_code}")
        
        return ""
        
    except Exception as e:
        print(f"    âŒ Apple Podcasts ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
        return ""

def main():
    # í™˜ê²½ë³€ìˆ˜ì—ì„œ ì„¤ì •ê°’ ê°€ì ¸ì˜¤ê¸°
    reading_source = os.environ.get('READING_SOURCE', '')
    preset_difficulty = os.environ.get('READING_DIFFICULTY', 'B2')  # ê¸°ë³¸ê°’ìœ¼ë¡œë§Œ ì‚¬ìš©
    podcast_rss = os.environ.get('PODCAST_RSS', '')
    podcast_name = os.environ.get('PODCAST_NAME', '')
    weekday_name = os.environ.get('WEEKDAY_NAME', '')
    podcast_apple_base = os.environ.get('PODCAST_APPLE_BASE', '')
    force_alternative = os.environ.get('FORCE_ALTERNATIVE', 'false').lower() == 'true'
    
    article_data = None
    podcast_data = None  # ëª…ì‹œì ìœ¼ë¡œ Noneìœ¼ë¡œ ì´ˆê¸°í™”

    print(f"=== í•™ìŠµ ìë£Œ ìˆ˜ì§‘ ì‹œì‘ ===")
    print(f"ë…í•´ ì†ŒìŠ¤: {reading_source}")
    print(f"íŒŸìºìŠ¤íŠ¸: {podcast_name}")
    print(f"íŒŸìºìŠ¤íŠ¸ RSS: {podcast_rss}")
    print(f"ìš”ì¼: {weekday_name}")
    print(f"ëŒ€ì•ˆ ëª¨ë“œ: {force_alternative}")
    print(f"====================")

    # ê¸°ì‚¬ ìˆ˜ì§‘ ë° ì‹¤ì œ ë‚´ìš© ë¶„ì„
    try:
        if reading_source == "20minutos":
            feed_url = "https://www.20minutos.es/rss/"
        elif "El PaÃ­s" in reading_source:
            if "ì‚¬ì„¤" in reading_source:
                feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/section/opinion"
            else:
                feed_url = "https://feeds.elpais.com/mrss-s/pages/ep/site/elpais.com/portada"
        elif reading_source == "El Mundo":
            feed_url = "https://e00-elmundo.uecdn.es/elmundo/rss/portada.xml"
        elif reading_source == "ABC":
            feed_url = "https://www.abc.es/rss/feeds/abc_EspanaEspana.xml"
        else:
            # ê¸°ë³¸ê°’
            feed_url = "https://www.20minutos.es/rss/"
        
        print(f"RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ ì •ë³´ ìˆ˜ì§‘ ì¤‘: {feed_url}")
        feed = feedparser.parse(feed_url)
        
        if feed.entries:
            # ëŒ€ì•ˆ ëª¨ë“œì—ì„œëŠ” ì—¬ëŸ¬ ê¸°ì‚¬ ì¤‘ì—ì„œ ì„ íƒ
            entry_index = 0
            if force_alternative:
                # ëŒ€ì•ˆ ëª¨ë“œì—ì„œëŠ” ë‘ ë²ˆì§¸ ë˜ëŠ” ì„¸ ë²ˆì§¸ ê¸°ì‚¬ ì‹œë„
                import random
                entry_index = min(random.randint(1, 3), len(feed.entries) - 1)
                print(f"ëŒ€ì•ˆ ëª¨ë“œ: {entry_index + 1}ë²ˆì§¸ ê¸°ì‚¬ ì„ íƒ")
            
            latest = feed.entries[entry_index]
            article_url = latest.link
            clean_title = latest.title.replace('&quot;', '"').replace('&amp;', '&').replace('&lt;', '<').replace('&gt;', '>')
            
            print(f"ê¸°ì‚¬ URL ì ‘ì† ì¤‘: {article_url}")
            # ì‹¤ì œ ê¸°ì‚¬ ë‚´ìš© ê°€ì ¸ì˜¤ê¸°
            article_content = get_article_content(article_url)
            
            if article_content:
                print(f"ê¸°ì‚¬ ë‚´ìš© ë¶„ì„ ì¤‘... (ë‚´ìš© ê¸¸ì´: {len(article_content)}ì)")
                
                # ë™ì  ë‚œì´ë„ ë¶„ì„
                analyzed_difficulty = analyze_text_difficulty(article_content)
                print(f"ë¶„ì„ëœ ë‚œì´ë„: {analyzed_difficulty}")
                
                # ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = extract_category_from_content(clean_title, article_content)
                
                print(f"ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬: {category}")
                
                article_data = {
                    'title': clean_title,
                    'url': article_url,
                    'published': latest.get('published', ''),
                    'category': category,
                    'difficulty': analyzed_difficulty,  # ë™ì ìœ¼ë¡œ ë¶„ì„ëœ ë‚œì´ë„ ì‚¬ìš©
                    'content_preview': article_content[:200] + "..." if len(article_content) > 200 else article_content
                }
            else:
                print("ê¸°ì‚¬ ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ì–´ RSS ìš”ì•½ ì‚¬ìš©")
                # ë‚´ìš©ì„ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìœ¼ë©´ RSS ìš”ì•½ ì‚¬ìš©
                summary = latest.get('summary', '')
                analyzed_difficulty = analyze_text_difficulty(summary) if summary else preset_difficulty
                category = extract_category_from_content(clean_title, summary)
                
                article_data = {
                    'title': clean_title,
                    'url': article_url,
                    'published': latest.get('published', ''),
                    'category': category,
                    'difficulty': analyzed_difficulty,
                    'content_preview': summary
                }
                
        else:
            print("RSS í”¼ë“œì—ì„œ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
    except Exception as e:
        print(f"ê¸°ì‚¬ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    # íŒŸìºìŠ¤íŠ¸ ì—í”¼ì†Œë“œ ìˆ˜ì§‘
    try:
        print(f"íŒŸìºìŠ¤íŠ¸ RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘: {podcast_rss}")
        feed = feedparser.parse(podcast_rss)
        
        print(f"í”¼ë“œ íŒŒì‹± ê²°ê³¼:")
        print(f"- í”¼ë“œ ì œëª©: {feed.feed.get('title', 'ì œëª© ì—†ìŒ')}")
        print(f"- ì—í”¼ì†Œë“œ ê°œìˆ˜: {len(feed.entries)}")
        print(f"- í”¼ë“œ ìƒíƒœ: {getattr(feed, 'status', 'N/A')}")
        
        if hasattr(feed, 'bozo') and feed.bozo:
            print(f"- RSS í”¼ë“œ íŒŒì‹± ê²½ê³ : {getattr(feed, 'bozo_exception', 'Unknown')}")
            
        # í”¼ë“œ ìƒíƒœê°€ 404ì´ê±°ë‚˜ ì—í”¼ì†Œë“œê°€ ì—†ìœ¼ë©´ ì¦‰ì‹œ ë°±ì—… í”¼ë“œ ì‹œë„
        if (hasattr(feed, 'status') and feed.status == 404) or len(feed.entries) == 0:
            print(f"âš ï¸  ë©”ì¸ RSS í”¼ë“œ ì‚¬ìš© ë¶ˆê°€ (ìƒíƒœ: {getattr(feed, 'status', 'N/A')}, ì—í”¼ì†Œë“œ: {len(feed.entries)})")
            print("ë°±ì—… RSS í”¼ë“œë“¤ì„ ì‹œë„í•©ë‹ˆë‹¤...")
            
            # ë°±ì—… RSS í”¼ë“œë“¤ ì‹œë„ - ê²€ì¦ëœ ì‹¤ì œ ì‘ë™í•˜ëŠ” í”¼ë“œ URLë“¤ ì‚¬ìš©
            backup_feeds = []
            
            # ìš”ì¼ì— ë”°ë¼ ì ì ˆí•œ ë°±ì—… í”¼ë“œë“¤ ì„¤ì •
            if weekday_name == "ìˆ˜ìš”ì¼":
                # ìˆ˜ìš”ì¼ì€ SpanishWithVicenteì´ì§€ë§Œ í”¼ë“œê°€ ì‘ë™í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë‹¤ë¥¸ ì˜µì…˜ë“¤ ì‹œë„
                backup_feeds = [
                    ("https://feeds.feedburner.com/SpanishPodcast", "SpanishPodcast", "https://podcasts.apple.com/us/podcast/spanishpodcast/id70077665"),
                    ("https://feeds.simplecast.com/54nAGcIl", "Radio Ambulante", "https://podcasts.apple.com/kr/podcast/radio-ambulante/id527614348"),
                    ("https://creators.spotify.com/pod/show/espanolistos/rss", "EspaÃ±olistos", "https://podcasts.apple.com/us/podcast/espa%C3%B1olistos/id1508733186")
                ]
            else:
                # ë‹¤ë¥¸ ìš”ì¼ë“¤ì˜ ì¼ë°˜ì ì¸ ë°±ì—… í”¼ë“œë“¤
                backup_feeds = [
                    ("https://feeds.feedburner.com/SpanishPodcast", "SpanishPodcast", "https://podcasts.apple.com/us/podcast/spanishpodcast/id70077665"),
                    ("https://feeds.simplecast.com/54nAGcIl", "Radio Ambulante", "https://podcasts.apple.com/kr/podcast/radio-ambulante/id527614348"),
                    ("https://creators.spotify.com/pod/show/espanolistos/rss", "EspaÃ±olistos", "https://podcasts.apple.com/us/podcast/espa%C3%B1olistos/id1508733186"),
                    ("https://feeds.feedburner.com/SpanishPodcast", "SpanishPodcast (ê¸ˆìš”ì¼)", "https://podcasts.apple.com/us/podcast/spanishpodcast/id70077665")
                ]
            
            for backup_url, backup_podcast_name, backup_apple_base in backup_feeds:
                try:
                    print(f"ë°±ì—… í”¼ë“œ ì‹œë„: {backup_url} ({backup_podcast_name})")
                    backup_feed = feedparser.parse(backup_url)
                    
                    # ë°±ì—… í”¼ë“œ ìƒíƒœ í™•ì¸
                    backup_status = getattr(backup_feed, 'status', 'N/A')
                    print(f"ë°±ì—… í”¼ë“œ ìƒíƒœ: {backup_status}, ì—í”¼ì†Œë“œ ê°œìˆ˜: {len(backup_feed.entries)}")
                    
                    if backup_feed.entries:
                        # ë°±ì—… í”¼ë“œì—ì„œë„ ìµœê·¼ ì—í”¼ì†Œë“œ í™•ì¸
                        recent_episodes = []
                        for entry in backup_feed.entries[:3]:  # ë°±ì—…ì—ì„œëŠ” 3ê°œë§Œ í™•ì¸
                            # NPR í”¼ë“œì¸ ê²½ìš° ì‹¤ì œ transcriptë¡œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ì¸ì§€ í™•ì¸
                            if 'npr.org' in backup_url:
                                episode_link = entry.link
                                if not is_spanish_content_by_transcript(episode_link, entry.title, entry.get('summary', '')):
                                    print(f"      âŒ ë°±ì—… í”¼ë“œì—ì„œ ì˜ì–´ ì½˜í…ì¸  ê±´ë„ˆë›°ê¸°: {entry.title}")
                                    continue
                                else:
                                    print(f"      âœ… ë°±ì—… í”¼ë“œì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  í™•ì¸: {entry.title}")
                            
                            if is_episode_recent(entry.get('published_parsed')):
                                recent_episodes.append(entry)
                        
                        if not recent_episodes:
                            # NPR í”¼ë“œì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ì°¾ê¸°
                            if 'npr.org' in backup_url:
                                print("      ìµœê·¼ ìŠ¤í˜ì¸ì–´ ì—í”¼ì†Œë“œê°€ ì—†ì–´ ì „ì²´ í”¼ë“œì—ì„œ ê²€ìƒ‰ ì¤‘...")
                                for entry in backup_feed.entries:  # ì „ì²´ í”¼ë“œ ê²€ìƒ‰
                                    if is_spanish_content_by_transcript(entry.link, entry.title, entry.get('summary', '')):
                                        recent_episodes = [entry]
                                        print(f"      âœ… ìŠ¤í˜ì¸ì–´ ì—í”¼ì†Œë“œ ë°œê²¬: {entry.title}")
                                        break
                                
                                # NPR í”¼ë“œì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ë¥¼ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ë‹¤ë¥¸ íŒŸìºìŠ¤íŠ¸ë¡œ ì „í™˜
                                if not recent_episodes:
                                    print(f"      ğŸš« {backup_podcast_name} í”¼ë“œì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                                    continue  # ë‹¤ìŒ ë°±ì—… í”¼ë“œë¡œ ì´ë™
                            else:
                                # ë‹¤ë¥¸ í”¼ë“œëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€
                                recent_episodes = [backup_feed.entries[0]]
                        
                        # ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ê°€ ìˆëŠ” ì—í”¼ì†Œë“œë§Œ ì²˜ë¦¬
                        if recent_episodes:
                            # ëŒ€ì•ˆ ëª¨ë“œì—ì„œëŠ” ë‹¤ë¥¸ ì—í”¼ì†Œë“œ ì„ íƒ
                            episode_index = 0
                            if force_alternative and len(recent_episodes) > 1:
                                import random
                                episode_index = random.randint(0, len(recent_episodes) - 1)
                                print(f"ëŒ€ì•ˆ ëª¨ë“œ: {episode_index + 1}ë²ˆì§¸ ì—í”¼ì†Œë“œ ì„ íƒ")
                            
                            latest = recent_episodes[episode_index]
                            print(f"ë°±ì—… í”¼ë“œì—ì„œ ì„ íƒëœ ì—í”¼ì†Œë“œ:")
                            print(f"  ì œëª©: {latest.title}")
                            print(f"  ë°œí–‰ì¼: {latest.get('published', 'N/A')}")
                            print(f"  RSS ì—í”¼ì†Œë“œ URL: {latest.link}")
                            
                            episode_number = extract_episode_number(latest.title)
                            duration = extract_duration_from_feed(latest)
                            topic = extract_topic_keywords(latest.title, latest.get('summary', ''))
                            
                            # Radio Ambulanteì¸ ê²½ìš° ì‹¤ì œ ì›¹ì‚¬ì´íŠ¸ URL ì‹œë„
                            if 'Radio Ambulante' in backup_podcast_name:
                                radio_ambulante_url = extract_radio_ambulante_url(latest)
                                if radio_ambulante_url:
                                    print(f"  Radio Ambulante ì›¹ì‚¬ì´íŠ¸ URL: {radio_ambulante_url}")
                                    episode_link = radio_ambulante_url
                                else:
                                    print(f"  Radio Ambulante ì›¹ì‚¬ì´íŠ¸ URL ì¶”ì¶œ ì‹¤íŒ¨, RSS URL ì‚¬ìš©")
                                    episode_link = latest.link
                            else:
                                episode_link = latest.link
                            
                            # Apple Podcasts ë§í¬ ìƒì„± - ì—í”¼ì†Œë“œ ì œëª© í¬í•¨
                            apple_link = generate_apple_podcast_link(backup_podcast_name, backup_apple_base, episode_link, episode_number, latest.title)
                            
                            # Radio Ambulanteì˜ ê²½ìš° Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URLì„ ë©”ì¸ URLë¡œ ì‚¬ìš©
                            final_episode_url = episode_link
                            if 'Radio Ambulante' in backup_podcast_name:
                                # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
                                if apple_link != backup_apple_base and validate_url(apple_link):
                                    # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ìœ¼ë©´ Apple URLì„ ì‚¬ìš©
                                    final_episode_url = apple_link
                                else:
                                    # Appleì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ì›ë³¸ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                                    final_episode_url = episode_link
                                    apple_link = backup_apple_base  # Apple ë§í¬ëŠ” ë©”ì¸ í˜ì´ì§€ë¡œ ì„¤ì •
                            else:
                                # ë‹¤ë¥¸ íŒŸìºìŠ¤íŠ¸ëŠ” ê¸°ì¡´ ë¡œì§ ìœ ì§€
                                if not validate_url(episode_link):
                                    final_episode_url = apple_link if validate_url(apple_link) else backup_apple_base
                                    
                                if not validate_url(apple_link):
                                    apple_link = backup_apple_base
                            
                            # ë°±ì—… í”¼ë“œ íŒŸìºìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„
                            backup_summary = latest.get('summary', '')
                            backup_difficulty = analyze_text_difficulty(backup_summary) if backup_summary else "B2"
                            
                            podcast_data = {
                                'title': latest.title,
                                'url': final_episode_url,  # Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                                'apple_link': apple_link,
                                'published': latest.get('published', ''),
                                'duration': duration,
                                'episode_number': episode_number or 'N/A',
                                'topic': topic,
                                'podcast_name': backup_podcast_name,
                                'summary': latest.get('summary', '')[:200],
                                'difficulty': backup_difficulty  # ë‚œì´ë„ ì •ë³´ ì¶”ê°€
                            }
                            
                            print(f"âœ… ë°±ì—… í”¼ë“œ ì„±ê³µ! ì‚¬ìš©ëœ í”¼ë“œ: {backup_podcast_name}")
                            print(f"   ì—í”¼ì†Œë“œ: {latest.title}")
                            print(f"âœ… ë°±ì—… í”¼ë“œì—ì„œ ì—í”¼ì†Œë“œ ë°œê²¬!")
                            break
                        else:
                            print(f"      ğŸš« {backup_podcast_name} í”¼ë“œì—ì„œ ì ì ˆí•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                            continue  # ë‹¤ìŒ ë°±ì—… í”¼ë“œë¡œ ì´ë™
                        
                        print(f"âœ… ë°±ì—… í”¼ë“œ ì„±ê³µ! ì‚¬ìš©ëœ í”¼ë“œ: {backup_podcast_name}")
                        print(f"   ì—í”¼ì†Œë“œ: {latest.title}")
                        print(f"âœ… ë°±ì—… í”¼ë“œì—ì„œ ì—í”¼ì†Œë“œ ë°œê²¬!")
                        break
                except Exception as backup_e:
                    print(f"ë°±ì—… í”¼ë“œ ì˜¤ë¥˜ ({backup_podcast_name}): {backup_e}")
                    continue
            
            # ëª¨ë“  ë°±ì—… í”¼ë“œì—ì„œë„ ì¤‘ë³µì´ê±°ë‚˜ ì‹¤íŒ¨í•œ ê²½ìš° ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ ì‹œë„
            if not podcast_data:
                print(f"\nğŸ”„ ëª¨ë“  ë°±ì—… í”¼ë“œ ì‹¤íŒ¨. ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ë¥¼ ì°¾ëŠ” ì¤‘...")
                current_weekday = datetime.now().weekday()
                alternatives = get_alternative_podcasts(current_weekday, podcast_name)
                alternative_podcast = try_alternative_podcast(alternatives, weekday_name)
                
                if alternative_podcast:
                    print(f"âœ… ëŒ€ì•ˆ íŒŸìºìŠ¤íŠ¸ì—ì„œ ìƒˆë¡œìš´ ì—í”¼ì†Œë“œ ë°œê²¬!")
                    podcast_data = alternative_podcast
                else:
                    print(f"âŒ ëª¨ë“  ëŒ€ì•ˆì—ì„œë„ ìƒˆë¡œìš´ ì—í”¼ì†Œë“œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
        
        elif feed.entries:
            print(f"í”¼ë“œì—ì„œ ìµœì‹  ì—í”¼ì†Œë“œ í™•ì¸ ì¤‘...")
            
            # ìµœê·¼ ë©°ì¹  ì´ë‚´ì˜ ì—í”¼ì†Œë“œë§Œ í•„í„°ë§
            recent_episodes = []
            for entry in feed.entries[:5]:  # ìµœê·¼ 5ê°œ ì—í”¼ì†Œë“œë§Œ í™•ì¸
                print(f"  ì—í”¼ì†Œë“œ í™•ì¸: {entry.title}")
                print(f"    ë°œí–‰ì¼: {entry.get('published', 'N/A')}")
                
                # NPR í”¼ë“œì¸ ê²½ìš° ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ ì¸ì§€ í™•ì¸
                if 'npr.org' in podcast_rss:
                    if not is_spanish_content_by_transcript(entry.link, entry.title, entry.get('summary', '')):
                        print(f"    âŒ ì˜ì–´ ì½˜í…ì¸ ë¡œ íŒë‹¨ë¨ - ê±´ë„ˆë›°ê¸°")
                        continue
                    else:
                        print(f"    âœ… ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸ (Radio Ambulante)ë¡œ í™•ì¸ë¨")
                
                if is_episode_recent(entry.get('published_parsed')):
                    recent_episodes.append(entry)
                    print(f"    âœ… ìµœê·¼ ì—í”¼ì†Œë“œë¡œ í™•ì¸ë¨")
                else:
                    print(f"    âŒ ì˜¤ë˜ëœ ì—í”¼ì†Œë“œ")
            
            if not recent_episodes:
                print("âš ï¸  ìµœê·¼ ìŠ¤í˜ì¸ì–´ ì—í”¼ì†Œë“œê°€ ì—†ìŠµë‹ˆë‹¤. ê°€ì¥ ìµœì‹  ìŠ¤í˜ì¸ì–´ ì—í”¼ì†Œë“œë¥¼ ì°¾ìŠµë‹ˆë‹¤.")
                # NPR í”¼ë“œì—ì„œ ìŠ¤í˜ì¸ì–´ ì½˜í…ì¸  ì°¾ê¸°
                for entry in feed.entries[:10]:  # ìµœê·¼ 10ê°œ ì¤‘ì—ì„œ ì°¾ê¸°
                    if 'npr.org' in podcast_rss:
                        if is_spanish_content_by_transcript(entry.link, entry.title, entry.get('summary', '')):
                            recent_episodes = [entry]
                            print(f"    âœ… ìŠ¤í˜ì¸ì–´ ì—í”¼ì†Œë“œ ë°œê²¬: {entry.title}")
                            break
                
                if not recent_episodes:
                    recent_episodes = [feed.entries[0]]  # ìµœí›„ì˜ ìˆ˜ë‹¨
            
            latest = recent_episodes[0]
            print(f"ì„ íƒëœ ì—í”¼ì†Œë“œ: {latest.title}")
            print(f"- ë§í¬: {latest.link}")
            print(f"- ë°œí–‰ì¼: {latest.get('published', 'N/A')}")
            print(f"- ìš”ì•½ ê¸¸ì´: {len(latest.get('summary', ''))}")
            
            episode_number = extract_episode_number(latest.title)
            duration = extract_duration_from_feed(latest)
            topic = extract_topic_keywords(latest.title, latest.get('summary', ''))
            
            episode_link = latest.link
            
            # ì—í”¼ì†Œë“œ ë§í¬ ìœ íš¨ì„± ê²€ì¦
            if not validate_url(episode_link):
                print(f"âš ï¸  ì—í”¼ì†Œë“œ ë§í¬ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {episode_link}")
                episode_link = podcast_apple_base  # ê¸°ë³¸ê°’ìœ¼ë¡œ Apple Podcasts ì‚¬ìš©
            
            # Apple Podcasts ë§í¬ ìƒì„± - ì—í”¼ì†Œë“œ ì œëª© í¬í•¨
            apple_link = generate_apple_podcast_link(podcast_name, podcast_apple_base, episode_link, episode_number, latest.title)
            
            # Radio Ambulanteì˜ ê²½ìš° Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URLì„ ë©”ì¸ URLë¡œ ì‚¬ìš©
            final_episode_url = episode_link
            if 'Radio Ambulante' in podcast_name:
                # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ëŠ”ì§€ í™•ì¸
                if apple_link != podcast_apple_base and validate_url(apple_link):
                    # Appleì—ì„œ ì •í™•í•œ ì—í”¼ì†Œë“œë¥¼ ì°¾ì•˜ìœ¼ë©´ Apple URLì„ ì‚¬ìš©
                    final_episode_url = apple_link
                else:
                    # Appleì—ì„œ ì°¾ì§€ ëª»í–ˆìœ¼ë©´ ì›ë³¸ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                    final_episode_url = episode_link
                    apple_link = podcast_apple_base  # Apple ë§í¬ëŠ” ë©”ì¸ í˜ì´ì§€ë¡œ ì„¤ì •
            else:
                # ë‹¤ë¥¸ íŒŸìºìŠ¤íŠ¸ëŠ” Apple ë§í¬ ìš°ì„  ì‚¬ìš©
                if apple_link != podcast_apple_base and validate_url(apple_link):
                    final_episode_url = apple_link
                else:
                    final_episode_url = episode_link
                    apple_link = podcast_apple_base
            
            # íŒŸìºìŠ¤íŠ¸ ë‚œì´ë„ ë¶„ì„ (ìš”ì•½ ë‚´ìš© ê¸°ë°˜)
            episode_summary = latest.get('summary', '')
            podcast_difficulty = analyze_text_difficulty(episode_summary) if episode_summary else "B2"
            
            podcast_data = {
                'title': latest.title,
                'url': final_episode_url,  # Appleì—ì„œ ì°¾ì§€ ëª»í•˜ë©´ ì—í”¼ì†Œë“œ URL ì‚¬ìš©
                'apple_link': apple_link,
                'published': latest.get('published', ''),
                'duration': duration,
                'episode_number': episode_number or 'N/A',
                'topic': topic,
                'podcast_name': podcast_name,
                'summary': latest.get('summary', '')[:200],
                'difficulty': podcast_difficulty  # ë‚œì´ë„ ì •ë³´ ì¶”ê°€
            }
            

            # ï¿½ ëŒ€ì•ˆ ëª¨ë“œì—ì„œëŠ” ì¤‘ë³µ ì²´í¬ë¥¼ ê±´ë„ˆë›°ê³  ë°”ë¡œ ì§„í–‰
            if not force_alternative:
                print(f"âœ… ì¼ë°˜ ëª¨ë“œ: ì¤‘ë³µ ì²´í¬ëŠ” create_notion_pages.pyì—ì„œ ìˆ˜í–‰ë©ë‹ˆë‹¤.")
            else:
                print(f"ğŸ”„ ëŒ€ì•ˆ ëª¨ë“œ: ì¤‘ë³µ ì²´í¬ë¥¼ ê±´ë„ˆë›°ê³  ì§„í–‰í•©ë‹ˆë‹¤.")
            
        else:
            print("ë©”ì¸ í”¼ë“œì— ì—í”¼ì†Œë“œê°€ ì—†ìŒ - ì´ ê²½ìš°ëŠ” ìœ„ì—ì„œ ì²˜ë¦¬ë¨")
            
    except Exception as e:
        print(f"íŒŸìºìŠ¤íŠ¸ ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        import traceback
        print(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")

    # í•™ìŠµ ìë£Œ ì •ë³´ë¥¼ í™˜ê²½ë³€ìˆ˜ë¡œ ì¶œë ¥
    # ëŒ€ì•ˆ ëª¨ë“œì—ì„œëŠ” GITHUB_OUTPUTì´ ì—†ì„ ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì¡°ê±´ë¶€ ì²˜ë¦¬
    if 'GITHUB_OUTPUT' in os.environ:
        try:
            with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
                if article_data:
                    f.write(f"article_title={article_data['title']}\n")
                    f.write(f"article_url={article_data['url']}\n")
                    f.write(f"article_category={article_data['category']}\n")
                    f.write(f"article_difficulty={article_data['difficulty']}\n")  # ë™ì  ë‚œì´ë„ ì¶œë ¥
                    f.write(f"article_memo={create_detailed_memo('article', article_data, weekday_name)}\n")
                
                if podcast_data:
                    f.write(f"podcast_title={podcast_data['title']}\n")
                    f.write(f"podcast_url={podcast_data['url']}\n")
                    f.write(f"podcast_apple={podcast_data['apple_link']}\n")
                    f.write(f"podcast_duration={podcast_data['duration']}\n")
                    f.write(f"podcast_topic={podcast_data['topic']}\n")
                    f.write(f"podcast_memo={create_detailed_memo('podcast', podcast_data, weekday_name)}\n")
        except Exception as e:
            print(f"GitHub Output íŒŒì¼ ì“°ê¸° ì˜¤ë¥˜: {e}")
    
    # ëŒ€ì•ˆ ëª¨ë“œì—ì„œëŠ” í‘œì¤€ ì¶œë ¥ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ í˜•íƒœë¡œ ê²°ê³¼ ì¶œë ¥
    if force_alternative or 'GITHUB_OUTPUT' not in os.environ:
        print("\n=== ìˆ˜ì§‘ëœ ìë£Œ ì •ë³´ (í™˜ê²½ë³€ìˆ˜ í˜•íƒœ) ===")
        if article_data:
            print(f'ARTICLE_TITLE="{article_data["title"]}"')
            print(f'ARTICLE_URL="{article_data["url"]}"')
            print(f'ARTICLE_CATEGORY="{article_data["category"]}"')
            print(f'ARTICLE_DIFFICULTY="{article_data["difficulty"]}"')
            print(f'ARTICLE_MEMO="{create_detailed_memo("article", article_data, weekday_name)}"')
        
        if podcast_data:
            print(f'PODCAST_TITLE="{podcast_data["title"]}"')
            print(f'PODCAST_URL="{podcast_data["url"]}"')
            print(f'PODCAST_APPLE="{podcast_data["apple_link"]}"')
            print(f'PODCAST_DURATION="{podcast_data["duration"]}"')
            print(f'PODCAST_TOPIC="{podcast_data["topic"]}"')
            print(f'PODCAST_MEMO="{create_detailed_memo("podcast", podcast_data, weekday_name)}"')
        print("=========================================")

    print("í•™ìŠµ ìë£Œ ìˆ˜ì§‘ ì™„ë£Œ!")
    if article_data:
        print(f"âœ… ê¸°ì‚¬: {article_data['title']}")
        print(f"   ì¹´í…Œê³ ë¦¬: {article_data['category']}")
        print(f"   ë‚œì´ë„: {article_data['difficulty']}")  # ë™ì  ë‚œì´ë„ ì¶œë ¥
    else:
        print(f"âŒ ê¸°ì‚¬ ìˆ˜ì§‘ ì‹¤íŒ¨")
        
    if podcast_data:
        print(f"âœ… íŒŸìºìŠ¤íŠ¸: {podcast_data['title']}")
        print(f"   ì£¼ì œ: {podcast_data['topic']}")
        print(f"   ì¬ìƒì‹œê°„: {podcast_data['duration']}")
    else:
        print(f"âŒ íŒŸìºìŠ¤íŠ¸ ìˆ˜ì§‘ ì‹¤íŒ¨")
        print(f"   RSS URL: {podcast_rss}")
        print(f"   íŒŸìºìŠ¤íŠ¸ëª…: {podcast_name}")

if __name__ == "__main__":
    main()
